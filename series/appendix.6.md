# Appendix 6: 特異点の幾何学: AIはなぜ汎化するのか

> [!NOTE]
> 本Appendixは、[Appendix 5](appendix.5.md) の双対構造（正則な情報幾何学）を前提とし、その枠組みが**破綻する**領域——特異モデル——に踏み込む。講義本編で繰り返し問うてきた「空間の形がモデルの性質を決める」というテーゼの、最も深い帰結の一つである。

## 正則な世界の限界

[Appendix 5](appendix.5.md) では、Fisher情報行列が「空間の曲がり具合」を測る計量として機能し、双対的な座標系（ $\boldsymbol{\theta}$ と $\boldsymbol{\eta}$ ）がきれいに張れる「正則（regular）」な世界を扱った。

しかし、現代の深層学習モデルの多くは、この前提を満たさない **「特異（singular）」なモデル** である。

### 正則と特異の違い

| 特徴 | 正則モデル（古典統計） | 特異モデル（ニューラルネット等） |
| --- | --- | --- |
| **パラメータ対分布** | 1対1（単射） | 多対1（冗長） |
| **Fisher情報行列** | 正定値（可逆） | **ランク落ち（不可逆・行列式0）** |
| **損失関数の谷** | 点（すり鉢状） | **平坦な底、あるいは分岐した谷** |
| **代表例** | 正規分布、ロジスティック回帰 | 多層パーセプトロン、Transformer |

ニューラルネットワークにおいて「あるニューロンの重みを0にする」と、そのニューロンに入力する前段の重みは、どんな値であっても出力に影響を与えなくなる。このときパラメータは特定の値に定まらず、Fisher情報行列はランク落ちし、逆行列（Cramér-Raoの下界などで必要）が存在しなくなる。これを **「特異点（singularity）」** と呼ぶ。

従来の統計学では、特異点は「扱いにくい例外」として避けられてきた。しかし渡辺澄夫らによる **特異学習理論（Singular Learning Theory, SLT）** は、**「特異点こそが、AIが未知のデータに対応（汎化）できる理由である」** という逆転の視点を提示した。

### なぜ深層学習は「特異」なのか

特異性は事故ではなく、ニューラルネットワークの構造そのものから必然的に生じる。代表的な発生源を整理しておく。

| 特異性の起源 | 説明 | 例 |
| --- | --- | --- |
| **ニューロンの消滅** | 重みが0になったニューロンへの入力側パラメータが不定になる | ReLUネットワークの死んだニューロン |
| **ニューロンの交換** | 隠れ層のニューロンを入れ替えても同一の関数を表す（対称性） | 任意の多層パーセプトロン |
| **ランクの縮退** | 重み行列のランクが落ちると、残りの自由度が冗長になる | 低ランク近似、LoRA的構造 |

これらはすべて、パラメータ空間のある領域で Fisher 行列の行列式が0になることを意味する。[第0回](series/00.md) で導入した多様体の言葉を使えば、パラメータ空間は滑らかな多様体ではなく、**特異点を持つ代数多様体** として捉える必要がある。

## 谷の幾何学：パラメータ数から「広さ」へ

### 自由エネルギーと汎化誤差

AIモデルが学習データ $D_n$ をどれだけうまく説明できているかだけでなく、未知のデータに対してどれだけ予測能力を持つか（汎化性能）を知るためには、単なる損失の最小値ではなく、**ベイズ自由エネルギー**（周辺尤度の負の対数）を考える必要がある。正則なモデルと特異モデルでは、その漸近形が異なる。

**正則モデル**（同定可能で、真値近傍でFisher情報行列が非退化）：

$$F(D_n) = n L_{min} + \frac{d}{2} \log n + O(1)$$

**特異モデル**（パラメータと分布が多対1）：

$$F(D_n) = n L_{min} + \lambda \log n - (m-1) \log \log n + O(1)$$

ここで $L_{min}$ は真の分布 $q(x)$ に対する期待損失（KLダイバージェンス）の最小値であり、実現可能（ $q$ がモデルに含まれる）なら理想化した設定では0になる（直感的には、学習ロスが到達しうる「下限」に対応する量と読んでよい）。 $n$ はデータ数、 $d$ はパラメータ数である。

正則モデルの第2項の係数はパラメータ数の半分 $d/2$ であり、BIC（ベイズ情報量基準）に一致する。パラメータが増えるほどペナルティが増え、汎化性能は悪化すると予測される。

一方、特異モデルでは係数 $\lambda$ （＝RLCT: **実対数正準閾値, Real Log Canonical Threshold**）はパラメータ数 $d$ とは一致せず、損失関数の谷の幾何学的構造から決まる量になる。 $m$ は学習ゼータ関数の極の **重複度（multiplicity）** であり、モデルによっては $m > 1$ となって $\log \log n$ 項が寄与する。

### RLCT：空間の「実質的な次元」

RLCT（= $\lambda$ ）は、損失関数の大域的最小値（loss landscapeの谷底）の **「幾何学的な広さ」** を測る指標である。

> [!NOTE]
> **記号についての注記**：文献により $\lambda$ の定義（係数としての取り出し方）が異なる場合がある。本Appendixでは、自由エネルギーの $\log n$ の係数をそのまま $\lambda$ （RLCT）と表記する。

- **谷が鋭い（狭い）**： $\lambda$ は大きい。わずかなズレでロスが増えるため、見かけのパラメータ数が同じでも「複雑」なモデルとして振る舞う。
- **谷が広い（平坦）**： $\lambda$ は小さい。パラメータが動いてもロスが変わらない領域が広いため、モデルは実質的に「単純」に振る舞う。

特異点付近では、パラメータ空間が代数幾何学的な特異点解消を経て、通常の次元よりも低い「実質的な次元（有効次元）」を持つようになる。これにより、**ベイズ漸近論の枠内では、モデルの複雑さペナルティはパラメータ数ではなくRLCTで決まる**。すなわち、パラメータ数がデータ数より遥かに多い巨大なモデルであっても、RLCTが小さければ（谷が広ければ）、汎化性能の劣化が抑えられる方向に働く。

ただしこれは「RLCTが小さければ必ず過学習しない」という無条件の保証ではない。RLCTの値は、真の分布がモデルに含まれるか（実現可能性）、事前分布の選び方、損失関数の設計などにも依存する。SLTが与えるのは、古典的な「パラメータ数＝複雑さ」という図式に代わる、より精密な複雑さの測り方である。

これは [第13回](series/13.md) で議論した「次元の呪い」に対する根本的な回答でもある。アンビエント次元（パラメータ数）ではなく、内在次元（RLCT）こそがモデルの実質的な複雑さを決めるのだ。

> [!NOTE]
> **「平坦な解（Flat Minima）」との関係**
> 深層学習の実務では「平坦な最小解（Flat Minima）ほど汎化する」という経験則が知られている。SLTはこの現象に厳密な数学的基礎を与える。特異点を含む領域は、確率的に選ばれる事後分布の体積が大きくなり（エントロピーが高くなり）、結果として単純で汎化性の高い解が選ばれやすくなるのである。

## 学習の相転移

特異学習理論がもたらすもう一つの重要な視点は、学習過程における **相転移（Phase Transition）** の発見である。

正則モデルの学習は、正解に向かって滑らかに収束する。しかし特異モデルでは、学習曲線（ロスや汎化誤差の推移）が階段状になることがある。

1. **プラトー（停滞期）**：サドル点（特異点）付近の平坦な領域にトラップされ、ロスが下がらない期間。
2. **相転移（急激な学習）**：ある瞬間に特異点から脱出し、より深い（あるいは広い）谷へと「落ちる」。

これは物理学における水の凍結や磁化のような相転移現象と構造的に類似している。最近のLLMで話題になる **"Grokking"（長時間の停滞の後に突然汎化する現象）** についても、SLTの枠組みを用いた相転移としての解釈・解析が進められている。

### 講義との接続：残差接続と相転移

[第8回](series/08.md) では、残差接続を「ベクトル場に沿った連続的な移動」として捉え直した。特異学習理論の視点を加えると、残差ブロックを積み重ねる深いネットワークは、学習の過程で特異点付近を通過しながら、段階的に表現の「相」を変えていることになる。学習初期に見られるプラトーは、ネットワークがまだ特異点付近の平坦な谷に留まっている状態であり、そこから脱出する瞬間が「表現の質的な転換」——すなわち相転移——に対応する。

## 特異点解消：裂け目を修復する数学

SLTの数学的中核は、**広中平祐の特異点解消定理**（1964年に証明、1970年にフィールズ賞受賞）にある。

直感的には、特異点がある空間（パラメータ空間）を、適切な座標変換（ブローアップ）によって **特異点のない滑らかな空間** に「引き伸ばす」操作である。

$$K(w) = \sum_i (p(x|w) - q(x))^2$$

この学習係数 $K(w)$ が特異点で退化している（微分が0になる方向が存在する）とき、特異点解消により新しい座標 $u$ で

$$K(u) = u_1^{2k_1} u_2^{2k_2} \cdots u_d^{2k_d}$$

という正規形に書き直せる。RLCTはこの指数 $k_i$ たちから決まり、谷の「幅」を幾何学的に特徴づける。

> [!TIP]
> 特異点解消を直感的に理解するために、2次元の例を考えよう。原点で2本の曲線が交差している（特異点）とする。ブローアップとは、原点を「方向ごとに引き剥がして」円に膨らませる操作であり、これにより2本の曲線は交差しなくなる。SLTでは、この操作をパラメータ空間の特異点に対して行い、谷の構造を解析可能にする。

## 情報幾何学と特異学習理論の統合

講義本編で扱った「Amariの幾何（正則）」と、ここで触れた「Watanabeの幾何（特異）」は、AIを理解するための車の両輪である。

- **Amariの幾何（多様体）**：
    学習が順調に進んでいるときの、滑らかな空間移動や、最適な最適化方向（自然勾配）を記述する。「地図」としての役割。

- **Watanabeの幾何（代数幾何）**：
    モデルのパラメータ設定そのものが冗長であることに起因する、学習の停滞、相転移、そして汎化の根本原理を記述する。「地図の裂け目」や「地形の質」の役割。

これらを統合することで、**「なぜ学習できるのか（最適化）」** と **「なぜ賢くなるのか（汎化）」** という2つの問いに対し、幾何学的な答えが出せるようになる。

### 他の回との接続

本Appendixの議論は、講義全体の複数の箇所と響き合う。

| 関連する回 | 接続点 |
| --- | --- |
| [第2回](series/02.md)（ノルムの呪い） | 高次元の距離の集中は、特異点付近の平坦性と同根の現象である |
| [第7回](series/07.md)（不確実性の復権） | ベイズ的な「分布としての表現」は、SLTの事後分布解析と自然に接続する |
| [第8回](series/08.md)（時間の発見） | 残差接続による連続力学系の描像は、特異点通過と相転移の舞台となる |
| [第13回](series/13.md)（高次元の深淵） | 「内在次元」の概念はRLCTの直感的な親戚にあたる |
| [Appendix 2](appendix.2.md)（純度問題） | データの質が loss landscape の谷の形状を変え、RLCTに影響する |
| [Appendix 5](appendix.5.md)（双対構造） | 正則な幾何の限界を明示し、本Appendixの出発点を与える |

## 講義本編との接続まとめ

| 概念 | 正則な幾何（Appendix 5） | 特異な幾何（Appendix 6） |
| --- | --- | --- |
| **対象** | 指数型分布族（正則） | 階層型モデル（特異） |
| **空間の構造** | 滑らかな多様体 | 特異点を持つ代数多様体 |
| **距離/計量** | Fisher情報行列（正定値） | ランク落ちしたFisher行列 |
| **モデル選択** | BIC（パラメータ数依存） | WBIC / 自由エネルギー（RLCT依存） |
| **最小解の形** | 点（Point） | 広い谷、あるいは連結成分 |
| **学習の描像** | 勾配降下による滑らかな移動 | 特異点付近での滞留と相転移 |
| **汎化の説明** | パラメータ数で決まる（BIC） | 谷の幾何学的広さで決まる（RLCT） |

> [!TIP]
> 特異学習理論は非常に奥深い分野であり、ここでは直感的な紹介に留めた。興味のある読者は、代数幾何学（特に広中の特異点解消定理）とベイズ統計学の接点を探求することをお勧めする。それは「知能の形」を数式で書き下すための、現在人類が持ち合わせている最も強力なレンズの一つである。

## 参考文献

### 特異学習理論（SLT）の基礎・総論

- **[渡辺澄夫 (2012). 『ベイズ統計の理論と方法』. コロナ社.](https://www.coronasha.co.jp/np/isbn/9784339024623/)**（ISBN: 978-4-339-02462-3）
    - ベイズ統計の基本を固めつつ、特異モデルを含むベイズ推論の考え方に入る入口として有用。SLTの厳密理論そのものは英語の専門書・論文に譲るが、概念整理に向く。
- **[Watanabe, S. (2009). *Algebraic Geometry and Statistical Learning Theory*. Cambridge University Press.](https://assets.cambridge.org/97805218/64671/copyright/9780521864671_copyright_info.pdf)**（ISBN: 978-0-521-86467-1）
    - SLTの中核（RLCT・ゼータ関数・特異点解消の応用）を体系化した基本文献。数学的詳細まで追いたい人向け。
- **[渡辺澄夫 (2006). 『代数幾何と学習理論』. 森北出版.](https://www.hanmoto.com/bd/isbn/9784627813212)**（ISBN: 978-4-627-81321-2）
    - 上記CUP書の日本語版に位置づく専門書。入手性は状況によるが、図書館等で参照できると強い。

### WBIC / WAIC / 一般化誤差（論文）

- **[Watanabe, S. (2013). “A Widely Applicable Bayesian Information Criterion.” *Journal of Machine Learning Research*.](https://jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf)**
    - WBICの代表的な定式化。Appendixの「自由エネルギーの漸近」と直結する。
- **[Watanabe, S. (2010). “Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory.” *Journal of Machine Learning Research* 11 (2010).](https://jmlr.org/papers/v11/watanabe10a.html)**
    - WAIC・CV・一般化誤差の関係を理論的に結び、SLTの予測（\(2\lambda/n\) 型の関係など）を理解するのに重要。

### 情報幾何学（正則モデル側の基礎）

- **[甘利俊一 (2019). 『新版 情報幾何学の新展開』. サイエンス社.](https://www.saiensu.co.jp/search/?isbn=978-4-7819-1463-3&y=2019)**（ISBN: 978-4-7819-1463-3）
    - 正則モデルにおけるFisher計量や自然勾配など、Appendix 5と接続する基礎を日本語で押さえたいときに有用。
- **[Amari, S. (2016). *Information Geometry and Its Applications*. Springer.](https://link.springer.com/book/10.1007/978-4-431-55978-8)**
    - 情報幾何の体系的な英語文献。理論と応用の射程が広い。

### 深層学習と特異性・平坦性・grokking

- **[Wei, S., Murfet, D., Gong, M., Li, H., Gell-Redman, J., Quella, T. “Deep Learning Is Singular, and That’s Good.” (arXiv:2010.11560)](https://arxiv.org/abs/2010.11560)** [著者公開PDF](https://www.suswei.com/publication/wei-2022-singular/wei-2022-singular.pdf)
    - 「ニューラルネットは特異モデルである」という点を前面に出し、古典的なLaplace近似などの限界も含めて議論する。SLTを深層学習理論へ接続する読み物として良い。
- **[Power, A., et al. (2022). “Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.” (arXiv:2201.02177)](https://arxiv.org/abs/2201.02177)**
    - 長い停滞の後に突然汎化が進む “grokking” を報告した代表的論文。SLTの相転移的な見方と関連づけて議論されることがある。
- **[Hochreiter, S., & Schmidhuber, J. (1997). “Flat Minima.” *Neural Computation* 9(1):1–42.](https://direct.mit.edu/neco/article/9/1/1/6027/Flat-Minima)** [著者公開PDF](https://www.bioinf.jku.at/publications/older/3304.pdf)
    - 「平坦な解（flat minima）が汎化に有利」という古典的主張の代表。RLCTの直感（谷の“広さ”）を説明するときの背景として参照しやすい。

### 数学的背景（代数幾何：特異点解消）

- **[Hironaka, H. (1964). “Resolution of Singularities of an Algebraic Variety Over a Field of Characteristic Zero.” *Annals of Mathematics* (1964).](https://www.jstor.org/stable/1970486)** [PDF（ミラー）](https://math.tecnico.ulisboa.pt/seminars/download.php?fid=2650)
    - 特異点解消の原典。SLTの数学的基盤として歴史的・理論的に重要。
