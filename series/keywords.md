
# キーワード集

## 第0回：幾何学という言語 ～この講義の羅針盤～

### 空間の構造（多様体論）

空間がどのような「形」をしているかを記述する、本講義の最も基礎となる概念群。

| キーワード | 説明 |
| --- | --- |
| **多様体** (Manifold) | 局所的にはユークリッド空間だが、大域的には異なる構造を持つ空間。 |
| **多様体仮説** | 高次元データの実質的な構造は、低次元の多様体上に集中しているという仮説。 |
| **リーマン多様体** | 距離、角度、曲率を定義するために「リーマン計量」を導入した多様体。ただし計量が当然の前提の話では単に「多様体」とよぶこともある。 |
| **曲率** (Curvature) | 空間がどれだけ「平ら」から逸脱しているかの指標（正・ゼロ・負）。 |
| **次元の呪い/祝福** | 高次元空間特有の性質（ランダムなベクトルが直交しやすいなど）。 |
| **スパース性** | 高次元空間の「ほとんどが空」であるという性質。距離の集中や直交性と並ぶ高次元の直感的特徴。 |

### 距離と移動（幾何学的操作）

空間内での「近さ」や「動き」を定義・制御するための道具。

| キーワード | 説明 |
| --- | --- |
| **測地線** (Geodesic) | 曲がった空間における局所的な最短経路。 |
| **リーマン計量** | 各点での距離や角度を測るための数学的装置。 |
| **接空間** | <a id="kw-tangent-space"></a>多様体上の点の近傍を「平ら」に近似する線形空間。リーマン計量はこの空間に内積を与える。 |
| **球面線形補間** (Slerp) | 球面上の測地線に沿った自然な補間方法。 |
| **平行移動** | ベクトルを向きを変えずに移動させる操作（曲がった空間では元の位置に戻ると向きがズレる）。 |

### 確率と学習（情報幾何学）

幾何学を統計的な推論やニューラルネットワークの学習に結びつける概念。

| キーワード | 説明 |
| --- | --- |
| **情報幾何学** | 確率分布の空間を多様体として扱う学問。 |
| **再パラメータ化不変性** | 座標系の取り方に依存しない性質。 |
| **フィッシャー情報行列** | <a id="kw-fisher-info"></a>確率分布の「変化しやすさ」を反映したリーマン計量。 |
| **自然勾配** (Natural Gradient) | <a id="kw-natural-gradient"></a>フィッシャー情報行列を用いて、分布空間での等しい変化量を目指す勾配補正手法。 |
| **統計多様体** | パラメータで表現される確率分布族が成す多様体。点は分布そのものに対応する。 |
| **最尤推定** | 観測データを最も尤もらしく説明する分布（パラメータ）を選ぶ推定法。情報幾何では多様体への射影として解釈される。 |
| **KLダイバージェンス** | <a id="kw-kl-divergence"></a>分布間の隔たりを測る非対称な尺度。 |
| **ブレグマンダイバージェンス** | KLダイバージェンスを含む広いクラスの非対称尺度。 |
| **十分統計量** | 分布族を特徴付けるために必要十分な統計量。情報幾何では座標系の選び方と関係する。 |
| **Softmax**, **Attention** | 連続的な値を離散的な確率や重みに変換する「界面」の操作。 |

### 具体的な空間モデル

幾何学的制約を明示的に設計に取り入れた具体的なモデルや空間の分類。

| キーワード | 説明 |
| --- | --- |
| **球面** (Positive Curvature) | 正の曲率を持つ空間。nGPTやArcFaceなどで利用される。 |
| **双曲空間** (Negative Curvature) | <a id="kw-hyperbolic-space"></a>負の曲率を持つ空間。階層構造の表現に適している。 |
| **ユークリッド空間** (Zero Curvature) | <a id="kw-euclidean-space"></a>直線、平行線、ピタゴラスの定理が成り立つ平坦な空間。古典的な機械学習（PCA、SVMなど）が前提とする。 |
| **位相** (Topology) | 形ではなく「穴」などの構造を扱う概念（TDAなど）。 |

## 第1回：かつての地図 ～平らな世界で戦っていた私たち～

### 空間の基本前提：ユークリッド幾何学

深層学習以前の機械学習が暗黙のうちに基礎としていた「平らな世界」の性質。

| キーワード | 説明 |
| --- | --- |
| **ユークリッド空間** | → [［再掲・第1回］](#kw-euclidean-space) |
| **線形性** | データの構造が直線や平面（線形部分空間）で捉えられるという仮定。 |
| **等方性** | 空間のすべての方向が対称であり、等しく重要であるという性質。 |
| **ピタゴラスの定理** | 距離を成分の二乗和の平方根で計算する、ユークリッド空間における距離計算の基礎。 |

### 線形次元削減と投影：PCAとSVD

高次元データを低次元に落とし込み、可視化や分析を行うための古典的手法。

| キーワード | 説明 |
| --- | --- |
| **主成分分析 (PCA)** | 分散が最大になる方向を見つけ、データを線形射影する次元削減手法。 |
| **特異値分解 (SVD)** | 行列を3つの行列に分解する手法。PCAの主成分抽出と密接に関連する。 |
| **線形射影** | 高次元空間から低次元空間への投影。「影絵」のようにデータの一部を映し出す。 |
| **固有顔 (Eigenfaces)** | 顔画像をPCAで低次元表現し、比較・認識する画像処理への応用例。 |
| **中心化** | 各次元の平均を引き、重心を原点に移す前処理。PCAの標準手順。 |
| **共分散行列** | 変数間の分散・共分散をまとめた行列。PCAでは固有値分解の対象となる。 |
| **固有値分解** | 正方行列を固有値と固有ベクトルに分解する操作。PCAの数学的基盤。 |
| **確率的PCA (PPCA)** | PCAを確率モデルとして解釈する枠組み。等方ガウスノイズを仮定する。 |
| **因子分析** | 観測変数を少数の潜在因子で説明する確率モデル。PPCAと近縁。 |
| **カーネルPCA** | カーネルトリックを用いて非線形な次元削減を行う拡張。 |
| **オートエンコーダ** | 非線形写像による次元削減と復元を学習するニューラルネットワーク。 |
| **スイスロール** | 非線形多様体の典型例として用いられる人工データ。 |

### 分離と境界の幾何学：SVMとロジスティック回帰

データをクラスごとに分類するための、幾何学的および統計的なアプローチ。

| キーワード | 説明 |
| --- | --- |
| **マージン最大化** | 二つのクラスを隔てる境界と、最も近いデータ点（サポートベクター）との距離を最大化するSVMの核心アイデア。 |
| **サポートベクター** | 決定境界を「支える」最小限のデータ点集合。 |
| **カーネルトリック** | データを高次元空間へ暗黙的に写像することで、元の空間では非線形なデータを線形分離可能にする手法。 |
| **ロジスティック回帰** | 決定境界ではなく、シグモイド関数を用いて「クラスに属する確率」を直接モデル化する統計的手法。 |
| **ヒンジ損失** | SVMで用いられる損失関数。マージン内の誤差のみを罰する。 |
| **ロジスティック損失** | ロジスティック回帰で用いられる損失関数。全点が損失に寄与する。 |
| **交差エントロピー損失** | 確率分布の不一致を測る損失。ロジスティック回帰の最尤推定に対応。 |
| **RBFカーネル** | 距離に基づく代表的カーネル。近い点同士の類似度を高く評価する。 |
| **RKHS** | 再生核ヒルベルト空間。カーネルが定める特徴空間の内積構造。 |

### 離散構造と意味の抽出：トピックモデル

テキストなどの離散的なデータを、トピックという抽象的概念で整理する手法。

| キーワード | 説明 |
| --- | --- |
| **Bag-of-Words (BoW)** | 単語の出現順序を無視し、頻度のみを扱う「単語の袋」としてのデータ表現。 |
| **トピックモデル** | 文書を複数の潜在トピックの混合として捉える確率的モデルの総称。 |
| **pLSA** | 文書と単語の共起を潜在トピックで説明する初期のトピックモデル。 |
| **LDA (Latent Dirichlet Allocation)** | 文書を複数の潜在的なトピックの混合として記述する確率的生成モデル。 |
| **分布的意味論の欠如** | 古典的なBoW手法において、単語同士の意味的な近さが表現に含まれていなかった限界。 |

### 古典的手法の限界と課題

「平らな世界」を前提としたことで見落とされていた構造や、次世代への課題。

| キーワード | 説明 |
| --- | --- |
| **非線形多様体** | スイスロールのように、ユークリッド空間内で巻いたり曲がったりしているデータ本来の形。 |
| **ノルムの呪い** | ベクトルの「長さ」と「方向」が区別されず、意味や確信度が混同されていた問題。 |
| **順序の喪失** | BoWのように、言語の本質である「時間の流れや文脈」を捨ててしまっていた点。 |
| **コサイン類似度** | <a id="kw-cosine-sim"></a>ベクトルのなす角に基づく類似度尺度。角度の重要性の文脈で登場。 |

## 第2回：ノルムの呪い ～意味と不確実性の未分化～

### ノルム（ベクトルの長さ）を巡る混迷

深層学習の初期から存在する、ベクトルの「大きさ」に複数の意味が混ざり合ってしまう問題に関する概念。

| キーワード | 説明 |
| --- | --- |
| **ノルムの三重の意味** | 「意味の強度」「モデルの確信度」「スケーリングノイズ」の3つが、一つの数値に未分化なまま混在している状態。 |
| **Word2Vec** | 分散表現を学習する古典的手法。（設定にもよるが）しばしば高頻度語ほどノルムが大きくなる傾向が観察され、検索でのハブ化などの議論につながる。 |
| **ハブ化 (Hubness)** | ノルムの大きなベクトルが、あらゆる検索クエリで上位に現れてしまう現象。 |
| **確信度の偽装** | Softmaxの出力（確率の尖り方）が、本質的な確信度ではなく、単なるロジットのスケール（ノルム）に左右される問題。 |
| **Layer Normalization** | <a id="kw-layer-norm"></a>各トークンの特徴方向について平均・分散（またはRMS）を正規化し、活性のスケール変動を抑える。結果として表現のスケール（ノルムの暴走）を緩和し得るが、L2ノルムを厳密にa一定値に固定するわけではない。（ただし、RMSNormでは各ベクトルの方向のみを保持し、スケールを一定化する側面が強い） |

### 次元の呪い：高次元空間の逆説

次元数が上がることで、我々の直感（ユークリッド幾何）が通用しなくなる幾何学的な性質。

| キーワード | 説明 |
| --- | --- |
| **距離の集中** (Concentration of distances) | <a id="kw-distance-concentration"></a>次元が上がるほど、あらゆる点間の距離が期待値付近に集中し、相対的な差が消滅する現象。 |
| **ランダム直交性** | <a id="kw-random-orthogonality"></a>高次元空間において、ランダムに選ばれた2つのベクトルは、統計的にほぼ90°（直交）をなすという性質。 |
| **赤道への集中** | 高次元球面では、ある軸（極）を固定すると、点の大半はその軸に対してほぼ直交する方向（赤道付近）に集まる、という体積（面積）集中の現象。 |
| **最近傍探索 (kNN) の困難** | すべての点が「だいたい同じ距離」になるため、「近さ」に基づく分類や検索が意味をなさなくなる問題。 |
| **アンビエント次元** | データが存在する「座標空間」の次元。内在次元との区別が重要になる。 |

### 「向き」と「長さ」を分離する試み

ノルムの曖昧さを解消し、情報を幾何学的に整理しようとした歴史的・構造的なアプローチ。

| キーワード | 説明 |
| --- | --- |
| **カプセルネットワーク (Capsule Networks)** | 「向き」で特徴の属性（姿勢など）を、「長さ」で存在確率を明示的に分離して表現しようとしたHintonらの提案。 |
| **squash関数** | ベクトルの向きを保ったまま、長さを0〜1の範囲に非線形に圧縮する関数。 |
| **制限ボルツマンマシン (RBM)** | エネルギー関数で表現を学習する生成モデル。ノルムの意味の曖昧さが顕在化した歴史的背景として登場。 |
| **動的ルーティング** | カプセルネットワークで下位カプセルから上位カプセルへの結合を入力依存で決める仕組み。 |
| **直交性の祝福** | ランダムな直交性を利用して、互いに干渉しない独立した表現を大量に埋め込めるというポジティブな側面。 |
| **内在次元** | <a id="kw-intrinsic-dim"></a>データが実際に分布している低次元多様体の次元。これを見極めることが、次元の呪いを回避する鍵となる。 |

### 数学的・実装的背景

理論の成立条件や、それを支える統計的な法則。

| キーワード | 説明 |
| --- | --- |
| **独立同分布 (i.i.d.)** | 距離の集中や直交性が導かれる前提となる、成分間の独立性と同一分布性。 |
| **球面符号 (Spherical code)** | 球面上の限られた「角度」の中に、どれだけ多くの非干渉なベクトルを詰め込めるかという数学的課題。 |
| **Zipf則** | Word2Vecのノルム分布が単語の頻度に依存する背景にある、自然言語の統計的法則。 |

## 第3回：プラネタリウムの建設 ～極座標へのパラダイムシフト～

### 球面上の学習と設計思想

ベクトルのノルムを固定し、「向き（角度）」のみを情報の自由度とする設計方針。

| キーワード | 説明 |
| --- | --- |
| **球面制約** | すべての表現ベクトルを単位球面（ノルム1）の上に制約する設計。 |
| **nGPT (Normalized GPT)** | Transformerの全レイヤーに球面制約を徹底し、学習効率を改善したアーキテクチャ。 |
| **プラネタリウム・メタファー** | 球面をドームに見立て、星（データ点）の距離ではなく、相対的な位置関係（角度）に注目する直感的理解。 |
| **正規化** | ベクトルの幾何学的な長さを1に揃える操作。LayerNormとは異なり、厳密な球面への射影を行う。 |

### 球面上の確率統計

ユークリッド空間における正規分布の概念を、曲がった球面状に拡張した数学的道具。

| キーワード | 説明 |
| --- | --- |
| **von Mises-Fisher (vMF) 分布** | <a id="kw-vmf"></a>球面上の「正規分布」に相当する確率分布。 |
| **平均方向** ( $\boldsymbol{\mu}$ ) | vMF分布における分布の中心を示す単位ベクトル。 |
| **集中度** ( $\kappa$ ) | <a id="kw-kappa"></a>分布の鋭さを制御するパラメータ。大きいほど確実で、小さいほど不確実。Softmaxの温度の逆数に対応する。 |
| **正規化定数** ( $C_d(\kappa)$ ) | vMF分布の確率密度が1になるように調整する係数。 |
| **変形ベッセル関数** ( $I_\nu$ ) | vMF分布の正規化定数に現れる特殊関数。 |
| **方向統計学** | 方向（角度）を持つデータの統計的性質を扱う学問。 |

### 球面における幾何学的指標

球面という正の曲率を持つ空間で「近さ」や「道筋」を定義する概念。

| キーワード | 説明 |
| --- | --- |
| **測地線距離** | 球面上の2点間の最短経路（大円の弧）の長さ。角度  そのものに対応する。 |
| **コサイン類似度** | → [［再掲・第3回］](#kw-cosine-sim) |
| **正の曲率** | 球面が持つ幾何学的性質。三角形の内角の和が180°を超えるなどの特徴を持つ。 |

### 球面上の最適化技術

制約条件（ノルム=1）を維持しながら効率的に学習を進めるための計算手法。

| キーワード | 説明 |
| --- | --- |
| **リーマン勾配** | 球面の接空間に射影された、制約を破らない勾配の方向。 |
| **接空間** | → [［再掲・第3回］](#kw-tangent-space) |
| **射影勾配法 (Retraction)** | 更新で球面から外れたベクトルを、正規化によって再び球面へ引き戻す手法。 |
| **リーマン最適化** | <a id="kw-riemann-optim"></a>多様体制約を満たすように、接空間とリトラクションを用いて最適化する枠組み。 |
| **混合精度学習** | float16/bfloat16などを使い、計算効率と数値安定性を両立する学習手法。 |
| **数値安定性とeps** | 混合精度学習（float16等）において、ゼロ除算やアンダーフローを防ぐための微小な補正値。 |

## 第4回：分類の再統一 I ～Softmaxと情報幾何学～

連続的な「向き」を、離散的な「カテゴリ」へと翻訳する界面の幾何学。

### 確率空間の構造

| キーワード | 説明 |
| --- | --- |
| **確率単体** (Simplex) | すべての確率の和が1になる制約を満たす空間。Softmaxの出力先となる凸集合。 |
| **確率単体の内部** | すべての確率が正となる領域。Softmaxの像は境界に達せず内部に留まる。 |
| **ロジット** (Logit) | 確率に変換される前の「生のスコア」。Softmaxの入力空間。 |
| **温度パラメータ** (Temperature) | Softmaxの分布の鋭さを調整するスケール。低温で尖り、高温で平滑化。 |
| **平行移動不変性** | 全ロジットに同じ定数を足してもSoftmaxの出力が変わらない性質。 |
| **商空間** | ロジット空間の冗長な1次元（ $\mathbf{1}$ 方向）を同一視した空間。 |
| **商写像** | ロジット空間の冗長な1次元（平行移動不変性）を「潰して」単体に写す操作。 |
| **最大エントロピー原理** | 与えられた制約下で最もランダム（仮定が少ない）な分布を選ぶ原理。Softmaxが指数型になる理論的根拠。 |

### 最適化の幾何学

| キーワード | 説明 |
| --- | --- |
| **Log-Sum-Exp** | Softmaxの分母に現れる滑らかな最大値関数。損失関数の凸性を保証し、最適化を助ける。 |
| **分配関数** (Log-Partition) | <a id="kw-log-partition"></a>全ての候補を正規化する項。SoftmaxやCRFで $\log \sum \exp$ として現れる。 |
| **自然勾配** | → [［再掲・第4回］](#kw-natural-gradient) |
| **フィッシャー情報行列** | → [［再掲・第4回］](#kw-fisher-info) |

### 双対構造と射影

| キーワード | 説明 |
| --- | --- |
| **e-接続** (Exponential Connection) | 指数型分布族の自然パラメータ空間で直線を与える接続。ロジット側の「まっすぐ」に対応する。 |
| **m-接続** (Mixture Connection) | 混合（凸結合）で直線を与える接続。確率単体側の「まっすぐ」に対応する。 |
| **双対アファイン接続** | Fisher計量に関して e-接続と m-接続が互いに双対をなす構造。 |
| **双対平坦空間** (Dually Flat Space) | e-平坦性とm-平坦性が同時に成り立つ空間。指数型分布族の中心的幾何構造。 |
| **自然パラメータ** ( $\boldsymbol{\theta}$ ) | 指数型分布族を対数線形で表す座標系。Softmaxでは（ゲージ同値を除いた）ロジットに対応する。 |
| **期待値パラメータ** ( $\boldsymbol{\eta}$ ) | 十分統計量の期待値で与えられる座標系。カテゴリカル分布では確率ベクトルに対応する。 |
| **ルジャンドル変換** | 自然パラメータと期待値パラメータを結ぶ双対変換。Softmaxは対数分配関数の勾配写像として現れる。 |
| **情報射影** (Information Projection) | KLダイバージェンスのもとで分布を部分空間へ射影する操作。e-射影（最尤）とm-射影（モーメント）がある。 |

### 離散化と構造化

| キーワード | 説明 |
| --- | --- |
| **構造化予測** | 点ごとの独立予測ではなく、系列全体の整合性を持つ出力を選ぶ枠組み。 |
| **CRF (Conditional Random Field)** | 系列ラベリングなどで条件付き確率を正規化する対数線形モデル。Softmaxの系列版として全体正規化を行う。 |
| **Viterbiアルゴリズム** | CRFなどで最尤系列を動的計画法で求める手法。 |
| **Gumbel-Softmax** | Gumbelノイズでサンプリングを近似し、微分可能に離散化する連続緩和。 |
| **Gumbel-Maxトリック** | Gumbelノイズを加えたargmaxがカテゴリカルサンプリングに一致する性質。 |
| **STE (Straight-Through Estimator)** | 順伝播は離散、逆伝播は連続勾配を使う近似的学習手法。 |
| **Top-k / Top-p サンプリング** | 高確率語のみからランダムに選ぶ生成手法。多様性と品質を調整する。 |

## 第5回：分類の再統一 II ～マージンの幾何学～

SVMの「距離マージン」と、球面設計における「角度マージン」の幾何学的統合。

### 境界と分離

| キーワード | 説明 |
| --- | --- |
| **SVM (Support Vector Machine)** | 最大マージン原理に基づき決定境界を決める分類器。ユークリッド空間の距離を基準に分離を行う。 |
| **最大マージン原理** | 決定境界とデータの最小距離を最大化し、汎化性能を高めるSVMの核心的思想。 |
| **ソフトマージンSVM** | スラック変数を導入し、分離不能なデータにも対応するSVM。マージン最大化と誤分類許容のトレードオフを制御する。 |
| **大円** | 球面における「直線（測地線）」。原点を通る超平面との交線であり、正規化された分類器の決定境界となる。 |
| **角度マージン** | 球面上でクラス間の分離度を測る単位。ユークリッド空間の距離マージンの球面版。 |

### 実装手法

| キーワード | 説明 |
| --- | --- |
| **ArcFace** | 正解クラスの角度にハンディキャップ $m$ を加え、決定境界を強制的に押し込むことでクラス内凝縮性を高める手法。 |
| **CosFace / SphereFace** | それぞれコサイン空間での減算、あるいは角度への乗算によってマージンを確保するアプローチ。 |

## 第6回：Transformerという測量士 ～動的な接続～

固定された空間ではなく、入力に応じて「どこを見るか」を動的に測量する機構。

### Attentionの幾何学

| キーワード | 説明 |
| --- | --- |
| **天体観測メタファー** | Query（望遠鏡）がKey（星の輝き）を捉え、Value（光の量）を統合する、Attentionの動的性質を捉えた直感的理解。 |
| **Query-Key-Value (QKV)** | QueryでKeyとの整合度を計算し、Valueを加重和で統合するTransformerの基本機構。 |
| **Scaled Dot-Product** | 内積を次元の平方根で割り、Softmaxが極端に鋭くなるのを防いで勾配を安定させる技術。 |
| **Cosine Attention** | $Q$ と $K$ をL2正規化することで、Attentionスコアを純粋な「角度（コサイン類似度）」にする設計。 |
| **Multi-head Attention** | 異なる部分空間で並列にAttentionを計算し、多様な関係を同時に捉える設計。 |

### 位置と回転

| キーワード | 説明 |
| --- | --- |
| **絶対位置埋め込み** | トークンの位置をそのまま符号化して加える方式（初期Transformerの正弦波埋め込みなど）。 |
| **RoPE** (Rotary Position Embedding) | トークンの相対位置をベクトルの「回転」として表現する手法。内積が相対距離のみに依存する幾何学的性質を持つ。 |
| **ALiBi** (Attention with Linear Biases) | Attention重みに線形バイアスを加え、相対位置の効果を導入する設計。 |
| **相対不変性** | トークン間の絶対的な位置ではなく、その「隔たり（回転角の差）」が意味決定に寄与する性質。 |

## 第7回：不確実性の復権 ～Variance Matters～

「点」としての表現から、確信度（集中度）を伴う「分布」としての表現への拡張。

### 球面上の分布

| キーワード | 説明 |
| --- | --- |
| **vMF分布** (von Mises-Fisher) | 単位球面上のガウス分布。平均方向 $\boldsymbol{\mu}$ と、その周りの鋭さを決める集中度  $\kappa$ で定義される。 |
| **集中度** | → [［再掲・第7回］](#kw-kappa) |
| **分布埋め込み** | データを1点に固定せず、確率分布（雲）として配置することで不確実性を明示的に扱う設計。 |
| **VAE (Variational Autoencoder)** | <a id="kw-vae"></a>潜在変数を分布として扱い、ELBO最適化で学習する生成モデル。潜在空間の不確実性を明示化する。 |

### 信頼性と比較

| キーワード | 説明 |
| --- | --- |
| **OOD検知** | 推定された  $\kappa$ が閾値以下のサンプルを「未知（分布外）」と判定し、ハルシネーション等を抑制する技術。 |
| **回転不変統計量** | 座標系の回転に左右されない統計量。  $\kappa$ を使うことで、モデル間の整列（Alignment）なしでの比較が可能になる。 |
| **Procrustes整列** | 対応点ペアを用いて、2つの埋め込み空間の回転・反転を最適化する整列手法。 |
| **ハルシネーション** | 低確信度領域で、根拠の薄い出力を確信的に生成してしまう現象。 |

### ガウス空間と典型集合

| キーワード | 説明 |
| --- | --- |
| **典型集合** (Typical Set) | 高次元ガウス分布で確率質量が集中する薄い球殻。密度最大点とは別に現れる。 |
| **測度集中** (Concentration of Measure) | 次元が上がるほど確率質量が特定の領域に集中する現象。典型集合の背景。 |
| **Hyperspherical VAE** | 潜在分布にvMF分布を用いるVAEの拡張。球面的潜在構造に適合する設計。 |

### 不確実性の定量

| キーワード | 説明 |
| --- | --- |
| **エントロピー** | 分布の広がり（不確実性）を測る指標。vMFでは $\kappa$ と次元の関数になる。 |
| **Bessel関数** | vMF分布の正規化定数やエントロピーに現れる特殊関数。数値計算が難しい。 |
| **vMF混合モデル** | 複数のvMF分布の混合で密度を表現し、OOD検知などに利用する。 |

### 学習と実装

| キーワード | 説明 |
| --- | --- |
| **再パラメータ化トリック（球面版）** | vMFなど球面分布でのサンプリング勾配推定を安定化する手法。 |
| $\kappa$ **の崩壊** | 学習が進むと $\kappa$ が極端に大きくなり、分布が点に収束して不確実性が失われる現象。 |

## 第8回：時間の発見 ～一撃からの脱却～

静的な「召喚」から、時間軸に沿った「連続的な変換（プロセス）」へのパラダイムシフト。

### 一撃型生成の限界

| キーワード | 説明 |
| --- | --- |
| **GAN** | 潜在変数 $\mathbf{z}$ を一度の変換で画像へ写像する生成器と識別器の敵対的学習。 |
| **VAE** | → [［再掲・第8回］](#kw-vae) |
| **中間過程の不可視性** | 生成が一撃で起きるため、途中状態の観測や介入が困難になる性質。 |
| **モード崩壊** | GANで特定パターンのみが生成され、多様性が失われる不安定現象。 |

### 流れの制御

| キーワード | 説明 |
| --- | --- |
| **残差接続** (ResNet) | 状態に変化量を加える $h_{t+1} = h_t + f(h_t)$ 形式。多様体上の「流れ（フロー）」の離散近似と見なせる。 |
| **Neural ODE** | 深層学習の「深さ」を連続的な「時間」と定義し、常微分方程式（ODE）としてモデル化する枠組み。 |
| **ベクトル場** | <a id="kw-vector-field"></a>各点に対して「次にどの方向に動くべきか」を指示する関数。ニューラルネットワークの本質的役割。 |

| キーワード | 説明 |
| --- | --- |
| **随伴法** (Adjoint Method) | ODEを逆時間方向に解くことで、中間状態をメモリに保存せずに効率的に学習するアルゴリズム。 |
| **Augmented Neural ODEs** | 状態空間の次元を拡張し、軌道の交差を許容することで表現力の制約を突破する設計。 |

### 安定化と正規化

| キーワード | 説明 |
| --- | --- |
| **ノルム暴走** | 学習中に表現ベクトルのノルムが爆発し、勾配発散や多様性崩壊を招く不安定性。 |
| **正規化層** | 活性化のスケールを制御し、空間の安定性を高める層（BatchNorm/LayerNorm/RMSNormなど）。 |
| **Batch Normalization** | ミニバッチ内の平均・分散で正規化する手法。 |
| **Layer Normalization** | → [［再掲・第8回］](#kw-layer-norm) |
| **RMSNorm** | 平均を引かず二乗平均平方根で割る正規化。 |

### 連続時間と数値解法

| キーワード | 説明 |
| --- | --- |
| **オイラー法** | 残差接続を時間刻みの数値積分として読むときの基礎的近似。 |
| **ODEソルバー** | Runge-Kutta等で $t=0$ から $t=T$ まで積分する数値解法。 |
| **適応ステップ** | 誤差制御に応じてステップ幅を自動調整する積分戦略。 |
| **剛性 (Stiffness)** | 変化のスケールが極端に異なるため、数値積分が難しくなる性質。 |

### 力学系の視点

| キーワード | 説明 |
| --- | --- |
| **位相空間** | 状態が点として配置され、時間発展が軌道として表れる空間。 |
| **軌道** | 時間に沿って状態がたどる連続的な経路。 |
| **アトラクター** | 軌道が最終的に引き寄せられる安定集合。 |

## 第9回：拡散と凝縮 ～熱力学との融合～

ノイズから意味が立ち上がるプロセスを、確率微分方程式とスコア関数の幾何学として捉える。

### 確率過程の幾何学

| キーワード | 説明 |
| --- | --- |
| **スコア関数** | データ分布の対数密度の勾配  $\nabla_{\mathbf{x}} \log p(\mathbf{x})$ 。データが「密集している方向」を指し示すベクトル場。 |
| **ランジュバン動力学** | スコア関数（勾配）に従って微小なノイズを加えながら移動し、データ分布からサンプリングを行う手法。 |
| **確率流ODE** | 拡散モデルの逆過程（SDE）と同一の周辺分布を持つ決定論的な軌道。生成をODEとして扱えるようにする。 |
| **確率微分方程式 (SDE)** | ドリフト項と拡散項を持つ確率過程の記述。拡散モデルの順過程・逆過程の基礎。 |
| **ブラウン運動** | 連続時間のランダム揺らぎを表す標準ウィーナー過程。SDEの拡散項で用いられる。 |
| **ベクトル場** | → [［再掲・第9回］](#kw-vector-field) |

### 生成のメカニズム

| キーワード | 説明 |
| --- | --- |
| **順過程と逆過程** | データをノイズへ拡散させる過程と、学習したスコアを用いてノイズからデータを復元（凝縮）させる過程。 |
| **Flow Matching** | SDEを介さず、データからノイズへの直線的な「流れ（ベクトル場）」を直接学習する、より幾何学的に明快な生成手法。 |
| **ノイズスケジュール ( $\\beta_t$ )** | 時間に応じてノイズ強度を制御する関数。拡散の速度と分散の増え方を決める。 |
| **VP-SDE** | Variance Preserving SDE。標準的な拡散モデルで用いられる $\mathbb{R}^d$ 上の定式化。 |
| **ガウス分布への収束** | 順過程の最終状態が $\mathcal{N}(0, I)$ に近づく性質。標準拡散の終点。 |
| **ノイズ予測ネットワーク** | $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ を学習し、スコア推定と同等の役割を果たすモデル。 |
| **スコアマッチング** | 正規化定数を使わずにスコア関数を学習する手法。ノイズ予測と等価。 |
| **エネルギーベースモデル (EBM)** | $p(\mathbf{x}) \\propto \\exp(-E(\mathbf{x}))$ の形で分布を表すモデル。拡散はEBMの現代的サンプリングとして再解釈できる。 |
| **分配関数** | → [［再掲・第9回］](#kw-log-partition) |

### サンプリングと高速化

| キーワード | 説明 |
| --- | --- |
| **DDPM** | SDEとしての離散化による標準サンプラー。多数ステップで確率的に生成。 |
| **DDIM** | 確率フローODEに基づく決定論的サンプラー。ステップ数を削減できる。 |
| **DPM-Solver** | ODE系の高精度ソルバーを用いた高速サンプリング手法。 |

### 球面上の拡散

| キーワード | 説明 |
| --- | --- |
| **球面拡散** | 状態空間を $S^{d-1}$ に制約した拡散。最終状態は球面上の一様分布。 |
| **熱核 (Heat Kernel)** | 球面上のブラウン運動の遷移分布。球面拡散の順過程に対応する。 |
| **von Mises-Fisher (vMF) 分布** | → [［再掲・第9回］](#kw-vmf) |

## 第10回：思考の連鎖 ～推論の軌跡～

一撃の回答ではなく、言語空間内での「推論の歩み」を軌跡として制御する。

| キーワード | 説明 |
| --- | --- |
| **Chain of Thought (CoT)** | 最終回答に至るまでの中間的な思考プロセスを言語化する手法。幾何学的には「意味のジャンプ」を小さな歩幅の軌跡に分解する操作。 |
| **One-shot推論** | 中間ステップを介さず、入力から出力へ直接ジャンプする推論モード。単純・パターン認識的な問題に向く。 |
| **Multi-step推論** | 中間ステップ（足場）を挟み、複数の小さな遷移で答えへ至る推論モード。複雑な論理や算術に向く。 |
| **System 1 / System 2** | 直感的な一撃の回答（S1）と、時間をかけて論理を組み立てる逐次的な推論（S2）の対比。 |
| **推論の結び目** | 言語空間において、論理的な矛盾や袋小路（トラップ）に陥り、正しい軌道から外れてしまう現象。 |
| **Test-time compute scaling** | 推論時の計算量を増やすことで性能が向上する現象。学習時スケーリングとは独立の軸。 |
| **Scratchpad** | 中間計算を明示的に書き出すための自由形式の作業領域。CoTと同様に推論の足場を作る。 |
| **Self-consistency** | 同一問題を複数回解き、多数決や集約で最終解を選ぶ手法。試行回数に応じて計算量が増える。 |
| **Tree of Thoughts** | 思考経路を木として探索し、評価関数で有望な経路を選ぶ探索型推論手法。 |
| **MCTS** | Monte Carlo Tree Search。探索幅と深さを制御しながら経路を評価する探索アルゴリズム。 |
| **Self-reflection / Verifier** | 生成結果を検証し、必要に応じて修正する検証・再生成の枠組み。 |
| **隠れマルコフモデル (HMM)** | 離散的な隠れ状態の遷移で観測系列を生成する古典的モデル。 |
| **マルコフ性** | 現在の状態が直前の状態のみに依存するという仮定。 |
| **Model Collapse** | 自己生成データで再学習することで、多様性が失われる現象。 |
| **Dimensional Collapse** | 表現空間の有効次元が減少し、少数の次元に表現が偏る現象。 |

## 第11回：感覚の統合 ～異種多様体の結婚～

画像、言語、音声といった異なる「形」を持つ多様体同士を、共通の幾何学的空間で結びつける。

| キーワード | 説明 |
| --- | --- |
| **マルチモーダル整列** | 異なるモダリティ（画像とテキストなど）を、共通の潜在空間（共通多様体）へ射影し、意味的な対応をとる操作。 |
| **CLIP** | 対照学習を用い、画像とテキストを共通の「向き」で表現することで、ゼロショット分類などを可能にしたモデル。 |
| **対照学習** | 正例ペアを近づけ、負例ペアを遠ざけるように埋め込みを学習する枠組み。CLIPではInfoNCE損失として実装される。 |
| **InfoNCE損失** | 対照学習の代表的損失。正例の相対確率を最大化し、負例との相対順位を下げる。温度パラメータで分布の鋭さを制御する。 |
| $L_2$ **正規化** | 埋め込みベクトルのノルムを1に揃え、類似度を角度（コサイン）として解釈可能にする操作。 |
| **ゼロショット転移** | 学習時に見ていないカテゴリでも、テキストプロンプトとの類似度で分類できる能力。 |
| **構成的理解** | 属性と物体の組み合わせなど、要素の組成関係を正しく区別する能力。CLIPでは苦手になりやすい。 |
| **モダリティギャップ** | CLIP等の統一空間において、画像群とテキスト群が完全に重ならず、一定の距離（ギャップ）を持って分離してしまう現象。 |
| **FEDA** (Frustratingly Easy Domain Adaptation) | 特徴空間を3倍（共通・ソース固有・ターゲット固有）に拡張し、情報の「棲み分け」を幾何学的な直交化によって実現する古典的手法。 |
| **統一 vs 分離** | 異なるドメインを同じ座標に押し込む（CLIP型）か、別の次元に逃がして干渉を防ぐ（FEDA型）かという設計上の対立軸。 |
| **概念の直交性** | 異なるモダリティが持つ独自の情報の「余分な次元」が、共通空間で互いに干渉しないように配置される幾何学的性質。 |

## 第12回：双曲幾何学 ～負の曲率の世界～

ユークリッド空間には収まりきらない、指数関数的に広がる「階層構造」を扱うための空間。

| キーワード | 説明 |
| --- | --- |
| **負の曲率** | 空間が「外側に反り返った」性質。三角形の内角の和が180°より小さくなる。 |
| **双曲空間** | → [［再掲・第12回］](#kw-hyperbolic-space) |
| **ポアンカレ円板** | 双曲空間をユークリッド平面上の円として投影したモデル。境界に近づくほど距離が無限に伸びる。 |
| **ポアンカレ半平面** | 上半平面で双曲空間を表す等角モデル。理論解析で用いられる。 |
| **ローレンツモデル** | 高次元空間内の双曲面として表すモデル。数値的に安定で実装に向く。 |
| **クラインモデル** | 単位円板内で測地線が直線として表れるモデル。角度は保存されない。 |
| **双曲距離** | <a id="kw-hyperbolic-distance"></a>双曲空間における距離。境界に近いほど同じユークリッド差でも距離が急増する。 |
| **Poincaré Embeddings** | ポアンカレ円板への埋め込みで階層構造を低歪みに表現する手法。 |
| **リーマン最適化** | → [［再掲・第12回］](#kw-riemann-optim) |
| **階層的埋め込み** | 木構造（親子関係）を、双曲空間の「原点からの距離（階層の深さ）」と「角度（分岐）」として自然に表現する手法。 |
| **Mixed-curvature spaces** | <a id="kw-mixed-curvature"></a>球面・双曲・ユークリッドなど複数の曲率を組み合わせた表現空間。 |

## 第13回：高次元の深淵 ～幾何学的な恐怖と祝福～

高次元空間に特有の集中現象と、それを活用・回避する設計指針。

| キーワード | 説明 |
| --- | --- |
| **距離の集中** | → [［再掲・第13回］](#kw-distance-concentration) |
| **赤道集中現象** | 高次元球面では、表面積の大半が赤道付近に集中する性質。 |
| **ランダム直交性** | → [［再掲・第13回］](#kw-random-orthogonality) |
| **内在次元** | → [［再掲・第13回］](#kw-intrinsic-dim) |
| **kNNの破綻** | 距離の集中により、最近傍と最遠傍の差が小さくなり近傍検索が不安定になる問題。 |
| **コサイン類似度** | → [［再掲・第13回］](#kw-cosine-sim) |
| **Hubness問題** | 一部の点が異常に多くの最近傍として選ばれる高次元特有の偏り。 |
| **近似最近傍探索 (ANN)** | LSHやHNSWなどで近傍探索を近似し、大規模データで計算量を抑える手法。 |
| **Product Quantization (PQ)** | 高次元を部分空間に分割して量子化し、検索効率を高める手法。 |
| **MoE (Mixture of Experts)** | <a id="kw-moe"></a>入力に応じて一部のExpertのみを活性化するスパースなモデル。 |
| **ルーティング (Top-k)** | <a id="kw-routing-topk"></a>どのExpertを使うかを決める機構。内積ベースのkNN的選択と解釈できる。 |
| **Expert Collapse** | 一部Expertに負荷が集中し、他が使われなくなる問題。 |
| **Load Balancing Loss** | Expertの利用率を均す補助損失。入力空間の分割を均等化する正則化。 |
| **Shared Expert** | 全入力に常時参加する共通Expert。共通部分空間の導入に相当。 |
| **敵対的サンプル (FGSM)** | 微小摂動で誤分類を引き起こす現象と、その代表的生成法。 |
| **RAG** | 埋め込み空間での近傍検索を用いる外部知識拡張手法。 |

## 第14回：トポロジーという顕微鏡 ～穴とループの発見～

具体的な「距離」ではなく、空間が持つ「つながり（穴やループ）」という不変の性質に注目する。

| キーワード | 説明 |
| --- | --- |
| **トポロジー（位相幾何学）** | 連続変形で保存される「つながり」や「穴」の性質を扱う数学分野。 |
| **同相（ホームオモルフィズム）** | 切ったり貼ったりせずに連続変形できる関係。位相的に同じことを意味する。 |
| **ベッチ数** | 各次元の穴の数を表す不変量。 $\beta_0$ は連結成分、 $\beta_1$ はループ、 $\beta_2$ は空洞。 |
| **持続的ホモロジー** | データのスケールを変化させながら、そこに出現・消失する「穴（不変量）」を追跡し、データの真の構造を探る手法。 |
| **バーコード / パーシステンス図** | 持続的ホモロジーの計算結果を、穴の「寿命」として可視化した図。データの幾何学的指紋となる。 |
| **TDA (トポロジカルデータアナリシス)** | <a id="kw-tda"></a>位相幾何学の道具を用いて、高次元データの複雑な形状や異常（穴の欠損など）を分析する手法。 |
| **フィルトレーション** | スケールを変えながら単体複体を増やしていく入れ子列。パーシステントホモロジーの基盤。 |
| **単体複体** | 点・辺・三角形などの単体で構成された離散的な幾何構造。 |
| **Vietoris–Rips複体** | 距離閾値内の点対から辺を張り、クリークで高次元単体を作る簡便な複体。 |
| **Čech複体** | 各点の球の共通部分が非空であるときに単体を作る複体。幾何学的に厳密。 |
| **持続性（persistence）** | 穴の寿命（death − birth）。対角線からの距離として解釈される。 |

幾何学的な洞察から、未来のAIアーキテクチャを展望する。

| キーワード | 説明 |
| --- | --- |
| **多様体の純度** | 表現空間に格納されたデータが、論理的・事実的にどれだけ整合（クリーン）であるかという、幾何学の次の課題。 |
| **幾何学的帰納バイアス** | 空間の対称性や曲率をモデルの設計に組み込むことで、少ないデータから効率的に学習させる指針。 |

## 第15回：次の時代を設計する

未解決問題と未来の設計課題を整理し、次の座標系を構想する最終回。

| キーワード | 説明 |
| --- | --- |
| **統一多様体** | 複数モダリティを単一の空間に埋め込めるかという問い。 |
| **モダリティ・ブリッジ** | 異なる曲率・次元の空間同士を翻訳・接続する発想。 |
| **離散と連続の界面設計** | argmaxやサンプリングに代わる、より滑らかな離散化の可能性。 |
| **統合情報理論 (IIT)** | 意識を「情報の統合度」として定量化しようとする理論。 |
| **自己参照ループ** | 表現空間が自分自身を埋め込む構造としての意識仮説。 |
| **学習可能な曲率** | タスクに応じて曲率そのものを最適化する試み。 |
| **可変曲率多様体** | 空間内で曲率が滑らかに変化する設計。 |
| **Mixed-curvature spaces** | → [［再掲・第15回］](#kw-mixed-curvature) |
| **Soft MoE** | 離散的ルーティングを連続化するMixture of Expertsの拡張。 |
| **連続的Expert場** | Expertを無限に増やした極限としての連続空間的解釈。 |
| **幾何学的説明可能性** | 判断経路・射影・測地線などで説明を与える視点。 |
| **低次元射影の限界** | 高次元多様体の本質を保った可視化が難しいという課題。 |

## Appendix 1: 量子化の幾何学

### 量子化の基本概念と目的

連続的な数値を限られたビット数の離散値へ変換する技術とその動機。

| キーワード | 説明 |
| --- | --- |
| **ビット精度** (FP32 / INT4 / 1-bit) | 32ビット浮動小数点から1ビット（2値）まで、重みを表現する情報の細かさ。 |
| **メモリ削減** | 数十億〜数千億のパラメータを持つLLMにおいて、消費メモリを劇的に抑える主要な目的。 |
| **推論高速化** | データ転送量の削減や、専用カーネル（Tensor Core等）の活用による計算の効率化。 |
| **KV cache 量子化** | 長文生成時にメモリを圧迫するKey/Valueキャッシュの容量を削減する実務的アプローチ。 |
| **アクティベーション量子化** | 入力に依存する中間表現を量子化する手法。キャリブレーションや分布変動の影響が大きい。 |

### 量子化の幾何学的解釈

重み空間や表現空間が離散化されることで生じる形状の変化。

| キーワード | 説明 |
| --- | --- |
| **重み空間の格子化** | 連続的な空間が離散的な格子点に制限される操作。幾何学的には「射影」と見なせる。 |
| **量子化誤差** $\epsilon = w - \hat{w}$ | 元の値と格子点への射影による「ずれ」。 |
| **決定境界の硬直化** | 柔軟だった境界が、重みの離散化によって階段状や硬直した形状へ変化する現象。 |
| **表現多様体のピクセル化** | 中間層の表現が分布する多様体の局所的な分解能が低下すること。 |
| **球面上の格子** | nGPTのような球面制約下での量子化。ノルム1を維持するための再正規化などの課題を伴う。 |

### 量子化の手法とアルゴリズム

幾何学的な歪みを最小限に抑えつつ離散化を行うための具体的な技術。

| キーワード | 説明 |
| --- | --- |
| **PTQ** (Post-Training Quantization) | 学習済みモデルを事後的に量子化する手法。GPTQやAWQなどが含まれる。 |
| **QAT** (Quantization-Aware Training) | 量子化の影響を考慮しながら再学習する手法。STE（ストレートスルー推定量）が利用される。 |
| **最近傍丸め (Round-to-Nearest)** | 最も近い格子点へ飛ばす、最も単純な最短距離射影。 |
| **SmoothQuant** | 重みとアクティベーションの分散を座標変換によって均等化し、量子化しやすくする手法。 |
| **BitNet / 1.58-bit** | 重みを{-1, 1}や{-1, 0, 1}に制限する極端な量子化。行列積を加減算に置き換える。 |
| **per-channel / group-wise 量子化** | チャンネルやグループ単位でスケールを持つ非一様な量子化。重要領域の歪みを抑える狙い。 |
| **Mixed-Precision Quantization** | 層や成分ごとに異なるビット幅を割り当て、精度と効率を両立させる設計。 |

### 誤差分析と頑健性

量子化による性能劣化を幾何学的・構造的な視点から評価する指標。

| キーワード | 説明 |
| --- | --- |
| **層ごとの感度** | 埋め込み層や出力層は感度が高く、FFN層は比較的頑健であるといった、多様体の曲率や役割による依存性。 |
| **過剰パラメータ化** | モデルの冗長性により、多少の量子化誤差が相殺され性能が維持される背景。 |
| **キャリブレーション** | 量子化パラメータ（スケール等）を決定するために少量のデータで分布を確認する工程。 |
| **Hausdorff距離** | 量子化前後で決定境界がどれだけ「ずれた」かを測る幾何学的指標。 |
| **フロベニウスノルム** | 重み空間での誤差 $\|W - \hat{W} \| _F$ を測る代表的な行列ノルム。 |

## Appendix 2: 多様体の純度問題: 幾何学がまだ解けていない課題

### 多様体の「純度」とその定義

幾何学的な空間設計（形式）と、そこに格納されるデータの中身（内容）の乖離に関する概念。

| キーワード | 説明 |
| --- | --- |
| **純度** | 表現空間において、論理的・事実的な整合性に反するサンプルがどの程度混入し、学習結果に影響を与えているかの指標。 |
| **混入率** | 問題のあるサンプルが学習分布にどの程度含まれているかの割合。 |
| **形式の安定と内容の真偽** | 正規化（球面化）は計算形式を安定させるが、その方向が「正しい」かどうか（真偽）には中立であるという本質的限界。 |
| **「正しさ」の3類型** | 論理整合性（推論規則）、事実整合性（世界知識）、規範整合性（価値判断）の区別。 |
| **影響度** | 特定のサンプルを除去・変更した際の、検証誤差や推論安定性の変化量。 |

### 既存のデータ選別手法と幾何学的解釈

現在利用されているデータクレンジング手法を、幾何学的な測定量の視点で再定義したもの。

| キーワード | 説明 |
| --- | --- |
| **表現距離と損失距離** | 埋め込み空間での近さ（表現距離）と、予測誤差の大きさ（損失距離/EL2N）の区別。 |
| **EL2Nスコア** | 予測誤差の大きさを「距離」の代理指標としてデータを選別する手法。 |
| **Influence Functions** | データ除去時の損失変化をHessian近傍の「曲率」として逆算し、影響度を測る手法。 |
| **kNN外れ値検出** | 表現空間における「密度」の低さを利用して、疎領域のデータを検出する手法。 |
| **TDA** | → [［再掲・Appendix 2］](#kw-tda) |

### 幾何学的選別の障壁と限界

幾何学単独ではデータの真偽を判定できない原理的な理由。

| キーワード | 説明 |
| --- | --- |
| **原理的な識別不能性** | 幾何学は「形（内在的性質）」を測るが、そこに付与された「意味（外部参照）」は測れないという制約。 |
| **論理と幾何の断絶** | 幾何学的な「三角不等式」が、論理学的な「含意（推移律）」を必ずしも保証しないという問題。 |
| **鶏と卵問題** | 異常検知の基準となる「正しい多様体」自体が、汚れたデータで学習されているという循環依存。 |
| **次元の呪い** | 高次元空間において距離の集中や推定誤差の爆発が起き、測定量の信頼性が低下する現象。 |

### 将来の研究方向と補完技術

幾何学の限界を乗り越え、データの純度を高めるためのアプローチ。

| キーワード | 説明 |
| --- | --- |
| **整合性ベンチ** | 推移律違反率、矛盾テスト、事実検証成功率など、論理・事実整合性を評価する指標群。 |
| **性能ベンチ** | 一般タスク精度、ロバスト性、推論安定性など、影響度の観点で評価する指標群。 |
| **Tool Use（外部検証）** | 計算エンジンや検索エンジンを用い、幾何学の外側で事実を検証する必然性。 |
| **合成データ** | 論理ルールに基づき、最初から「正しい」多様体を設計・生成する手法。 |
| **論理制約の埋め込み学習** | 推移律などの論理規則を損失関数に組み込み、表現距離に制約を与える試み。 |
| **動態論への接続** | 静的な「配置の歪み」としての異常を、推論時の「軌道のトラップ（幻覚）」として捉え直す動的な視点。 |

## Appendix 3: 動的剪定の幾何学: 柔軟な回路がもたらす知能

入力に応じて計算経路や情報の流れが変わる、動的な構造としてのAttentionとMoEを整理する。

### 動的な経路選択

| キーワード | 説明 |
| --- | --- |
| **動的剪定** | 入力に応じて、情報の流れや計算経路を動的に変える設計思想。 |
| **情報的剪定** | 標準Attentionのように、重み付けで寄与を弱める「見かけの剪定」。 |
| **計算的剪定** | Sparse AttentionやMoEのように、計算自体を省く剪定。 |
| **ルーティング** | → [［再掲・Appendix 3］](#kw-routing-topk) |

### スパース化とMoE

| キーワード | 説明 |
| --- | --- |
| **Sparse Attention** | → [Top-k参照](#kw-routing-topk) |
| **MoE** | → [［再掲・Appendix 3］](#kw-moe) |
| **直交部分空間仮説** | Expertが干渉しない部分空間を担当するという理想化された仮説。 |

### スパース性の条件

| キーワード | 説明 |
| --- | --- |
| **高次元空間の空虚さ** | 高次元ではランダムベクトル同士がほぼ直交し、多くの方向が「空」に近いという性質。 |
| **信号対雑音比 (SNR)** | 有効信号と無関係ノイズの比率。スパース化が有効かどうかの直感的指標として用いる。 |

### MoEの安定化

| キーワード | 説明 |
| --- | --- |
| **ルーティング崩壊 (Routing Collapse)** | トークンが特定のExpertに集中し、他のExpertが使われなくなる現象。 |
| **Capacity Factor** | 各Expertが受け入れるトークン数の上限。過負荷を防ぐための上限設定。 |
| **Expert Choice Routing** | トークンがExpertを選ぶのではなく、Expert側がTop-Kでトークンを選ぶ方式。 |

### 効率化手法

| キーワード | 説明 |
| --- | --- |
| **GQA (Grouped-Query Attention)** | 複数のQueryヘッドでKey/Valueを共有し、KVヘッド数を削減する設計。 |
| **KVキャッシュ** | 自己回帰生成で過去トークンのKey/Valueを再利用するための保存領域。 |
| **Memory Wall** | メモリ容量・帯域の制約が計算性能のボトルネックになる問題。 |
| **LoRA (Low-Rank Adaptation)** | 低ランク分解で重み更新の自由度を制限し、効率的に微調整する手法。 |
| **MoD (Mixture of Depths)** | トークンごとに通過層数を動的に変え、深さ方向の計算を剪定する設計。 |

### メモリI/O最適化

| キーワード | 説明 |
| --- | --- |
| **FlashAttention** | タイリング・オンラインSoftmax・再計算を組み合わせ、HBM↔SRAM I/Oを削減する注意機構。 |
| **HBM** | GPUの外部高帯域メモリ。容量は大きいがレイテンシが高い。 |
| **SRAM** | GPUオンチップの高速メモリ（共有メモリやL1相当）。容量が小さい。 |
| **タイリング** | 行列を小さなブロックに分割し、オンチップに載せて計算する手法。 |
| **オンラインSoftmax** | Softmaxを段階的に更新し、巨大な中間行列の保存を回避する手法。 |
| **再計算 (Recomputation)** | 中間結果を保存せず、必要時に再計算してメモリ使用量を削減する戦略。 |

## Appendix 4: 空間の「物差し」再考: 2点間から情報の密度まで

距離・内積・ダイバージェンス・情報計量という「測り方」を整理し、情報幾何の見通しを与える。

### 対称的な尺度（距離と内積）

| キーワード | 説明 |
| --- | --- |
| **ユークリッド距離** | 2点間の直線距離。対称かつ三角不等式を満たす。 |
| **内積** | ベクトルの大きさと方向を同時に測る基本量。 |
| **コサイン類似度** | → [［再掲・Appendix 4］](#kw-cosine-sim) |
| **測地距離** | 曲がった空間での最短経路に沿った距離。 |
| **双曲距離** | → [［再掲・Appendix 4］](#kw-hyperbolic-distance) |
| **Poincaré球モデル** | 双曲空間を単位球内に表すモデル。境界に近いほど距離が拡大する。 |

### 非対称な尺度（情報の隔たり）

| キーワード | 説明 |
| --- | --- |
| **KLダイバージェンス** | → [［再掲・Appendix 4］](#kw-kl-divergence) |
| **Forward KL** | 真の分布から見たモデルのズレを測るKL。 |
| **Reverse KL** | モデルから見た真の分布のズレを測るKL。 |
| **フィッシャー情報行列** | → [［再掲・Appendix 4］](#kw-fisher-info) |
| **自然勾配法** | フィッシャー情報行列に基づき最急降下方向を補正する手法。 |

### 情報計量（感度の尺度）

| キーワード | 説明 |
| --- | --- |
| **情報密度** | フィッシャー情報行列が表す「パラメータ変化に対する感度」を比喩的に表した用語。 |
| **計量テンソル** | 各点での距離測定を定めるテンソル。フィッシャー情報行列が担う。 |
| **K-FAC** | フィッシャー情報行列をKronecker分解で近似する自然勾配の近似手法。 |
