# 第3回：プラネタリウムの建設 ～極座標へのパラダイムシフト～

## 注意事項

本回で扱う「球面上の学習」には、成立条件がある：

- 「球面上の学習」が厳密に成り立つのは、すべてのベクトルが **$L_2$ 正規化（ノルム1）** されている場合である。
- nGPTはこれを設計として徹底するが、標準的なTransformerは必ずしもそうではない。
- vMF分布の解釈は、埋め込みを**方向分布として明示的にモデル化**した場合に有効であり、すべての埋め込み空間に自動的に当てはまるわけではない。
- 「球面上で考えれば常に良い」わけではなく、タスクやデータの性質によっては、ノルムに意味がある場合もある。

## Arsenal（この回の武器庫）

| 項目 | 内容 |
| --- | --- |
| **Key Concepts** | 球面、$L_2$正規化、vMF分布、測地線距離、コサイン類似度 |
| **Key Metaphor** | プラネタリウム（ドーム上の星座）、極座標（半径と角度の分離） |
| **Key Question** | ノルムを「捨てる」のではなく「固定する」と何が変わるのか |
| **Key Implementation** | 正規化のタイミング、数値安定性、勾配の扱い |

## トピック

ノルム1制約と球面多様体の発見

---

## 導入：平面から球面へ

第2回で、ノルムが「三重の意味」を担っていたことを見た。意味の強度、確信度、スケーリングノイズ——これらが混在し、制御が困難だった。

一つの解決策は、**ノルムを固定する**ことである。すべてのベクトルを単位球面上（ノルム1）に制約すれば、ノルムは定数となり、「向き」だけが自由度として残る。

これは単なる正規化テクニックではない。**空間そのものを変える**という幾何学的な決断である。ユークリッド空間 $\mathbb{R}^d$ から、単位球面 $S^{d-1}$ へ。平らな世界から、曲がった世界へ。

本回では、この「球面への跳躍」を、nGPTの設計思想、プラネタリウムというメタファー、そしてvon Mises-Fisher分布という数学的道具を通じて理解する。

---

## nGPTの衝撃：設計の徹底

### nGPTとは何か

**nGPT（Normalized GPT）** は、2024年10月にNVIDIAのLoshchilovらが発表したアーキテクチャである（Loshchilov et al., 2024）。その核心的なアイデアは、**Transformerのすべての表現を単位球面上に制約する**ことである。

具体的には：

- 埋め込みベクトル：正規化
- 注意機構の出力：正規化
- FFNの出力：正規化
- 残差接続後：正規化

これにより、**同等の精度に達するまでの学習ステップ数を4〜20倍削減**したと報告されている（シーケンス長に依存）。

> [!NOTE]
> **「高速化」の意味：** nGPTの主張は「学習ステップ数の削減」であり、壁時計時間（実時間）やFLOPsがそのまま4〜20倍改善されるとは限らない。正規化のオーバーヘッドがあるため、1ステップあたりの計算コストは増加する可能性がある。総合的な効率改善の程度は、実装やハードウェアに依存する。

### 「正規化」と「球面制約」の違い

ここで重要な区別がある。

**従来の正規化（例：LayerNorm）：**
- 統計量（平均・分散）を正規化
- ベクトルのノルムは必ずしも1にならない
- 「スケールを安定させる」が目的

**nGPTの球面制約：**
- ベクトルのノルムを厳密に1に固定（$L_2$正規化）
- すべての演算後に再正規化
- 「球面上で計算する」が目的

> [!IMPORTANT]
> **設計の徹底が本質：** 単に `F.normalize` を呼ぶだけでは、球面上の学習にはならない。行列積や加算などの演算を行うと、結果は一般に球面から外れる。nGPTは、**更新で球面から外れたら正規化で戻す**ことで、表現が常に球面上にあることを保証する。（多様体最適化の文脈では、こうした「多様体へ戻す写像」を retraction と呼ぶ。）この「球面制約の維持」が、単なる正規化との違いである。

### なぜ高速化するのか（仮説）

nGPTが高速化する理由について、いくつかの仮説が考えられる：

| 仮説 | 説明 |
| --- | --- |
| **勾配の安定化** | ノルムが固定されると、勾配のスケールも安定する |
| **損失ランドスケープの改善** | 球面上では、局所解の構造が単純になる可能性 |
| **学習率の設定が容易** | ノルムが一定なので、学習率の調整が楽になる |

> [!CAUTION]
> **仮説段階：** 上記は提案されている説明であり、厳密に証明されたメカニズムではない。nGPTの高速化の正確な原因は、さらなる研究が必要である。

### 歴史的文脈：なぜ2010年代には普及しなかったか

球面制約という発想自体は新しくない。例えば、NormFace（Wang et al., 2017）は顔認証において$L_2$正規化の有効性を示している。しかし、Transformer全体に球面制約を適用する設計は、2010年代には広く普及しなかった。その理由として、以下が考えられる：

1. **スケーリングと損失設計の非自明さ：** NormFaceでも指摘されているように、正規化を入れた学習は単純ではなく、スケールや損失関数の設計に工夫が必要だった（Wang et al., 2017）
2. **最適化技術との組み合わせ：** 球面制約を維持しながら効率的に最適化する実践的なレシピが確立されていなかった
3. **相対的なオーバーヘッド：** 当時のモデル規模では、正規化の計算コストが相対的に大きかった可能性がある

> [!CAUTION]
> **歴史的推測：** 上記は「なぜ普及しなかったか」についての推測であり、確定的な因果関係ではない。球面上の最適化自体は、多様体最適化（Absil et al., 2008; Bonnabel, 2013）として理論的には研究されていた。nGPTの貢献は、これをTransformerに適用し、大規模言語モデルで有効性を実証した点にある。

現在は、以下の技術が整っていることが、球面制約の実用化を後押ししている可能性がある：

- **AdamWなどの最適化手法：** 重み減衰と適応的学習率の組み合わせ
- **残差接続：** 勾配の流れを安定させる
- **混合精度学習：** 計算効率の向上
- **大規模GPUメモリ：** より大きなバッチサイズが可能に

---

## プラネタリウム・メタファー

### 球面を「ドーム」として捉える

nGPTの設計を直感的に理解するため、**プラネタリウム**というメタファーを導入しよう。

プラネタリウムでは、星々がドーム（半球面）に投影される。私たちは地上からドームを見上げ、星の「配置」——星座——を認識する。星までの距離は（投影されているので）問題にならない。重要なのは、星同士の**相対的な位置関係（角度）** である。

このメタファーを表現学習に適用する：

| プラネタリウム | 表現学習 |
| --- | --- |
| ドーム | 単位球面 $S^{d-1}$ |
| 星 | データ点（埋め込みベクトル） |
| 星座 | 意味的なクラスター |
| 星同士の角度 | 類似度 |
| 星の明るさ | （この比喩では使わない——ノルムを捨てた） |

### 角度が意味を担う

プラネタリウムでは、二つの星の「近さ」は角度で測られる。球面上の角度（測地線距離）は、単位ベクトルの内積と直接対応する：

$$\cos\theta = \mathbf{u}^\top \mathbf{v} \quad (\|\mathbf{u}\| = \|\mathbf{v}\| = 1)$$

- $\cos\theta = 1$（$\theta = 0°$）：同じ方向、完全に類似
- $\cos\theta = 0$（$\theta = 90°$）：直交、無関係
- $\cos\theta = -1$（$\theta = 180°$）：反対方向、対立

これが**コサイン類似度**の幾何学的意味である。

> [!NOTE]
> **単位正規化が前提：** コサイン類似度の式 $\frac{\mathbf{u}^\top \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}$ は、一般のベクトルに対して定義される。しかし、$\|\mathbf{u}\| = \|\mathbf{v}\| = 1$ のとき、これは単に内積 $\mathbf{u}^\top \mathbf{v}$ になる。nGPTのように球面制約がある場合、コサイン類似度と内積は同一である。

### 測地線距離

球面上の「最短経路」は測地線（大円の弧）である。二点間の測地線距離は：

$$d_{\text{geo}}(\mathbf{u}, \mathbf{v}) = \arccos(\mathbf{u}^\top \mathbf{v}) = \theta$$

これは角度そのものである。第0回で導入した測地線の概念が、ここで具体化される。

---

## von Mises-Fisher分布：球面上の「正規分布」

### なぜ球面上の確率分布が必要か

プラネタリウムで「星座」を表現するには、単に点を配置するだけでなく、**点の分布**を考える必要がある。

ユークリッド空間では、正規分布（ガウス分布）が「最も自然な」分布として使われる。では、球面上では何が対応するのか。

その答えが **von Mises-Fisher (vMF) 分布** である。

### vMF分布の定義

$d$ 次元単位球面 $S^{d-1}$ 上のvMF分布は、以下の確率密度関数を持つ：

$$p(\mathbf{x} \mid \boldsymbol{\mu}, \kappa) = C_d(\kappa) \exp(\kappa \boldsymbol{\mu}^\top \mathbf{x})$$

ここで：

| パラメータ | 意味 | プラネタリウムでの解釈 |
| --- | --- | --- |
| $\boldsymbol{\mu}$ | 平均方向（単位ベクトル） | 星座の「中心」 |
| $\kappa \geq 0$ | 集中度パラメータ | 星の「まとまり具合」 |
| $C_d(\kappa)$ | 正規化定数 | （確率の和が1になるよう調整） |

正規化定数は：

$$C_d(\kappa) = \frac{\kappa^{d/2 - 1}}{(2\pi)^{d/2} I_{d/2-1}(\kappa)}$$

ここで $I_\nu$ は第一種変形ベッセル関数。

### 集中度 $\kappa$ の直感

$\kappa$ は「分布の鋭さ」を制御する：

| $\kappa$ の値 | 分布の形状 | プラネタリウムでの解釈 |
| --- | --- | --- |
| $\kappa = 0$ | 球面上の一様分布 | 星がドーム全体にランダムに散らばる |
| $\kappa$ 小（例：1〜5） | 緩やかに集中 | 星座がぼんやり見える |
| $\kappa$ 中（例：10〜50） | 明確に集中 | 星座がはっきり見える |
| $\kappa \to \infty$ | 一点に集中（デルタ関数に近づく） | 星が一点に凝縮 |

### vMFと正規分布の対応

vMF分布は、多くの点で正規分布と類似している：

| 正規分布（$\mathbb{R}^d$） | vMF分布（$S^{d-1}$） |
| --- | --- |
| 平均 $\boldsymbol{\mu}$ | 平均方向 $\boldsymbol{\mu}$ |
| 分散 $\sigma^2$（または共分散） | 集中度の逆数 $1/\kappa$ |
| 等方的な広がり | 球面上での等方的な広がり |
| 指数関数的な減衰 | 指数関数的な減衰（角度に対して） |

> [!NOTE]
> **厳密な対応ではない：** vMFは「球面上の正規分布」と呼ばれることがあるが、これは比喩的な表現である。数学的には、vMFは最大エントロピー原理から導かれる分布であり、正規分布とは異なる性質も持つ。

### Softmaxとの関係

興味深いことに、vMF分布の形式は**Softmax関数**と密接に関連している。

**前提条件：** 以下の対応が成り立つのは、次の条件が満たされる場合である：
- クラス $k$ に対応する代表ベクトル $\boldsymbol{\mu}_k$ が**単位ノルム**
- 入力の埋め込み $\mathbf{x}$ も**単位ノルム**
- ロジットが**内積**（またはスケール付き内積）として計算される

この条件下で：

$$p(k \mid \mathbf{x}) \propto \exp(\kappa \boldsymbol{\mu}_k^\top \mathbf{x})$$

これは、温度 $T = 1/\kappa$ のSoftmax関数と等価である：

$$p(k \mid \mathbf{x}) = \frac{\exp(\boldsymbol{\mu}_k^\top \mathbf{x} / T)}{\sum_j \exp(\boldsymbol{\mu}_j^\top \mathbf{x} / T)}$$

> [!CAUTION]
> **前提が崩れる場合：** 標準的なTransformerの分類ヘッドでは、出力層の重み行列や入力埋め込みが正規化されていないことが多い。この場合、ロジットは内積ではなく一般の線形変換となり、vMFとの対応は厳密には成り立たない。nGPTのように球面制約を徹底する場合に、この対応が意味を持つ。

> [!IMPORTANT]
> **第4回への伏線：** この対応は、Softmaxを情報幾何学の観点から再解釈する際の出発点となる。温度パラメータ $T$（または $\kappa$）が、確率分布空間でどのような役割を果たすかを、次回詳しく見る。

---

## 球面上の学習：幾何学的な意味

### 勾配の方向

球面上で最適化を行うとき、勾配は球面に**接する方向**でなければならない。球面から離れる方向（半径方向）への勾配成分は、制約を破るからである。

具体的には、点 $\mathbf{x} \in S^{d-1}$ における接空間は：

$$T_\mathbf{x} S^{d-1} = \{\mathbf{v} \in \mathbb{R}^d : \mathbf{v}^\top \mathbf{x} = 0\}$$

すなわち、$\mathbf{x}$ と直交するベクトルの集合。

ユークリッド空間での勾配 $\nabla f(\mathbf{x})$ を接空間に射影するには：

$$\text{grad}_S f(\mathbf{x}) = \nabla f(\mathbf{x}) - (\nabla f(\mathbf{x})^\top \mathbf{x}) \mathbf{x}$$

これが**リーマン勾配**（球面上の勾配）である。

### 正規化による近似

実際の実装では、リーマン勾配を明示的に計算する代わりに、以下の近似がよく使われる：

1. 通常のユークリッド勾配で更新
2. 結果を球面に射影（正規化）

```python
# 疑似コード
x = x - lr * grad  # ユークリッド空間での更新
x = x / x.norm()   # 球面への射影
```

これは「射影勾配法」の一種であり、学習率が十分小さければ、リーマン勾配降下法に近い挙動を示す。

> [!CAUTION]
> **近似の限界：** 学習率が大きい場合、この近似は不正確になる。また、運動量を使う場合、運動量ベクトルも接空間上にあるべきだが、この近似ではそれが保証されない。より厳密なリーマン最適化手法については、Bonnabel (2013) などを参照。

---

## 実装ノート：正規化の実践

> [!NOTE]
> 以下のコードは PyTorch >= 1.9 を前提とする。

### 基本的な正規化

```python
import torch
import torch.nn.functional as F

def l2_normalize(x, dim=-1, eps=1e-12):
    """L2正規化（球面への射影）
    
    Args:
        x: 入力テンソル
        dim: 正規化する次元
        eps: 数値安定性のための小さな値
    
    Returns:
        単位ノルムに正規化されたテンソル
    """
    return F.normalize(x, p=2, dim=dim, eps=eps)

# 使用例
embeddings = torch.randn(32, 768)  # バッチサイズ32、次元768
normalized = l2_normalize(embeddings)

# 確認：ノルムが1になっている
print(f"ノルムの平均: {normalized.norm(dim=-1).mean():.6f}")  # ≈ 1.0
print(f"ノルムの標準偏差: {normalized.norm(dim=-1).std():.6f}")  # ≈ 0.0
```

### 数値安定性の問題

正規化には数値安定性の問題がある：

```python
def demonstrate_instability():
    """ノルムが極端に小さい場合の問題"""
    
    # 非常に小さいベクトル
    tiny = torch.tensor([1e-20, 1e-20, 1e-20])
    
    # eps なしで正規化すると...
    try:
        normalized_unsafe = tiny / tiny.norm()
        print(f"Unsafe: {normalized_unsafe}")  # NaN または Inf
    except:
        print("Unsafe normalization failed")
    
    # eps ありで正規化
    normalized_safe = F.normalize(tiny, eps=1e-12)
    print(f"Safe: {normalized_safe}")  # 有限の値

demonstrate_instability()
```

### nGPT風の設計：球面制約の維持

```python
import torch.nn as nn

class NormalizedLinear(nn.Module):
    """正規化を含む線形層（nGPT風）"""
    
    def __init__(self, in_features, out_features, eps=1e-12):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features, bias=False)
        self.eps = eps
        
        # 重みも正規化して初期化
        with torch.no_grad():
            self.linear.weight.data = F.normalize(
                self.linear.weight.data, dim=1, eps=eps
            )
    
    def forward(self, x):
        # 入力は正規化済みと仮定
        out = self.linear(x)
        # 出力を正規化
        return F.normalize(out, dim=-1, eps=self.eps)

class NormalizedResidual(nn.Module):
    """正規化を含む残差接続（nGPT風）"""
    
    def __init__(self, module, alpha=0.1, eps=1e-12):
        super().__init__()
        self.module = module
        self.alpha = alpha  # 残差のスケール
        self.eps = eps
    
    def forward(self, x):
        # 残差を加算
        out = x + self.alpha * self.module(x)
        # 正規化して球面に戻す
        return F.normalize(out, dim=-1, eps=self.eps)
```

> [!IMPORTANT]
> **概念と実装のギャップ：** 上記のコードは「nGPT風」の設計を示すものであり、nGPTの正確な実装ではない。nGPTの詳細は原論文（Loshchilov et al., 2024）を参照されたい。

### vMF分布の可視化

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import iv  # ベッセル関数

def vmf_pdf_2d(theta, kappa):
    """2次元（円上）のvMF分布の確率密度
    
    平均方向を θ=0 として、角度 θ での密度を返す
    """
    # 2次元の場合、正規化定数は 1/(2π I_0(κ))
    normalization = 1 / (2 * np.pi * iv(0, kappa))
    return normalization * np.exp(kappa * np.cos(theta))

# 異なる κ での分布を可視化
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 左：確率密度のプロット
theta = np.linspace(-np.pi, np.pi, 1000)
kappas = [0, 1, 5, 10, 50]

ax = axes[0]
for kappa in kappas:
    if kappa == 0:
        # κ=0 は一様分布
        pdf = np.ones_like(theta) / (2 * np.pi)
    else:
        pdf = vmf_pdf_2d(theta, kappa)
    ax.plot(theta, pdf, label=f'κ = {kappa}')

ax.set_xlabel('Angle θ (radians)')
ax.set_ylabel('Probability Density')
ax.set_title('von Mises-Fisher Distribution (2D)')
ax.legend()
ax.set_xlim(-np.pi, np.pi)

# 右：極座標でのプロット（円上の分布）
ax = axes[1]
for kappa in kappas:
    if kappa == 0:
        r = np.ones_like(theta) / (2 * np.pi)
    else:
        r = vmf_pdf_2d(theta, kappa)
    # 極座標での x, y
    x = r * np.cos(theta)
    y = r * np.sin(theta)
    ax.plot(x, y, label=f'κ = {kappa}')

ax.set_aspect('equal')
ax.set_title('vMF Distribution on Circle')
ax.legend()

plt.tight_layout()
plt.show()
```

---

## 球面設計の限界と注意点

### ノルムに意味がある場合

球面制約は万能ではない。ノルムが意味を持つタスクでは、ノルムを捨てることで情報が失われる：

| タスク | ノルムの意味 | 球面制約の適否 |
| --- | --- | --- |
| 分類（表現の比較） | 不要なことが多い | ✓ 適している |
| 回帰（連続値の予測） | 出力の大きさが重要 | ✗ 適さない |
| 不確実性推定 | ノルムが確信度を表す場合 | △ タスク依存 |
| 生成モデル | 分布のスケールが重要 | △ 設計次第 |

### 球面と超球面の混同

「球面」という言葉には注意が必要：

| 用語 | 数学的定義 | 例 |
| --- | --- | --- |
| 1次元球面 $S^1$ | $\mathbb{R}^2$ 内の円 | 単位円 |
| 2次元球面 $S^2$ | $\mathbb{R}^3$ 内の球面 | 地球表面 |
| $(d-1)$次元球面 $S^{d-1}$ | $\mathbb{R}^d$ 内の超球面 | 高次元の単位球面 |

768次元の埋め込みを正規化すると、それは $S^{767}$ 上の点になる。

### 曲率の影響

球面は**正の曲率**を持つ多様体である。第0回で触れたように、正曲率空間では三角形の内角の和が180°を超える。

この曲率は、学習ダイナミクスに影響を与える可能性がある。例えば、球面上では「遠い二点を直線で結ぶ」ことができない（測地線は曲がる）。

> [!NOTE]
> **双曲空間との対比：** 第12回では、負の曲率を持つ双曲空間を扱う。階層構造の表現には、球面より双曲空間が適していることがある。空間の曲率は、表現したい構造に応じて選ぶべきである。

---

## 参考文献

### nGPTと球面上の学習

- Loshchilov, I., Hsieh, C.-P., Sun, S., & Ginsburg, B. (2024). nGPT: Normalized Transformer with Representation Learning on the Hypersphere. *arXiv:2410.01131*.
  - nGPTの原論文。Transformerのすべての表現を単位球面上に制約する設計。

- Wang, F., Xiang, X., Cheng, J., & Yuille, A. L. (2017). NormFace: L2 Hypersphere Embedding for Face Verification. *ACM MM 2017*. arXiv: [arXiv:1704.06369](https://arxiv.org/abs/1704.06369).
  - 顔認証における球面埋め込みの先駆的研究。

### von Mises-Fisher分布

- Mardia, K. V., & Jupp, P. E. (2000). *Directional Statistics*. Wiley.
  - 方向統計学の標準的教科書。vMF分布を含む球面上の確率分布を体系的に扱う。

- Banerjee, A., Dhillon, I. S., Ghosh, J., & Sra, S. (2005). Clustering on the Unit Hypersphere using von Mises-Fisher Distributions. *JMLR*, 6, 1345–1382.
  - vMF分布を用いたクラスタリング。機械学習への応用。

### リーマン最適化

- Bonnabel, S. (2013). Stochastic Gradient Descent on Riemannian Manifolds. *IEEE Transactions on Automatic Control*, 58(9), 2217–2229.
  - 多様体上の確率的勾配降下法の理論的基礎。

- Absil, P.-A., Mahony, R., & Sepulchre, R. (2008). *Optimization Algorithms on Matrix Manifolds*. Princeton University Press.
  - 行列多様体（Stiefel多様体、Grassmann多様体など）上の最適化。球面も特殊ケースとして含む。

---

## 次回予告

第4回「分類の再統一 I」では、Softmaxを情報幾何学の観点から再解釈する。

vMF分布とSoftmaxの関係を深掘りし、「温度」パラメータの幾何学的意味を理解する。さらに、自然勾配がなぜ有効なのかを、確率分布空間の構造から説明する。
