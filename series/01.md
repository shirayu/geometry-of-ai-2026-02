# 第1回：かつての地図 ～平らな世界で戦っていた私たち～

## 注意事項

本回は歴史的文脈の整理であり、幾何学的主張の厳密な条件は問わない。
「ユークリッド空間前提」という当時の暗黙の仮定を意識し、何が見落とされていたかを振り返ることが目的である。

## 導入：平らな世界の住人たち

深層学習以前の機械学習には、ある暗黙の前提があった。**データはユークリッド空間に住んでいる**、という仮定である。

ユークリッド空間とは、私たちが日常で経験する「平らな」空間だ。直線は直線のまま、平行線は交わらず、三角形の内角の和は180°。ピタゴラスの定理が成り立ち、距離は素朴な二乗和の平方根で計算できる。

この「平らさ」は、計算を単純にし、理論を美しくした。しかし同時に、**見落とされた構造**があった。

本回では、深層学習以前の代表的な手法を振り返り、それらが前提としていた「平らな世界」の限界を掘り起こす。これは批判ではなく、考古学である。古い地図を読み解くことで、なぜ新しい地図が必要になったのかを理解する。

## PCAとSVD：影絵の時代

### 高次元の「見える化」という切実な課題

1990年代以前から、データ分析者は高次元データと格闘してきた。顔画像、遺伝子発現、テキストの単語頻度——いずれも数百から数万の次元を持つ。人間の目は3次元までしか見えない。どうすれば高次元データの「構造」を把握できるのか。

**主成分分析（PCA: Principal Component Analysis）** は、この問いに対する古典的な回答である。アイデアは単純だ：**分散が最大になる方向を見つけ、その方向にデータを射影する**。

### PCAの幾何学的解釈

PCAを幾何学的に理解しよう。$d$ 次元空間にデータ点 $\{\mathbf{x}_1, \ldots, \mathbf{x}_n\}$ が散らばっているとする。PCAは以下を行う：

1. データの重心を原点に移動（中心化）
2. 分散が最大になる方向（第1主成分）を見つける
3. その方向と直交する中で、次に分散が最大になる方向（第2主成分）を見つける
4. これを繰り返し、$k$ 個の主成分を得る

数学的には、**中心化済み**（各列の平均を引いた）データ行列 $X$ の共分散行列 $C = \frac{1}{n} X^\top X$ を固有値分解し、上位 $k$ 個の固有ベクトルを取り出す操作に対応する。

> [!NOTE]
> **共分散のスケーリング：** 統計学では共分散を $\frac{1}{n-1}$ で割る（不偏推定）ことも多いが、主成分の**方向**自体はスケーリングで変わらない。固有値の絶対値は変わるが、順序や固有ベクトルは同一である。また、$X$ が中心化されていない場合、$X^\top X$ は共分散行列ではなく二次モーメント行列になる点に注意。

**特異値分解（SVD: Singular Value Decomposition）** は、PCAと密接に関連する行列分解である：

$$X = U \Sigma V^\top$$

ここで $U$ は左特異ベクトル、$\Sigma$ は特異値の対角行列、$V$ は右特異ベクトル。PCAの主成分は $V$ の列ベクトルに対応する。

### 「影絵」という比喩

PCAは、高次元空間から低次元空間への **線形射影** である。これは、3次元の物体に光を当てて2次元の壁に影を落とすことに似ている。

影絵では、物体の3次元的な構造の一部が失われる。横から見た影と正面から見た影は異なり、どちらも「本当の形」ではない。PCAは「最も情報量の多い影」を選ぶが、それでも情報の損失は避けられない。

| 影絵の特徴 | PCAでの対応 |
| --- | --- |
| 3D→2Dの投影 | 高次元→低次元の射影 |
| 光の向きで影が変わる | 射影方向で見え方が変わる |
| 奥行き情報が失われる | 低分散方向の情報が失われる |
| 影は物体より単純 | 主成分は元データより単純 |

### PCAの暗黙の前提

PCAが有効に機能するためには、いくつかの暗黙の前提がある：

**1. 線形性の仮定**

PCAは線形射影しか行えない。データが非線形な多様体上に分布している場合、線形射影では構造を捉えられない。

例えば、「スイスロール」と呼ばれるデータ（2次元の帯を3次元空間で巻いたもの）を考えよう。このデータの「本質的な次元」は2だが、PCAで2次元に射影すると、巻きが重なって構造が潰れてしまう。

**2. 分散＝重要性の仮定**

PCAは「分散が大きい方向が重要」と仮定する。しかし、これは常に正しいわけではない。分散が小さい方向に、分類に重要な情報が隠れていることもある。

**3. 確率的解釈における等方ノイズの仮定**

PCAの手続き自体は、ノイズについての仮定を必要としない（単に分散が大きい方向を取るだけ）。しかし、PCAを確率モデルとして解釈する**確率的PCA（PPCA）** や因子分析では、等方ガウスノイズを仮定することが多い。

この確率的解釈のもとでは、ノイズが方向依存（異方的）な場合、主成分がノイズの大きい方向（分散が大きい方向）を拾いやすくなる。つまり、「信号」ではなく「ノイズ」を主成分として抽出してしまう可能性がある。

> [!NOTE]
> **手続きと解釈の区別：** 「PCAは等方ノイズを仮定する」という言い方は、確率的解釈の文脈では正しいが、PCAという手続き自体には当てはまらない。この区別は、手法の適用範囲を考える上で重要である。

> [!NOTE]
> **非線形な拡張：** PCAの限界を克服するため、**カーネルPCA**（Schölkopf et al., 1998）や**オートエンコーダ**（Hinton & Salakhutdinov, 2006）などの非線形次元削減手法が開発された。これらは第2部以降で扱う「曲がった空間」への布石となった。

### 歴史的文脈

PCAの数学的基礎は、Karl Pearsonによる1901年の論文に遡る。Harold Hotellingが1933年に統計学の文脈で再定式化し、現代的な形になった。

1990年代には、顔認識における「固有顔（Eigenfaces）」（Turk & Pentland, 1991）など、画像処理への応用が盛んに行われた。これは、顔画像を主成分で表現し、低次元空間で比較するという手法である。

当時、これは画期的だった。しかし、固有顔は「平均的な顔からのズレ」を線形に表現するものであり、照明変化や表情変化といった非線形な変動には脆弱だった。

## SVM vs ロジスティック回帰：幾何か統計か

### 二つの哲学

1990年代後半から2000年代前半、**サポートベクターマシン（SVM）** は機械学習の主役の一角を占めていた。同時期に、統計学の世界では**ロジスティック回帰**が分類問題の標準ツールとして使われていた（他にも決定木系やベイズ系の手法が分野によっては主流だった）。

両者は同じ問題（二値分類）を解くが、その哲学は対照的である。

| 観点 | SVM | ロジスティック回帰 |
| --- | --- | --- |
| 目的 | 境界からの距離（マージン）を最大化 | 尤度（確率）を最大化 |
| 出力 | 決定関数の符号（±1） | クラス確率 $p(y=1 \mid \mathbf{x})$ |
| 幾何学的解釈 | 超平面による空間の分割 | 確率分布のパラメータ推定 |
| 損失関数 | ヒンジ損失 $\max(0, 1 - y \cdot f(\mathbf{x}))$ | 交差エントロピー損失 |

### SVMの幾何学：マージン最大化

SVMの核心的アイデアは、**マージン最大化**である。

二つのクラスを分離する超平面は無数に存在する。SVMは、その中で「最も安全な」超平面を選ぶ。「安全」とは、どちらのクラスの点からも最も遠い、という意味である。

数学的に定式化しよう。超平面を $\mathbf{w}^\top \mathbf{x} + b = 0$ とする。点 $\mathbf{x}_i$ から超平面までの距離は：

$$\frac{|\mathbf{w}^\top \mathbf{x}_i + b|}{\|\mathbf{w}\|}$$

マージン（両クラスの最近傍点から超平面までの距離の和）を最大化する問題は、以下の最適化問題に帰着する：

$$\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{s.t.} \quad y_i(\mathbf{w}^\top \mathbf{x}_i + b) \geq 1 \quad \forall i$$

この制約条件のもとで、**幾何学的マージン**（超平面から最近傍点であるサポートベクターまでの距離）は $\frac{1}{\|\mathbf{w}\|}$ となり、両側を合わせた幅は $\frac{2}{\|\mathbf{w}\|}$ である。したがって、$\|\mathbf{w}\|^2$ を最小化することは、マージンを最大化することに対応する。

### 「サポートベクター」の幾何学的意味

最適解において、制約条件が等号で成り立つ点（$y_i(\mathbf{w}^\top \mathbf{x}_i + b) = 1$）を**サポートベクター**と呼ぶ。これらは超平面に最も近い点であり、境界を「支えている」。

幾何学的に言えば、サポートベクターは「境界の形を決める最小限の点集合」である。他の点を除去しても、サポートベクターが残っていれば、同じ超平面が得られる。

> [!NOTE]
> **アイデア上の類似：** サポートベクターの数が少ないほど、モデルは「疎」である。この「一部のデータ点だけが決定に寄与する」という性質は、後のMoEの「一部のExpertだけ活性化する」というアイデアと**構造的に類似している**。ただし、これは歴史的な因果関係ではなく、独立に発展した概念の事後的な類比である。

### カーネルトリック：暗黙の高次元空間

線形分離不可能なデータに対処するため、SVMは**カーネルトリック**を用いる。

アイデアは、データを高次元空間（特徴空間）に写像し、その空間で線形分離を行うことだ。例えば、2次元で円形に分布するデータは、3次元に持ち上げれば平面で分離できる。

カーネル関数 $k(\mathbf{x}, \mathbf{x}') = \phi(\mathbf{x})^\top \phi(\mathbf{x}')$ を使えば、高次元空間での内積を、元の空間での計算だけで求められる。これにより、無限次元の特徴空間すら扱える（例：RBFカーネル）。

**カーネルトリックの幾何学的意味：**

カーネルは、元の空間に「歪んだ距離感」を導入する、と比喩的に言うことができる。RBFカーネル $k(\mathbf{x}, \mathbf{x}') = \exp(-\gamma \|\mathbf{x} - \mathbf{x}'\|^2)$ は、近い点同士の類似度を高く、遠い点同士の類似度を低く評価する。

> [!CAUTION]
> **比喩と厳密な対応の区別：** 「カーネルが歪んだ距離感を与える」という表現は直感的な比喩である。厳密には、カーネルが定めるのは**再生核ヒルベルト空間（RKHS）上の内積**であり、これは特徴空間 $\phi(\mathbf{x})$ 上のユークリッド内積に対応する。重要な点として、カーネルは**入力空間上の距離を直接置き換えるのではなく、特徴空間での内積（したがって距離）を定める**。第0回で述べた「リーマン計量」は入力空間上に直接計量を入れる概念であり、カーネルとは異なる。ただし、「空間の幾何構造を変える」という発想は共通している。

### ロジスティック回帰：確率の世界

ロジスティック回帰は、決定境界ではなく**確率分布**を直接モデル化する。

$$p(y = 1 \mid \mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{x} + b) = \frac{1}{1 + \exp(-(\mathbf{w}^\top \mathbf{x} + b))}$$

ここで $\sigma$ はシグモイド関数。出力は0から1の間の値を取り、クラス1に属する確率として解釈できる。

幾何学的には、ロジスティック回帰も超平面 $\mathbf{w}^\top \mathbf{x} + b = 0$ を決定境界とする。しかし、SVMが「境界からの距離」を重視するのに対し、ロジスティック回帰は「確率的な柔らかさ」を重視する。

### 当時は「水と油」に見えた

1990年代から2000年代前半、SVMとロジスティック回帰は異なるコミュニティで発展した。SVMは機械学習・パターン認識の文脈で、ロジスティック回帰は統計学・計量経済学の文脈で使われることが多かった。

しかし、両者の関係は後に明らかになる。

**損失関数の観点から見た統一：**

| 手法 | 損失関数 | 特徴 |
| --- | --- | --- |
| SVM | $\max(0, 1 - y \cdot f(\mathbf{x}))$（ヒンジ損失） | 境界から離れた点は損失ゼロ |
| ロジスティック回帰 | $\log(1 + \exp(-y \cdot f(\mathbf{x})))$（ロジスティック損失） | すべての点が損失に寄与 |

両者は、$y \cdot f(\mathbf{x})$（マージン）を引数とする損失関数として統一的に理解できる。この視点は、後の深層学習における損失関数設計の基礎となった。

> [!IMPORTANT]
> **第5回への伏線：** 角度マージン損失（ArcFace等）は、この「マージン」の概念を角度空間に拡張したものである。SVMの幾何学的発想は、形を変えて現代の深層学習に受け継がれている。

## pLSA/LDA：離散的な星空

### 「単語の袋」という割り切り

テキストデータを扱う古典的な手法に、**トピックモデル**がある。代表的なものは、**pLSA**（Hofmann, 1999）と**LDA**（Blei et al., 2003）である。

これらの手法は、**Bag-of-Words（単語の袋）** という表現を前提とする。文書を、単語の出現頻度ベクトルとして表現し、単語の順序は無視する。

例えば、「猫が犬を追いかける」と「犬が猫を追いかける」は、Bag-of-Wordsでは同じ表現になる。

### LDAの生成モデル

LDAは、文書がどのように生成されるかを確率モデルとして記述する：

1. 各文書について、トピック分布 $\theta$ をDirichlet分布から生成
2. 各単語について：
   - トピック $z$ を $\theta$ からサンプリング
   - 単語 $w$ をトピック $z$ の単語分布からサンプリング

このモデルにより、「この文書は60%が政治、30%がスポーツ、10%が経済」といったトピック混合を推定できる。

### 「離散的な星空」という比喩

LDAが描く世界を、星空に喩えてみよう。

- **トピック**は星座に対応する。「政治」「スポーツ」「経済」といった抽象的な概念。
- **単語**は星に対応する。各星座（トピック）に属する星（単語）の集まり。
- **文書**は、複数の星座を含む夜空の一部。

この比喩で見えてくるのは、LDAの世界が**離散的**だということだ。トピックは有限個であり、単語も有限個。連続的な意味空間ではなく、離散的なカテゴリの組み合わせとして文書を捉える。

### 何が見落とされていたか

Bag-of-Wordsとトピックモデルには、根本的な限界がある。

**1. 順序（文脈）の欠如**

「犬が猫を追う」と「猫が犬を追う」の区別がつかない。これは、言語の本質的な側面——**順序**——を捨てていることに起因する。

第8回で扱う「時間の発見」は、この限界を克服するための旅の始まりである。RNNやTransformerは、順序情報を明示的に扱うことで、Bag-of-Wordsの限界を突破した。

**2. 分布的意味論の欠如**

Bag-of-Wordsでは、単語は独立した離散記号として扱われる。「王」と「女王」の意味的な近さ、「パリ」と「フランス」の関係性は、モデルに組み込まれていない。

後のWord2Vec（Mikolov et al., 2013）やGloVe（Pennington et al., 2014）は、単語を連続ベクトル空間に埋め込むことで、この限界を克服した。

**3. ユークリッド空間への暗黙の依存**

LDAは直接的にはユークリッド空間を仮定しないが、単語・文書の表現は結局のところ高次元ベクトルとして扱われる。このベクトル空間の「形」については、深く考察されていなかった。

> [!NOTE]
> **先進的だった点：** LDAは「意味の次元削減」という発想を持っていた。文書を数万次元の単語頻度ベクトルではなく、数十次元のトピック混合として表現する。この「圧縮しつつ意味を保つ」という発想は、後のオートエンコーダや表現学習に受け継がれている。

## 「平らな世界」の限界：まとめ

### 三つの暗黙の仮定

本回で見てきた古典的手法は、以下の暗黙の仮定を共有している：

| 仮定 | 内容 | 見落とされた構造 |
| --- | --- | --- |
| **線形性** | データの構造は線形部分空間で捉えられる | 非線形多様体 |
| **等方性** | 全方向が等しく重要 | 方向依存の構造（角度の意味） |
| **順序の無視** | 要素の順序は意味を持たない | 時間・文脈・因果 |

### ユークリッド空間の「呪縛」

これらの仮定は、**ユークリッド空間**という「平らな舞台」を前提としている。

ユークリッド空間では：

- 距離は二乗和の平方根（ピタゴラス）
- 全方向が対称（等方性）
- 曲率はゼロ（平坦）

この舞台は計算に便利だが、現実のデータが持つ構造を捉えきれない場合がある。

### 「見落とし」の具体例

**1. 正規化の欠如**

古典的手法では、ベクトルのノルム（長さ）と方向が混同されていた。「大きさ」が重要なのか「向き」が重要なのかが、明示的に区別されていなかった。

第2回では、この「ノルムの呪い」を詳しく分析する。

**2. 角度の軽視**

SVMのマージンはユークリッド距離で測られるが、特定の条件下では**角度（コサイン類似度）** の方が安定した類似度尺度になることがある。

> [!NOTE]
> **コサイン類似度が有効な条件：** テキストや埋め込み空間では、ベクトルのノルムが「頻度」「確信度」「重要度」といった意味を持つことがあり、純粋な類似度とは別の情報が混入しやすい。このような場合、正規化してコサイン類似度を使うと、意味的な類似度がより安定して測れることが多い。ただし、これは経験則であり、**距離の集中現象はL2距離にもコサイン類似度にも起こりうる**（第13回参照）。「高次元では常に角度が良い」わけではなく、データの分布・正規化の有無・タスクに依存する。

第3回では、角度を中心に据えた「プラネタリウム」の設計を導入する。

**3. 確率分布の幾何の無視**

ロジスティック回帰やLDAは確率分布を扱うが、「確率分布の空間」が持つ幾何構造（情報幾何学）は考慮されていなかった。

第4回では、Softmaxを情報幾何学の観点から再解釈する。

## 歴史的意義：批判ではなく考古学

本回で振り返った手法は、当時の計算資源と理論的枠組みの中で、最善の選択だった。

PCAは、計算が軽く、解釈が容易で、多くの問題で「十分に良い」結果を出した。SVMは、理論的保証（マージン理論）と実用的性能を兼ね備え、2000年代前半の機械学習において重要な役割を果たした。LDAは、大規模テキストコーパスから「意味」を抽出する道を開いた。

これらの手法を「古い」と切り捨てるのは、不公平である。むしろ、**何が暗黙の前提だったかを理解する**ことで、現代の手法がなぜそのように設計されているかが見えてくる。

> [!IMPORTANT]
> 古い手法を学ぶ意義は、「使えるか使えないか」ではなく、「何を仮定していたか」を理解することにある。仮定を意識することで、その仮定が成り立たない状況を認識でき、新しい設計の必要性が見えてくる。

## 実装ノート：古典手法の幾何学的可視化

> [!NOTE]
> 以下のコードは scikit-learn >= 1.0, matplotlib >= 3.5 を前提とする。

### PCAによる次元削減と情報損失の可視化

> [!NOTE]
> **scikit-learnと中心化：** 本文の数式では「中心化済みデータ行列 $X$」を前提としているが、`sklearn.decomposition.PCA` は内部で自動的に中心化（平均を引く処理）を行う。したがって、以下のコードでは手動で中心化する必要はない。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# 手書き数字データ（64次元）
digits = load_digits()
X, y = digits.data, digits.target

# PCAで2次元に削減
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 累積寄与率
cumsum = np.cumsum(pca.explained_variance_ratio_)
print(f"2成分での累積寄与率: {cumsum[1]:.2%}")
# → 約28%程度。72%の分散が「失われている」

# 可視化
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.5, s=10)
plt.colorbar(scatter, label='Digit')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA: 64D → 2D（約28%の分散を保持）')

plt.subplot(1, 2, 2)
pca_full = PCA().fit(X)
plt.plot(np.cumsum(pca_full.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('累積寄与率')
plt.axhline(y=0.9, color='r', linestyle='--', label='90%')
plt.legend()

plt.tight_layout()
plt.show()
```

### SVMの決定境界とマージンの可視化

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.datasets import make_blobs

# 2クラスのデータ生成
X, y = make_blobs(n_samples=100, centers=2, random_state=42, cluster_std=1.5)

# 線形SVM
svm = SVC(kernel='linear', C=1.0)
svm.fit(X, y)

# 決定境界の可視化
plt.figure(figsize=(8, 6))

# データ点
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', s=50)

# サポートベクターを強調
plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], 
            s=200, facecolors='none', edgecolors='green', linewidths=2,
            label=f'Support Vectors (n={len(svm.support_vectors_)})')

# 決定境界とマージン
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

xx = np.linspace(xlim[0], xlim[1], 100)
yy = np.linspace(ylim[0], ylim[1], 100)
XX, YY = np.meshgrid(xx, yy)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = svm.decision_function(xy).reshape(XX.shape)

# 決定境界（実線）とマージン（破線）
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], 
           linestyles=['--', '-', '--'], linewidths=[1, 2, 1])

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('SVM: マージン最大化の幾何学')
plt.legend()
plt.show()
```

### Bag-of-Words の限界：順序の喪失

```python
from collections import Counter

def bag_of_words(sentence):
    """文を単語の袋（頻度辞書）に変換"""
    words = sentence.lower().split()
    return Counter(words)

# 意味が異なるが、Bag-of-Wordsでは同一
sentence1 = "the cat chased the dog"
sentence2 = "the dog chased the cat"

bow1 = bag_of_words(sentence1)
bow2 = bag_of_words(sentence2)

print(f"文1: '{sentence1}'")
print(f"  → BoW: {dict(bow1)}")
print(f"文2: '{sentence2}'")
print(f"  → BoW: {dict(bow2)}")
print(f"BoWは同一か: {bow1 == bow2}")  # True

# 出力:
# 文1: 'the cat chased the dog'
#   → BoW: {'the': 2, 'cat': 1, 'chased': 1, 'dog': 1}
# 文2: 'the dog chased the cat'
#   → BoW: {'the': 2, 'dog': 1, 'chased': 1, 'cat': 1}
# BoWは同一か: True
```

## 参考文献

### 主成分分析（PCA）

- Pearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. *Philosophical Magazine*, 2(11), 559–572.
    - PCAの原論文。最小二乗法の観点から導入。

- Hotelling, H. (1933). Analysis of a Complex of Statistical Variables into Principal Components. *Journal of Educational Psychology*, 24(6), 417–441.
    - 統計学の文脈でのPCAの再定式化。

- Turk, M., & Pentland, A. (1991). Eigenfaces for Recognition. *Journal of Cognitive Neuroscience*, 3(1), 71–86.
    - 固有顔による顔認識。PCAの画像処理への応用例。

### サポートベクターマシン（SVM）

- Cortes, C., & Vapnik, V. (1995). Support-Vector Networks. *Machine Learning*, 20(3), 273–297. DOI: [10.1007/BF00994018](https://doi.org/10.1007/BF00994018)
    - SVMの原論文。

- Schölkopf, B., Smola, A., & Müller, K.-R. (1998). Nonlinear Component Analysis as a Kernel Eigenvalue Problem. *Neural Computation*, 10(5), 1299–1319.
    - カーネルPCAの導入。非線形次元削減への拡張。

### トピックモデル

- Hofmann, T. (1999). Probabilistic Latent Semantic Analysis. *UAI '99*.
    - pLSAの原論文。

- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. *JMLR*, 3, 993–1022.
    - LDAの原論文。トピックモデルの標準的手法。

### 単語埋め込み（後続手法への橋渡し）

- Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. *NeurIPS 2013*.
    - Word2Vecの論文。Bag-of-Wordsからの脱却。

- Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. *EMNLP 2014*.
    - GloVe。共起統計に基づく単語埋め込み。

## 次回予告

第2回「ノルムの呪い」では、ユークリッド空間の「平らさ」がもたらす具体的な問題を分析する。特に、ノルム（ベクトルの長さ）が担っていた複数の意味——「大きさ」と「確信度」と「重要度」——が混線していた背景を整理する。

高次元空間での「距離の集中」現象にも触れ、なぜ角度ベースの設計が必要になるのかを準備する。
