# 第7回：不確実性の復権 ～Variance Matters～

## 注意事項

- $\kappa$ （集中度）を「確信度」として解釈できるのは、埋め込みをvMF分布として明示的にモデル化した場合に限られる。
- 単に $L_2$ 正規化して $\cos$ 類似度を使うだけでは、 $\kappa$ は自動的には得られない（点埋め込みに「霧」はない）。
- エントロピーの近似式 $H \approx -\log \kappa$ は、高次元・高 $\kappa$ の条件下での近似であり、一般には成り立たない。
- 本回で扱う「分布埋め込み」は、標準的な深層学習ではまだ主流ではない。しかし、不確実性を扱う設計の重要性は増しており、将来の標準になる可能性がある。

## 導入：点から雲へ

第3回で、表現空間を「プラネタリウム」として捉えた。単位球面上に配置された点が、意味の「星」として輝いている。しかし、この描像には重大な欠落がある。

**星は点ではない。** 実際の星は、大気の揺らぎや観測機器の精度によって「にじんで」見える。この「にじみ」には情報が含まれている。シャープに見える星は近くにあるか、明るいか、大気が安定している。ぼやけて見える星は、遠くにあるか、暗いか、大気が乱れている。

埋め込み空間でも同じことが言える。「りんご」という単語の表現は、文脈によってぶれるだろうか。「量子もつれ」という専門用語の表現は、学習データの量によって確信度が異なるだろうか。従来の点埋め込みは、この「ぶれ」や「確信度」を無視してきた。

本回では、点表現から**分布表現**へのパラダイムシフトを議論する。表現を「点」ではなく「雲」として扱うことで、不確実性を明示的にモデル化できる。これは、「分かりません」と言えるAIへの第一歩である。

## ノルムの分離という思想

### 従来：混沌としたノルム

第2回で見たように、従来の埋め込みではベクトルの長さ（ノルム）が様々な意味を担っていた。

| 解釈 | 例 |
| --- | --- |
| 単語の頻度 | 高頻度語は長いベクトル |
| 意味の「強さ」 | 感情的な単語は長いベクトル |
| 学習の進行度 | よく学習された概念は長いベクトル |
| 数値的なアーティファクト | 初期化や最適化の偶然 |

これらの意味が混在しているため、ノルムを解釈することは困難だった。「このベクトルが長いのは、頻度が高いからか、意味が強いからか、それとも偶然か」を区別できない。

### これから：意味と確信度の分離

本講義のアプローチは、この混沌を**設計によって解消する**ことである。

**方向（direction）**：意味を表す。「りんご」と「みかん」は異なる方向を向く。

**広がり（spread）**：確信度を表す。確信度が高い概念は鋭く局在し、低い概念はぼやけて広がる。

この分離を実現するのが、**方向分布**の導入である。特に、単位球面上の方向分布として**von Mises-Fisher（vMF）分布**が有力な選択肢となる。

> [!IMPORTANT]
> **設計としての分離：** この「意味と確信度の分離」は、データから自動的に得られるものではない。vMF分布を使い、 $\kappa$ （集中度）を明示的に学習する設計を採用して初めて実現する。単に $L_2$ 正規化するだけでは、点埋め込みのままであり、不確実性は表現されない。

## VAEとガウス空間の重力

### 潜在空間を「押し込める」力

分布表現の先駆的な成功例として、**変分オートエンコーダ（VAE）** (Kingma & Welling, 2014) がある。VAEは、データを潜在空間上の分布として符号化し、そこからサンプリングして復元する。この枠組みは、「点ではなく分布を学ぶ」という本回のテーマの出発点として重要である。

VAEの学習目標には、**KLダイバージェンス正則化**が含まれる。潜在分布 $q(\mathbf{z} \mid \mathbf{x})$ を標準正規分布 $\mathcal{N}(\mathbf{0}, \mathbf{I})$ に近づける項である。

$$\mathcal{L} _{\text{VAE}} = \underbrace{\mathbb{E} _{q(\mathbf{z}|\mathbf{x})}[\log p(\mathbf{x}|\mathbf{z})]} _{\text{復元項}} - \underbrace{D _{\text{KL}}(q(\mathbf{z}|\mathbf{x}) \| \mathcal{N}(\mathbf{0}, \mathbf{I}))} _{\text{正則化項}}$$

この正則化項は、事後分布のパラメータを標準正規事前分布に合わせる圧力として働く。具体的には、平均 $\boldsymbol{\mu} \to \mathbf{0}$ （原点へ寄せる）と分散 $\sigma^2 \to 1$ （広がりを固定する）の二つの力が同時に作用する。幾何学的には、潜在表現を原点中心のガウス分布へ押し込める**正則化の重力**と見なすことができる。

### ユークリッド空間の不均一性

ここで問題になるのが、ガウス分布が生む空間の**不均一性**である。

標準正規分布 $\mathcal{N}(\mathbf{0}, \mathbf{I})$ の確率密度関数（pdf）は原点で最大値をとる。しかし、高次元では**密度が最大の場所にサンプルが集まるとは限らない**。次元 $d$ が大きくなると、半径 $r$ の球殻の体積が $r^{d-1}$ に比例して急増するため、密度と体積の積——すなわち確率質量——は原点から離れた $r \approx \sqrt{d}$ 付近の薄い殻に集中する（concentration of measure）。これが**典型集合**と呼ばれる領域である。

| | 確率密度（pdf） | 確率質量（典型集合） |
| --- | --- | --- |
| 原点付近 | 最大 | ほぼゼロ（体積が微小） |
| $r \approx \sqrt{d}$ の殻 | 中程度 | **ほぼ全質量が集中** |
| 遠方 | 急激に減少 | ほぼゼロ（密度が微小） |

VAEの潜在空間では、正則化が事後分布をこの標準正規分布に合わせようとする。結果として、潜在表現は $\sqrt{d}$ 付近の典型帯に集中する傾向がある。この帯の外——原点付近も遠方も——は事前分布の下で訪れにくく、デコーダの学習が手薄になりやすい。そのため、典型帯の外での復元・生成・補間は不安定になりがちである。空間全体が均等に使われるわけではなく、**半径方向に狭い「居住可能帯」ができる**のがガウス潜在空間の実践上の特徴である（帯の外が禁じられているわけではないが、確率的に訪れにくいため学習が行き届きにくい）。

> [!NOTE]
> **典型集合と「殻」の直感：** 高次元ガウス分布のサンプルが球殻に集中する現象は、低次元の直感に反する。2次元や3次元では中心付近の密度が高いため「サンプルも中心に集まる」という直感が働きやすいが、低次元でも典型半径は0ではない（2次元で $r \approx 1$ 、3次元で $r \approx \sqrt{2}$ ）。次元が上がるにつれてこの典型半径が $\sqrt{d}$ へと伸び、殻への集中がより顕著になる。VAEの潜在空間でも同様の現象が起き、表現が実質的に使える領域が制限される。

### 球面への脱出：vMFという回答

ガウス事前分布の根本的な制約は、**半径方向の自由度を強く持つ一方で、角度（方向）主体の構造に不向き**なことにある。潜在表現が意味する「何であるか」が主に方向で決まる場合、半径方向の変動はノイズでしかない。にもかかわらず、ガウス事前分布は半径方向にも広い自由度を許し、典型帯という制約をかける。

第3回で「プラネタリウム」として導入した球面表現は、この問題に対する幾何学的な回答である。単位球面 $\mathbb{S}^{d-1}$ 上では、すべての点が原点から等距離にあり、半径方向の自由度がそもそも存在しない。情報はすべて方向に集約される。nGPT（第4回）が示した $L_2$ 正規化の効果も、この「半径の消去」によるものであった。

さらに、球面上で一様分布を事前分布に置けば、ガウス空間にあった半径方向の典型帯のような偏りは生じない。ただし、学習された事後分布は当然偏る——それは意味の構造を反映した望ましい偏りである。重要なのは、**空間の幾何そのものが半径方向の不均一性を持たない**という点である。

球面上での分布表現として**vMF分布**を選ぶことは、この設計思想の帰結である。

| | VAE（ガウス） | vMF（球面） |
| --- | --- | --- |
| 空間 | $\mathbb{R}^d$ （ユークリッド空間） | $\mathbb{S}^{d-1}$ （単位球面） |
| 正則化の方向 | 標準正規への引力（ $\boldsymbol{\mu} \to \mathbf{0}, \sigma \to 1$ ） | 球面上の一様分布への引力 |
| 半径方向の偏り | あり（典型帯 $\sqrt{d}$ に集中） | なし（半径の自由度が存在しない） |
| 確信度の表現 | 分散 $\sigma^2$ | 集中度 $\kappa$ |
| 意味と確信度の分離 | ノルムに意味が混入しうる | 方向＝意味、 $\kappa$ ＝確信度 |

Davidson et al. (2018) は、VAEの潜在空間にvMF分布を導入した**Hyperspherical VAE**を提案し、潜在構造が球面的な場合にガウス事前分布より有利になりうることを実験的に示した。これは、データの潜在構造に合った幾何学を事前分布に採用する——すなわち「正則化の形を変える」——ことの重要性を示す事例である。

> [!IMPORTANT]
> **VAEは「分布を学ぶ」先駆者：** VAEが示した「潜在表現を分布として扱う」というアイデア自体は強力であり、本回で扱う分布埋め込みの思想的な源流の一つである。問題は分布を学ぶこと自体にではなく、その分布を配置する**空間の形**（ユークリッド vs 球面）にある。

## von Mises-Fisher分布：球面上のガウス分布

### vMF分布とは何か

**von Mises-Fisher（vMF）分布**は、単位球面上の方向分布である。ユークリッド空間でのガウス分布の「球面版」と考えることができる。

$d$ 次元単位球面 $\mathbb{S}^{d-1}$ 上のvMF分布は、以下の確率密度関数を持つ：

$$p(\mathbf{x} \mid \boldsymbol{\mu}, \kappa) = C_d(\kappa) \exp(\kappa \boldsymbol{\mu}^\top \mathbf{x})$$

ここで：

- $\mathbf{x} \in \mathbb{S}^{d-1}$ ：観測される方向（単位ベクトル）
- $\boldsymbol{\mu} \in \mathbb{S}^{d-1}$ ：平均方向（分布の「中心」）
- $\kappa \geq 0$ ：集中度パラメータ
- $C_d(\kappa)$ ：正規化定数（Bessel関数を含む）

### $\kappa$ の直感的理解

$\kappa$ は、分布がどれだけ平均方向 $\boldsymbol{\mu}$ の周りに「集中」しているかを表す。

| $\kappa$ の値 | 分布の形状 | 直感的解釈 |
| --- | --- | --- |
| $\kappa = 0$ | 球面上の一様分布 | 方向の情報なし、完全な不確実性 |
| $\kappa$ 小 | ぼやけた広い分布 | 低確信度、複数の可能性 |
| $\kappa$ 大 | 鋭く集中した分布 | 高確信度、ほぼ確定 |
| $\kappa \to \infty$ | 点（デルタ分布）に収束 | 完全な確信、従来の点埋め込み |

プラネタリウムのメタファーで言えば：

- 大きい $\kappa$ ：「あの星座しか見えない」（晴れた夜空、高確信）
- 小さい $\kappa$ ：「複数の星座が重なって見える」（霧の夜、低確信）

> [!NOTE]
> **vMF分布とガウス分布の関係：** 高次元の単位球面上で $\kappa$ が十分大きい場合、vMF分布は接空間上のガウス分布に近似できる。これは、地球の表面が局所的に平面に見えるのと同じ原理である。

### 正規化定数の複雑さ

vMF分布の正規化定数 $C_d(\kappa)$ は、修正Bessel関数を含む複雑な形をしている：

$$C_d(\kappa) = \frac{\kappa^{d/2 - 1}}{(2\pi)^{d/2} I_{d/2 - 1}(\kappa)}$$

ここで $I_{\nu}(\kappa)$ は第一種修正Bessel関数である。

この複雑さが、vMF分布の実装上の困難の源である。特に：

- Bessel関数の数値計算は、 $\kappa$ が大きいときにオーバーフローしやすい
- 勾配計算には Bessel 関数の比が現れ、数値的に不安定になりうる

> [!CAUTION]
> **実装上の注意：** vMF分布からのサンプリングや対数尤度の計算は、ナイーブな実装では数値的に不安定になりやすい。後述の実装ノートで、安定した計算方法を紹介する。

## 不確実性の定量化

### エントロピーとしての不確実性

分布の「ぼやけ具合」を定量化する自然な方法は、**エントロピー**である。エントロピーが高いほど、分布は広がっており、不確実性が高い。

vMF分布のエントロピー $H$ は、 $\kappa$ と次元 $d$ の関数として表される：

$$H = -\log C_d(\kappa) - \kappa \cdot A_d(\kappa)$$

ここで $A_d(\kappa) = I_{d/2}(\kappa) / I_{d/2 - 1}(\kappa)$ は Bessel 関数の比である。

### 近似式の条件

文献やチュートリアルで見かける単純な近似式：

$$H \approx -\log \kappa + \text{const.}$$

これは、**高次元かつ高** $\kappa$ **という条件下**での近似であり、一般には成り立たない。

| 条件 | 近似の妥当性 |
| --- | --- |
| 高次元（ $d \gg 1$ ）、高 $\kappa$ （ $\kappa \gg 1$ ） | 良い近似 |
| 低次元（ $d$ が小さい） | 近似が崩れる |
| 低 $\kappa$ （ $\kappa \approx 0$ ） | 近似が崩れる |

> [!IMPORTANT]
> **教育的簡略化の注意：** 「 $\kappa$ が大きいほど集中（低不確実性）」という定性的な理解は正しい。しかし、「 $H \approx -\log \kappa$ 」という定量的な式を使う場合は、適用条件を確認すること。

### 確信度としての $\kappa$

実用上、 $\kappa$ を「確信度」として直接使うことが多い。この解釈が有効なのは：

1. **vMF分布を明示的にモデル化している場合**：埋め込みを点ではなく分布として学習し、 $\kappa$ をパラメータとして推定している
2. $\kappa$ **の推定が安定している場合**：学習データが十分にあり、 $\kappa$ の推定値が信頼できる

単に $L_2$ 正規化しただけの点埋め込みでは、 $\kappa$ に相当する量は存在しない。「確信度」を持つためには、設計段階で分布表現を採用する必要がある。

## Out-of-Distribution（OOD）検知

### 問題：ドーム上の「暗闘」

プラネタリウムのドーム（単位球面）には、星（学習された概念）が点在している。では、星のない領域に入力が来たとき、モデルはどうすべきか。

従来の点埋め込みでは、入力は必ずどこかの方向に射影される。星がない領域でも、最も近い星に無理やり関連付けられる。これが、**Out-of-Distribution（OOD）入力に対する脆弱性**の一因である。

### 分布表現による解決策

分布表現を使えば、OOD検知に有力なシグナルが得られる。

**低 $\kappa$ 領域の検出**：入力に対して推定される $\kappa$ が低い場合、モデルはその入力について「よく分からない」と言っている。これはOODの可能性を示唆する。

**密度による検出**：vMF混合モデルを使えば、入力が高密度領域にあるか低密度領域にあるかを評価できる。低密度領域はOODの可能性が高い。

| 検出方法 | 利点 | 欠点 |
| --- | --- | --- |
| $\kappa$ 閾値 | シンプル、解釈しやすい | 閾値の設定が難しい |
| 密度ベース | 理論的に正当化しやすい | 計算コストが高い |
| アンサンブル | 頑健性が高い | モデルを複数持つ必要がある |

> [!NOTE]
> **「分かりません」と言えるAI：** OOD検知は、AIの信頼性にとって本質的に重要である。医療診断や自動運転など、高リスクなアプリケーションでは、「分かりません」と言えることが安全性の鍵となる。分布表現は、この能力を表現空間のレベルで実現する一つのアプローチである。

## ハルシネーションの幾何学

### ハルシネーションとは何か

大規模言語モデル（LLM）の**ハルシネーション**は、モデルが事実と異なる情報を、あたかも確信を持っているかのように生成する現象である。

幾何学的に見ると、ハルシネーションは以下のように解釈できる：

> **ハルシネーション = 低 $\kappa$ 領域で無理やり星座を結ぶ行為**

星がまばらな領域（学習データが少ない、または概念が曖昧な領域）で、モデルはそれでも何らかの出力を生成しなければならない。結果として、存在しない星座を「でっち上げて」しまう。

### 幾何学的な対策

分布表現を採用すれば、ハルシネーションに対する幾何学的な対策が可能になる。

**閾値ベースの抑制**： $\kappa$ が閾値以下の場合、出力を控える（「分かりません」と答える）。

$$\text{if } \kappa < \kappa_{\text{threshold}} \text{: output "I don't know"}$$

**確信度の伝播**：生成の各ステップで確信度を追跡し、累積的な不確実性が高まったら警告を出す。

**サンプリングの調整**：低確信度領域では、より多様なサンプリング（高温）を行い、単一の「でっち上げ」に固執しないようにする。

> [!CAUTION]
> **比喩の限界：** 上記の「星座を結ぶ」という比喩は、ハルシネーションの直感的理解には有用だが、実際のLLMの動作を完全に説明するものではない。LLMのハルシネーションには、表現空間の問題だけでなく、自己回帰生成の性質、学習データのバイアス、デコーディング戦略など、複数の要因が絡んでいる。

## 整列（Alignment）からの解放

### 従来の問題：座標系の不一致

異なるモデルや異なる学習から得られた埋め込み空間を比較したい場面がある。例えば：

- 言語Aの埋め込みと言語Bの埋め込みを対応付けたい
- 時刻 $t_1$ の埋め込みと時刻 $t_2$ の埋め込みの変化を測りたい
- モデルAとモデルBの表現を比較したい

従来のアプローチでは、**Procrustes整列**が必要だった。これは、一方の座標系を回転・反転して、もう一方に合わせる操作である。

$$\min_R \|X R - Y\|_F^2 \quad \text{subject to } R^\top R = I$$

この整列は、対応点のペアが必要であり、計算コストもかかる。

### $\kappa$ による座標フリーの比較

分布表現を採用し、 $\kappa$ のような**回転不変量**を使えば、座標系に依存しない比較が可能になる場合がある。

| 比較対象 | 座標依存 | $\kappa$ 依存 |
| --- | --- | --- |
| 2つの点の位置 | 座標系に依存 | - |
| 2つの点の距離 | 座標系に依存しない | - |
| 分布の集中度 | 座標系に依存しない | $\kappa$ で直接比較可能 |

**具体例**：「言語Aで『犬』の確信度」と「言語Bで『dog』の確信度」を比較したい場合、座標系を合わせなくても、それぞれの $\kappa$ を比較できる。

> [!NOTE]
> **限定的な解放：** これは万能の解決策ではない。意味的な対応関係（「犬」と「dog」が同じ概念を指す）を知るには、依然として何らかの整列が必要である。 $\kappa$ で比較できるのは、確信度のような**スカラー量**に限られる。

## 分布埋め込みの学習

### アーキテクチャの選択肢

分布埋め込みを学習するには、モデルが点ではなく分布のパラメータを出力する必要がある。

**vMF出力層**：通常の埋め込み層の代わりに、 $(\boldsymbol{\mu}, \kappa)$ を出力する層を使う。

$$
\text{入力}
\rightarrow
\text{エンコーダ}
\rightarrow
[\mu, \log \kappa]
\rightarrow
\text{vMF分布}
$$

ここで $\log \kappa$ を出力するのは、 $\kappa > 0$ の制約を自然に満たすためである。

**損失関数の設計**：分布間の距離を測る損失関数が必要。KLダイバージェンス、期待コサイン類似度、あるいは対照学習の変種が使われる。

### 学習の課題

分布埋め込みの学習には、点埋め込みにはない課題がある。

$\kappa$ **の崩壊**：学習が進むと、すべての $\kappa$ が極端に大きくなり（点に収束）、不確実性の情報が失われることがある。正則化や事前分布で対処する。

**サンプリングの分散**：損失関数の計算にサンプリングが必要な場合、勾配の分散が大きくなりやすい。Reparameterization Trickの球面版が必要。

**計算コスト**：Bessel関数の計算、サンプリング、分布間距離の計算など、点埋め込みより計算量が増える。

## 実装ノート

> [!NOTE]
> 以下のコードは PyTorch >= 1.9 を前提とする。vMF関連の実装について：
>
> - **サンプリング**：Wood's algorithmに基づく実装を示す
> - **KL・正規化定数**：高κ・高次元での漸近近似を使用。κが小さい領域では誤差が大きくなる可能性があるため、厳密計算が必要な場合はscipy.special.ive等を使用すること

### vMF分布のパラメータ化

<details>
<summary>コード例: 07_vmf_layer.py</summary>

```07_vmf_layer.py
import torch
import torch.nn as nn
import torch.nn.functional as F


class vMFLayer(nn.Module):
    """vMF分布のパラメータを出力する層

    入力から平均方向μと集中度κを推定する。
    """

    def __init__(self, input_dim, output_dim, kappa_min=1.0, kappa_max=100.0):
        super().__init__()
        self.output_dim = output_dim
        self.kappa_min = kappa_min
        self.kappa_max = kappa_max

        # μを出力する層（正規化前）
        self.mu_layer = nn.Linear(input_dim, output_dim)

        # log κを出力する層（スカラー）
        self.kappa_layer = nn.Linear(input_dim, 1)

    def forward(self, x):
        """
        Args:
            x: 入力テンソル [batch, input_dim]

        Returns:
            mu: 平均方向 [batch, output_dim]（単位ベクトル）
            kappa: 集中度 [batch, 1]
        """
        # 平均方向（正規化して単位ベクトルに）
        mu = self.mu_layer(x)
        mu = F.normalize(mu, dim=-1)

        # 集中度（正の値に制約）
        log_kappa = self.kappa_layer(x)
        # Softplusで正の値に、さらに範囲を制限
        kappa = F.softplus(log_kappa)
        kappa = self.kappa_min + (self.kappa_max - self.kappa_min) * torch.sigmoid(kappa - 5)

        return mu, kappa
```

</details>

### vMF分布からのサンプリング

vMF分布からのサンプリングは、rejection samplingやWood's algorithmを使う。以下は高次元で効率的な方法：

<details>
<summary>コード例: 07_vmf_sampling.py</summary>

```07_vmf_sampling.py
import math

import torch
import torch.nn.functional as F


def sample_vmf(mu, kappa, num_samples=1):
    """vMF分布からサンプリング（Wood's algorithm）

    Args:
        mu: 平均方向 [batch, dim]
        kappa: 集中度 [batch, 1]
        num_samples: サンプル数

    Returns:
        samples: サンプル [batch, num_samples, dim]
    """
    batch_size, dim = mu.shape
    device = mu.device

    # κが非常に小さい場合は一様分布からサンプル
    if kappa.max() < 1e-6:
        samples = torch.randn(batch_size, num_samples, dim, device=device)
        return F.normalize(samples, dim=-1)

    # Householder変換でμを北極に写す
    # e1 = [1, 0, 0, ..., 0]
    e1 = torch.zeros(dim, device=device)
    e1[0] = 1.0

    results = []
    for _ in range(num_samples):
        # wをサンプル（μ方向の成分）
        w = _sample_w(kappa.squeeze(-1), dim)  # [batch]

        # 球面上の一様な方向をサンプル（μに直交する成分）
        v = torch.randn(batch_size, dim - 1, device=device)
        v = F.normalize(v, dim=-1)

        # 球面座標からデカルト座標へ
        sqrt_term = torch.sqrt(torch.clamp(1 - w**2, min=1e-10))

        # 北極周りのサンプルを構成
        sample_around_north = torch.zeros(batch_size, dim, device=device)
        sample_around_north[:, 0] = w
        sample_around_north[:, 1:] = sqrt_term.unsqueeze(-1) * v

        # Householder変換でμ周りに回転
        sample = _householder_rotation(sample_around_north, e1.expand(batch_size, -1), mu)
        results.append(sample)

    return torch.stack(results, dim=1)


def _sample_w(kappa, dim):
    """vMF分布のw成分をサンプル（rejection sampling）"""
    device = kappa.device
    batch_size = kappa.shape[0]

    # 近似パラメータ
    c = torch.sqrt(4 * kappa**2 + (dim - 1) ** 2)
    b = (c - 2 * kappa) / (dim - 1)
    a = ((dim - 1) + c) / (2 * kappa)

    # Rejection sampling
    w = torch.zeros(batch_size, device=device)
    done = torch.zeros(batch_size, dtype=torch.bool, device=device)

    max_iter = 1000
    for _ in range(max_iter):
        if done.all():
            break

        # 提案分布からサンプル
        eps = torch.rand(batch_size, device=device)
        u = torch.rand(batch_size, device=device)

        z = torch.cos(math.pi * eps)
        w_proposal = (1 + b * z) / (b + z)

        # 受理確率
        t = kappa * w_proposal + (dim - 1) * torch.log(1 - a * w_proposal)
        accept = (dim - 1) * torch.log(1 - a * b) - 1 + torch.log(u) < t

        w = torch.where(~done & accept, w_proposal, w)
        done = done | accept

    return w


def _householder_rotation(x, u, v):
    """Householder反射を用いてuをvに写す変換をxに適用

    uとvが単位ベクトルのとき、w = normalize(v - u) に対する
    Householder反射 H = I - 2ww^T は、uをvに写す。

    注意：厳密には反射であり回転ではない（行列式が-1）。
    vMFサンプリングの文脈では、北極周りの点をμ周りに移動させる
    という目的は多くの場合達成できる。ただし、以下の場合は
    2回の反射（＝回転）やRodriguesの回転公式を検討すること：
    - 時間方向の連続性・滑らかさが必要な場合
    - 後段処理が「回転」としての性質（det=+1）を仮定する場合
    """
    # u, vは単位ベクトル
    # 反射軸: w = normalize(v - u)
    w = v - u
    w_norm = w.norm(dim=-1, keepdim=True)

    # u ≈ v の場合（反射不要）
    mask = (w_norm < 1e-6).squeeze(-1)
    if mask.all().item():
        return x

    w = w / (w_norm + 1e-8)

    # Householder反射: x' = x - 2(x·w)w
    x_reflected = x - 2 * (x * w).sum(dim=-1, keepdim=True) * w

    # u ≈ v の場合は元のxを返す
    x_result = torch.where(mask.unsqueeze(-1), x, x_reflected)

    return x_result
```

</details>

### vMF間のKLダイバージェンス

> [!CAUTION]
> **以下の実装は高κ・高次元での漸近近似に基づく。** κが小さい領域（まさに不確実性が高く重要な領域）では近似誤差が大きくなる。厳密な計算が必要な場合は、scipy.special.ive（スケール付きBessel関数）等を使った安定実装を検討すること。詳細はDiethe (2015) を参照。

<details>
<summary>コード例: 07_vmf_kl_divergence.py</summary>

```07_vmf_kl_divergence.py
import math

import torch


def kl_divergence_vmf_approx(mu1, kappa1, mu2, kappa2):
    """2つのvMF分布間のKLダイバージェンス【高κ近似版】

    KL(vMF(μ1, κ1) || vMF(μ2, κ2))

    WARNING: この実装は高κ・高次元での漸近近似を使用。
    κ < 10 程度の領域では誤差が大きくなる可能性がある。

    Args:
        mu1, mu2: 平均方向 [batch, dim]
        kappa1, kappa2: 集中度 [batch, 1]

    Returns:
        kl: KLダイバージェンス [batch]（近似値）
    """
    dim = mu1.shape[-1]

    # コサイン類似度
    cos_sim = (mu1 * mu2).sum(dim=-1, keepdim=True)

    # Bessel関数の比（高κ近似）
    A1 = _bessel_ratio_approx(kappa1, dim)

    # 正規化定数の差（対数、高κ近似）
    log_C1 = _log_normalizer_vmf_approx(kappa1, dim)
    log_C2 = _log_normalizer_vmf_approx(kappa2, dim)

    # KLダイバージェンス
    kl = log_C2 - log_C1 + kappa1 * A1 - kappa2 * A1 * cos_sim

    return kl.squeeze(-1)


def _bessel_ratio_approx(kappa, dim):
    """Bessel関数の比 I_{d/2}(κ) / I_{d/2-1}(κ) の高κ近似

    WARNING: κが小さい（< 10程度）場合、この近似は不正確。
    厳密計算には scipy.special.ive を使用すること。
    """
    # 高κでの近似: A(κ) ≈ 1 - (d-1)/(2κ)
    nu = dim / 2 - 1
    return 1 - (2 * nu + 1) / (2 * kappa + 1e-8)


def _log_normalizer_vmf_approx(kappa, dim):
    """vMF分布の対数正規化定数 log C_d(κ) の高κ近似

    WARNING: κが小さい（< 10程度）場合、この近似は不正確。
    厳密計算には scipy.special.ive を使用すること。

    近似式: log C ≈ (d/2 - 1) * log κ - d/2 * log(2π) - κ
    （Stirling近似に基づく、高κ・高次元での漸近展開）
    """
    nu = dim / 2 - 1
    return nu * torch.log(kappa + 1e-8) - (dim / 2) * math.log(2 * math.pi) - kappa


# 参考：厳密計算が必要な場合のスケルトン
def _log_normalizer_vmf_exact(kappa, dim):
    """vMF分布の対数正規化定数（厳密版、要scipy）

    log C_d(κ) = (d/2 - 1) * log κ - (d/2) * log(2π) - log I_{d/2-1}(κ)

    数値安定性のため、スケール付きBessel関数 ive を使用：
    I_ν(κ) = ive(ν, κ) * exp(κ)
    よって log I_ν(κ) = log(ive(ν, κ)) + κ

    WARNING: この関数はNumPy配列またはPython floatを想定。
    PyTorch tensorを渡す場合は事前に変換すること：
        kappa_np = kappa.detach().cpu().numpy()
        result_np = _log_normalizer_vmf_exact(kappa_np, dim)
        result = torch.from_numpy(result_np).to(kappa.device)
    """
    # from scipy.special import ive
    # import numpy as np
    # nu = dim / 2 - 1
    # # ive(nu, kappa) は exp(-kappa) * I_nu(kappa) を返す（オーバーフロー防止）
    # log_ive = np.log(np.maximum(ive(nu, kappa), 1e-300))
    # log_bessel = log_ive + kappa  # log I_nu(kappa) を復元
    # return nu * np.log(kappa) - (dim / 2) * np.log(2 * np.pi) - log_bessel
    raise NotImplementedError("厳密計算にはscipyが必要。上記コメントを参照。")


# 互換性のためのエイリアス（旧名で呼び出すコードがある場合用）
kl_divergence_vmf = kl_divergence_vmf_approx
```

</details>

### 確信度に基づくOOD検知

> [!IMPORTANT]
> **κ閾値によるOOD検知は、必ず校正（calibration）が必要。** モデルが出力するκは、学習データやアーキテクチャに依存し、そのままでは「確率的に正しい確信度」を表さないことが多い。運用前に検証セットで以下を行うこと：
>
> - 閾値の選定（Precision-Recallトレードオフの確認）
> - 温度スケーリング等によるκの校正
> - 分布内/分布外のκ分布の可視化

<details>
<summary>コード例: 07_ood_detector.py</summary>

```07_ood_detector.py
import warnings

import torch


class OODDetector:
    """κに基づくOOD検知

    WARNING: 閾値はデータセット・モデルに依存する。
    必ず検証セットで校正すること。
    """

    def __init__(self, threshold=10.0):
        self.threshold = threshold
        self._is_calibrated = False

    def calibrate(self, kappa_in, kappa_out, target_fpr=0.05):
        """検証セットで閾値を校正

        Args:
            kappa_in: 分布内データのκ。1D tensor [n_in] を想定。
                      2D [n_in, 1] の場合は自動的にsqueezeされる。
            kappa_out: 分布外データのκ。1D tensor [n_out] を想定。
                       2D [n_out, 1] の場合は自動的にsqueezeされる。
            target_fpr: 目標偽陽性率（分布内をOODと誤判定する率）

                        NOTE: 本実装では「分布内サンプルを誤ってOODと判定する率」を
                        FPR（False Positive Rate）と定義している。
                        文献によっては「OODサンプルを誤って分布内と判定する率」を
                        FPRとする場合もあるため、比較時は定義に注意すること。

        Returns:
            calibrated_threshold: 校正された閾値
        """
        # 形状を1Dに統一
        kappa_in = kappa_in.squeeze()
        kappa_out = kappa_out.squeeze()

        # 分布内データのκ分位点から閾値を決定
        # target_fpr = 0.05 なら、分布内の5%がOODと判定される閾値
        self.threshold = torch.quantile(kappa_in, target_fpr).item()
        self._is_calibrated = True

        # 検出率（分布外をOODと正しく判定する率）を計算
        tpr = (kappa_out < self.threshold).float().mean().item()

        print(f"Calibrated threshold: {self.threshold:.2f}")
        print(f"  FPR (in-dist as OOD): {target_fpr:.1%}")
        print(f"  TPR (out-dist as OOD): {tpr:.1%}")

        return self.threshold

    def is_ood(self, kappa):
        """κが閾値以下ならOODと判定

        Args:
            kappa: 推定された集中度 [batch, 1] または [batch]

        Returns:
            ood_mask: OODフラグ [batch]
        """
        if not self._is_calibrated:
            warnings.warn(
                "OODDetector is not calibrated. Results may be unreliable.",
                stacklevel=2,
            )
        return kappa.squeeze(-1) < self.threshold

    def get_confidence(self, kappa):
        """κを[0, 1]の確信度スコアに変換

        Args:
            kappa: 集中度 [batch, 1] または [batch]

        Returns:
            confidence: 確信度スコア [batch]
        """
        # シグモイドで[0, 1]に変換
        # threshold周りで0.5になるように調整
        return torch.sigmoid((kappa.squeeze(-1) - self.threshold) / 5.0)


# 使用例
detector = OODDetector(threshold=10.0)

# 仮のκ値
kappa_in_distribution = torch.tensor([[50.0], [30.0], [25.0]])
kappa_out_of_distribution = torch.tensor([[5.0], [3.0], [1.0]])

print("In-distribution:")
print(f"  OOD: {detector.is_ood(kappa_in_distribution)}")
print(f"  Confidence: {detector.get_confidence(kappa_in_distribution)}")

print("Out-of-distribution:")
print(f"  OOD: {detector.is_ood(kappa_out_of_distribution)}")
print(f"  Confidence: {detector.get_confidence(kappa_out_of_distribution)}")
```

</details>

### 座標フリーの分布比較

<details>
<summary>コード例: 07_coordinate_free_comparison.py</summary>

```07_coordinate_free_comparison.py
import torch


def compare_distributions_coordinate_free(kappa1, kappa2):
    """座標系に依存しない分布の比較

    確信度（κ）のみを使った比較。
    座標系の整列なしで、2つのモデルの「確信度」を比較できる。

    Args:
        kappa1, kappa2: 2つのモデルからのκ [batch, 1]

    Returns:
        comparison: 比較結果の辞書
    """
    kappa1 = kappa1.squeeze(-1)
    kappa2 = kappa2.squeeze(-1)

    return {
        "kappa1_mean": kappa1.mean().item(),
        "kappa2_mean": kappa2.mean().item(),
        "kappa_diff": (kappa1 - kappa2).mean().item(),
        "kappa_ratio": (kappa1 / (kappa2 + 1e-8)).mean().item(),
        "more_confident": "model1" if kappa1.mean() > kappa2.mean() else "model2",
        "correlation": torch.corrcoef(torch.stack([kappa1, kappa2]))[0, 1].item(),
    }


# 使用例：2つのモデルの確信度を比較
kappa_model_a = torch.tensor([50.0, 30.0, 45.0, 20.0])
kappa_model_b = torch.tensor([40.0, 35.0, 50.0, 15.0])

comparison = compare_distributions_coordinate_free(kappa_model_a.unsqueeze(-1), kappa_model_b.unsqueeze(-1))

print("Coordinate-free comparison:")
for k, v in comparison.items():
    print(f"  {k}: {v}")
```

</details>

## Visualization Break：Attentionの幾何学を見る

本回の最後に、Attentionと不確実性の関係を可視化するデモを示す。

### デモ：確信度を持つAttention

<details>
<summary>コード例: 07_attention_uncertainty_viz.py</summary>

```07_attention_uncertainty_viz.py
import matplotlib.pyplot as plt
import numpy as np


def visualize_attention_with_uncertainty(sentence, attention_weights, kappa_values):
    """確信度付きAttentionの可視化

    Args:
        sentence: 単語のリスト
        attention_weights: Attention重み [seq_len, seq_len]
        kappa_values: 各トークンのκ値 [seq_len]
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # 左：Attentionヒートマップ
    ax1 = axes[0]
    im = ax1.imshow(attention_weights, cmap="Blues")
    ax1.set_xticks(range(len(sentence)))
    ax1.set_yticks(range(len(sentence)))
    ax1.set_xticklabels(sentence, rotation=45, ha="right")
    ax1.set_yticklabels(sentence)
    ax1.set_xlabel("Key")
    ax1.set_ylabel("Query")
    ax1.set_title("Attention Weights")
    plt.colorbar(im, ax=ax1)

    # 右：確信度バー
    ax2 = axes[1]
    colors = plt.cm.RdYlGn(kappa_values / kappa_values.max())
    ax2.barh(range(len(sentence)), kappa_values, color=colors)
    ax2.set_yticks(range(len(sentence)))
    ax2.set_yticklabels(sentence)
    ax2.set_xlabel("Confidence (κ)")
    ax2.set_title("Per-token Confidence")
    ax2.axvline(x=10, color="r", linestyle="--", label="OOD threshold")
    ax2.legend()

    plt.tight_layout()
    plt.savefig("attention_with_uncertainty.png", dpi=150)
    plt.close()

    print("Saved: attention_with_uncertainty.png")


# サンプルデータで実行
sentence = ["The", "quantum", "cat", "sat", "sleepily"]
np.random.seed(42)

# Attentionは「cat」と「sat」に集中するパターン
attention = np.array(
    [
        [0.3, 0.1, 0.3, 0.2, 0.1],
        [0.1, 0.4, 0.2, 0.2, 0.1],
        [0.2, 0.1, 0.4, 0.2, 0.1],
        [0.1, 0.1, 0.3, 0.4, 0.1],
        [0.2, 0.1, 0.2, 0.2, 0.3],
    ]
)

# 確信度：一般的な単語は高κ、専門用語「quantum」は低κ
kappa = np.array([45.0, 8.0, 50.0, 55.0, 35.0])

visualize_attention_with_uncertainty(sentence, attention, kappa)
```

</details>

この可視化は、以下を示している：

1. **Attentionの分布**：どの単語がどの単語に注目しているか
2. **確信度の違い**：「quantum」のような専門用語は低い $\kappa$ （赤系）、「The」「sat」のような一般的な単語は高い $\kappa$ （緑系）

> [!NOTE]
> **Multi-headでの拡張：** 実際のTransformerでは、複数のヘッドがそれぞれ異なるAttentionパターンを学習する。各ヘッドに対応する「確信度」を可視化することで、どのヘッドがどの種類の関係を確信を持って捉えているかを分析できる。

## まとめ

| 概念 | 定義 | 本回での役割 |
| --- | --- | --- |
| **VAE** | 潜在空間を分布として学習する生成モデル | 分布表現の先駆、ガウス空間の限界の例示 |
| **vMF分布** | 単位球面上の方向分布 | 分布埋め込みの基礎 |
| $\kappa$ （**集中度**） | 分布の集中の度合い | 確信度の定量化 |
| **分布埋め込み** | 点ではなく分布としての表現 | 不確実性の明示的モデル化 |
| **OOD検知** | 分布外入力の検出 | 低 $\kappa$ 領域の活用 |
| **ハルシネーション** | 低確信度での誤った生成 | $\kappa$ 閾値による抑制 |
| **座標フリー比較** | 回転不変量による比較 | 整列なしの分布比較 |

### 本回のポイント

点埋め込みから分布埋め込みへの移行は、単なる技術的な拡張ではない。**「分かりません」と言える能力**をモデルに与えるという、質的な変化である。

ただし、この能力は**設計によって獲得される**ものであり、自動的には得られない。 $L_2$ 正規化しただけでは点は点のままであり、 $\kappa$ のような確信度パラメータは存在しない。vMF分布（または他の方向分布）を明示的にモデル化し、 $\kappa$ を学習する設計を採用して初めて、不確実性を扱える表現が得られる。

プラネタリウムのメタファーで言えば：

> **点だけでなく、その周りの「霧」を見よ。**

星の位置（点埋め込み）だけでなく、星の周りのにじみ具合（分布の広がり）にも情報がある。霧が濃い領域では慎重に、霧が晴れた領域では自信を持って。この使い分けが、信頼できるAIへの道を開く。

## 次回予告

第8回「時間の発見」では、時間軸を持つ表現を議論する。

これまで、埋め込みは「一瞬のスナップショット」として扱ってきた。入力が与えられ、一撃で表現が計算され、出力が返される。しかし、意味の理解や生成は、本当に一瞬で完結するのだろうか。

拡散モデルは「ノイズから意味が徐々に立ち上がる」プロセスを明示的に扱う。Chain of Thoughtは「推論の軌跡」を言語化する。これらの手法は、時間という軸を表現学習に導入することの重要性を示唆している。次回は、この「時間の発見」を幾何学的に探求する。

## 参考文献

### 分布埋め込みとvMF分布

- Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes. *ICLR 2014*. arXiv: [arXiv:1312.6114](https://arxiv.org/abs/1312.6114).
    - VAEの原論文。潜在空間をガウス分布として正則化する枠組みを提案。本回では、ガウス空間の不均一性と球面表現への動機付けの文脈で参照。
- Nagata et al. (2023). [Variance Matters: Detecting Semantic Differences without Corpus/Word Alignment](https://aclanthology.org/2023.emnlp-main.965/). *EMNLP 2023*.
    - 本回の中心的な参考文献。埋め込みの分散が意味の違いを検出する上で重要であることを示した。
- Mardia, K. V., & Jupp, P. E. (2000). *Directional Statistics*. Wiley Series in Probability and Statistics. Wiley.
    - 方向統計学の標準的教科書。vMF分布を含む球面上の分布理論を体系的に扱う。
- Davidson, T. R., Falorsi, L., De Cao, N., Kipf, T., & Tomczak, J. M. (2018). Hyperspherical Variational Auto-Encoders. *UAI 2018*. arXiv: [arXiv:1804.00891](https://arxiv.org/abs/1804.00891).
    - VAEの潜在空間にvMF分布を導入。球面上のサンプリングとreparameterization trickを扱う。
- Diethe, T. (2015). A Note on the Kullback-Leibler Divergence for the von Mises-Fisher distribution. *arXiv:1502.07104*. arXiv: [arXiv:1502.07104](https://arxiv.org/abs/1502.07104).
    - vMF分布間のKLダイバージェンスの厳密な導出。数値安定な計算方法についても議論。本回の実装ノートで近似を使う際の参照先。

### 不確実性の定量化

- Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. *ICML 2016*. arXiv: [arXiv:1506.02142](https://arxiv.org/abs/1506.02142).
    - Dropoutによる不確実性推定の古典的論文。分布埋め込みとは異なるアプローチだが、不確実性の重要性を示した。
- Sensoy, M., Kaplan, L., & Kandemir, M. (2018). Evidential Deep Learning to Quantify Classification Uncertainty. *NeurIPS 2018*. arXiv: [arXiv:1806.01768](https://arxiv.org/abs/1806.01768).
    - 証拠理論に基づく不確実性の定量化。OOD検知への応用を扱う。

### OOD検知

- Hendrycks, D., & Gimpel, K. (2017). A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. *ICLR 2017*. arXiv: [arXiv:1610.02136](https://arxiv.org/abs/1610.02136).
    - OOD検知のベースライン手法。Softmax確率を使った単純だが効果的な方法を提案。
- Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. *NeurIPS 2018*. arXiv: [arXiv:1807.03888](https://arxiv.org/abs/1807.03888).
    - Mahalanobis距離に基づくOOD検知。特徴空間の幾何学を活用。

### ハルシネーションと信頼性

- Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Madotto, A., & Fung, P. (2023). Survey of Hallucination in Natural Language Generation. *ACM Computing Surveys*, 55(12), 1-38. arXiv: [arXiv:2202.03629](https://arxiv.org/abs/2202.03629).
    - ハルシネーションの包括的サーベイ。原因、検出、緩和策を整理。

### 球面上のサンプリング

- Wood, A. T. A. (1994). Simulation of the von Mises Fisher Distribution. *Communications in Statistics - Simulation and Computation*, 23(1), 157-164. DOI: [10.1080/03610919408813161](https://doi.org/10.1080/03610919408813161).
    - vMF分布からの効率的なサンプリングアルゴリズム。実装ノートで使用した方法の原典。
