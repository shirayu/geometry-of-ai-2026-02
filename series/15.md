# 第15回：次の時代を設計する

## 導入：地図を閉じる時

14回にわたる旅を経て、私たちは深層学習という大陸の地図を描いてきた。平坦なユークリッド空間から始まり、球面、双曲空間、そして時間軸を含む動的な多様体へ。測地線、曲率、トポロジーという道具を手に、手法の陳腐化に抗う「空間の形」を探ってきた。

しかし、地図は完成したわけではない。むしろ、この講義で描いた地図は「既知の領域」に過ぎず、未踏の大陸はまだその先に広がっている。最終回となる本講義では、これまでの旅を簡潔に振り返った上で、**未解決の問い**と**未来の設計課題**に焦点を当てる。

ライブラリは古くなるが、幾何学は古くならない。この原則のもと、私たちが次に建てるべき「座標系」を考えよう。

---

## Part 1: 深層学習の旅の振り返り（15分）

### 〜2012年頃：平坦な世界での戦い

かつて機械学習は、ユークリッド空間という平坦な地図上で行われていた。PCA（主成分分析）は共分散行列の固有ベクトルを求め、SVM（サポートベクターマシン）はマージンを最大化し、LDA（線形判別分析）はクラス間分散を最大化した。

これらの手法は優れていたが、ノルムと角度を分離しない設計ゆえに、**意味の距離と不確実性が混線**していた（第2回）。高次元の呪いと格闘しながら、私たちは特徴エンジニアリングという手作業で空間を整形していた。

### 2012年〜2017年：深層学習革命と正規化の萌芽

AlexNet（2012）がImageNetで圧勝し、深層学習時代が幕を開けた。しかし当初、深層ネットワークは「なぜ動くのか」が謎に包まれていた。

転機となったのは正規化技術の登場である。Batch Normalization（2015）、Layer Normalization（2016）は、学習の安定化に寄与しただけでなく、**表現を球面に近づける**という幾何学的効果を持っていた（第3回）。ResNet（2015）の残差学習も、勾配の流れを幾何学的に整理する試みと見なせる。

この時期、私たちは無意識のうちに「平坦な地図」から脱却し始めていた。

### 2017年〜2020年：Transformerと角度中心設計の躍進

"Attention Is All You Need"（2017）の登場で、パラダイムは決定的に変わった。Attentionは内積（角度）を中核に据え、動的に空間を変形する機構である（第6回）。

同時期、顔認識分野ではArcFace（2019）が角度マージンを導入し、von Mises-Fisher分布（vMF）による球面上の確率モデルが注目された（第3回、第5回）。BERT（2018）、GPT（2019）の成功は、**角度中心の設計**が言語にも有効であることを示した。

この段階で、深層学習は「ノルムを抑え、角度で勝負する」方向へ明確に舵を切った。

### 2020年〜現在：時間発展と拡張幾何学

次の転換は、**時間軸の導入**だった。拡散モデル（DDPM, 2020）は、ノイズから意味を立ち上がらせるプロセスを確率的微分方程式（SDE）として定式化した（第9回）。生成は「一撃」ではなく、時間発展する軌跡として捉えられるようになった。

球面化はさらに徹底された。nGPT（2024）は、すべての重みと活性化を球面上に配置する極端な設計を提示し（第3回補論）、従来の「正規化は補助的」という見方を覆した。

空間の形も多様化した。双曲幾何学は階層構造の表現に適していることが示され（第12回）、Mixed-curvature spaces（ICML 2025）は複数の曲率を組み合わせる試みである。

そしてスパース性の活用が加速した。Mixture of Experts（MoE）は、高次元空間の「ほとんどが空」という性質を設計原理に転換し、2025年以降急速に普及している（第13回）。Llama 4（2025）のような大規模モデルがMoEを採用したことは、この流れを象徴する。

トポロジーという新しい顕微鏡も導入された。Topological Data Analysis（TDA）は、形ではなく「穴」や「ループ」といった構造的不変量を測る（第14回）。RoPE（Rotary Position Embedding）は、回転による位置表現を標準化し、角度の幾何学をさらに洗練させた。

> [!NOTE]
> **キーメッセージ：**
> - 「How（手法）」から「Why（空間の形）」へ
> - ライブラリは古くなるが、幾何学は古くならない
> - **連続と離散の界面**を常に意識せよ
> - **スパース性**は呪いではなく設計原理である

---

## Part 2: 未解決問題と未来（20分）

過去を振り返ることは、未来を設計するためだ。ここでは、本講義が触れてきたテーマに関連する**6つの未解決問題**を提示する。これらは技術的課題であると同時に、哲学的・数学的問いでもある。

### 問い1：統一多様体は存在するのか？

現在の深層学習は、モダリティごとに異なる空間を持つ。画像はCNN的な局所構造、言語はTransformer的な系列構造、音声は時間周波数構造。マルチモーダルモデル（CLIP, Flamingo等）は、これらを**共通の埋め込み空間**に射影しようとする（第11回）。

しかし、問いは残る：

- **すべてのモダリティを単一の球面（または双曲空間）に埋め込むことは可能か？**
- それとも、各モダリティは異なる曲率・次元の多様体上にあり、それらを「翻訳」する方が自然なのか？
- 生物の脳はどうしているのか？視覚野、聴覚野、言語野は異なる構造を持つが、最終的に統合される。その幾何学は？

> [!CAUTION]
> 「統一」が常に望ましいとは限らない。異なる空間を保ったまま接続する「ブリッジ」の方が、柔軟性が高い可能性もある。

### 問い2：離散と連続の最適な界面は？

深層学習は本質的に**連続と離散の境界**で動作する。重みは連続だが、出力はしばしば離散（argmax、サンプリング）。この界面は最適なのか？

- 現在の argmax や Gumbel-Softmax は、連続空間から離散選択への「雑な射影」ではないか？
- より「滑らかな」離散化は可能か？量子コンピューティングの重ね合わせ状態との接続は？
- 第4回補論で触れた「離散化の幾何学」は、まだ発展途上である。

**具体例：** 機械翻訳で「次の単語」を選ぶとき、argmaxは最も確率の高い1単語だけを選ぶ。しかし、複数の候補が僅差の場合、その「曖昧さ」自体が意味を持つのではないか？離散化のタイミングを遅らせることで、より豊かな表現が可能になるかもしれない。

### 問い3：意識は多様体で記述できるのか？

これは科学的問いであると同時に、哲学的問いである。意識のハードプロブレム（なぜクオリアが存在するのか）に、幾何学は何を言えるのか？

**統合情報理論（IIT）** は、意識を「情報の統合度」として定量化する試みである。これを幾何学的に解釈すれば、意識とは「多様体上の自己参照ループ」かもしれない。

- 自己認識 = 表現空間が自分自身を埋め込むこと？
- クオリアの違い（「赤さ」と「青さ」）= 異なる部分多様体の形状？
- 意識の連続性 = 多様体上の連結な経路の存在？

> [!WARNING]
> これは思考実験である。現時点で、意識を数学的に完全に記述できる理論は存在しない。しかし、問うこと自体が次の発見を生む。

### 問い4：曲率は学習可能か？

第12回で双曲幾何学を扱い、階層構造には負の曲率が適していることを見た。しかし、最適な曲率は**タスク依存**である。ならば、曲率自体を学習パラメータにできないか？

**最近の研究動向：**

- **Mixed-curvature spaces（ICML 2025）：** 同じ空間内で、異なる領域が異なる曲率を持つ。例えば、階層的なカテゴリは双曲、並列的な属性はユークリッド、周期的な時間は球面、といった具合に。
- **学習可能な曲率パラメータ：** 一部の研究では、曲率を勾配降下で最適化する試みがある。しかし、曲率が変わると測地線も変わるため、最適化の安定性が課題。

**未解決：** 局所的に曲率が滑らかに変わる多様体（可変曲率多様体）での効率的な学習アルゴリズムは？

### 問い5：MoEの先にあるものは？

第13回で扱ったMoEは、離散的な「Expert選択」に基づく。しかし、Expertの数を増やし続けると、どこかで連続的な空間に近づくのではないか？

**思考実験：**

- **Expert数を無限に増やしたら？** → 連続的なExpert空間？各点が「微小な専門家」？
- **Soft MoE（ICLR 2024）** は、離散ルーティングを連続化する試みだが、まだ完全な連続空間ではない。
- **「専門家」の概念自体を学習できるか？** 現在は人間が「Expertを8個作る」と決めているが、最適な分割を自動発見できないか？

> [!NOTE]
> この問いは、「離散と連続の界面」（問い2）とも深く関連する。MoEを極限まで押し進めると、新しい計算パラダイムが見えるかもしれない。

### 問い6：説明可能性とは何か？（幾何学的に）

AIの判断を説明せよ、という社会的要請がある。しかし、「説明」とは幾何学的に何を意味するのか？

**幾何学的解釈：**

- **「なぜこう判断したか」** = 「入力から出力まで、多様体上のどの経路を通ったか」
- **「この特徴が重要だった」** = 「この方向への射影が大きかった」
- **「AとBは似ている」** = 「測地線距離が近い」

問題は、これらの幾何学的事実を**人間に伝達可能な形**で表現することだ。高次元空間の経路を3次元に射影すると、本質が失われる。トポロジー的な「決定境界の形」（穴の数、連結成分）を可視化する技術（TDA的アプローチ）は、一つの方向性だが、まだ発展途上である。

**未解決：** 高次元多様体の「本質的な構造」を、低次元でも保存する射影法は？

---

## Part 3: ワークショップ（45分）

理論だけでは不十分だ。ここでは、受講者自身が「次の座標系」を構想するワークショップを行う。

### 課題：「あなたが考える次の座標系は何か？」

以下の進行で、5-6人のグループに分かれて議論し、発表する。

#### タイムライン

1. **ブレインストーミング（15分）**
   - 制約なく自由に発想する
   - 「実現可能性」は一旦脇に置く
   - ホワイトボードやメモを使い、アイデアを可視化

2. **発表準備（5分）**
   - グループで最も刺激的なアイデアを1つ選ぶ
   - スライド1-2枚にまとめる（手書きスケッチでも可）
   - 以下を含めること：
     - 提案する座標系の名前
     - どのような問題を解決するか
     - 既存の座標系との違い

3. **発表（各グループ5分）**
   - 簡潔に、しかし情熱を持って
   - 質疑応答は発表後にまとめて行う

#### 発表テーマ例（インスピレーション）

グループは以下のテーマに縛られる必要はないが、考えるヒントとして：

- **時空を統合した4次元多様体での学習**
  - 時間を単なるインデックスではなく、空間と対等な次元として扱う
  - 相対性理論的なメトリックは必要か？
  
- **離散と連続を橋渡しするハイブリッド幾何学**
  - グラフ構造と多様体を統一的に扱う空間
  - 離散ノードを「多様体上の特異点」として解釈

- **感情や美的感覚の多様体**
  - 「悲しみ」と「喜び」の測地線距離は？
  - 美しさを測る曲率は存在するか？

- **ソーシャルネットワークの動的多様体**
  - 人間関係を時間発展する双曲空間で表現
  - コミュニティ = 曲率が局所的に高い領域？

- **創造性を測る幾何学的指標**
  - 新規性 = 既存の表現空間からの測地線距離？
  - 創造性の高い生成 = 多様体の境界を拡張すること？

- **「連続的MoE」の設計**
  - Expertを離散的に選ぶのではなく、連続的な「Expert場」から値を読み取る
  - 問い5への具体的な回答

#### クラス全体でのディスカッション（10分）

すべての発表後、以下を問う：

- どの提案が最も刺激的だったか？なぜ？
- 実現可能性は？技術的障壁は何か？
- 必要な数学的道具は？（微分幾何、トポロジー、確率論…）
- 倫理的・社会的含意はあるか？

> [!IMPORTANT]
> このワークショップに「正解」はない。目的は、**問いを立てる力**を養うことである。

---

## Part 4: 最終メッセージ（10分）

### 深層学習以前：ユークリッド空間での機械学習

かつて機械学習は、平坦な地図上での戦いだった。特徴エンジニアリングという手作業で空間を整形し、線形分類器という単純な道具で境界を引いた。それでも、多くの問題は解けた。

### 今：「物理法則」としてのAI

今、深層学習は単なる統計的手法を超えつつある。拡散モデルは熱力学と共鳴し、Transformerは動的な幾何学を操り、MoEはスパース性を設計原理に転換した。

AIは「物理法則」に従い始めている。ノイズから秩序が立ち上がり、情報が流れ、構造が自己組織化する。私たちがやっているのは、パラメータの調整ではなく、**空間の設計**である。

### 未来：？

しかし、未来は白紙だ。

この講義で学んだのは「過去の座標系」である。次の座標系は、まだ誰も知らない。それは曲がっているかもしれないし、離散と連続が溶け合っているかもしれない。時間軸が複数あるかもしれないし、私たちがまだ名前を持たない構造を持つかもしれない。

**それを発見しにいこう。**

---

## まとめ：3つの原則

最後に、この講義を通じて伝えたかった3つの原則をまとめる。

### 1. 流行を追うな、空間の形を問え

来年には新しいモデルが登場し、今年のSOTAは過去になる。しかし、幾何学の視点は残る。

どんな新しい手法が出ても、それが**どの多様体上で、どの測地線に沿って動いているか**を問え。本質を見抜く目を養え。

### 2. 数式を恐れるな、しかし数式に溺れるな

数式は「道具」であって「目的」ではない。リーマン計量が書けなくても、「空間が曲がっている」という直感があれば十分な場面は多い。

一方で、直感だけでは限界がある。厳密性は、直感を裏切る現象（赤道集中、距離の集中）を発見する。

**直感と厳密性のバランス**を保て。

### 3. 次のプラネタリウムを建てよう

第3回で、私たちは「プラネタリウム」という比喩を導入した。球面上に星（表現）を配置し、その配置が意味の地図になる。

しかし、それは一つの座標系に過ぎない。次のプラネタリウムは、球面ではないかもしれない。双曲空間かもしれないし、時間発展する多様体かもしれない。あるいは、まだ誰も見たことのない形をしているかもしれない。

**それを建てるのは、あなただ。**

---

## 謝辞

本講義は、AIとの対話を通じて構築された。完璧ではないが、誠実であろうと努めた。誤りがあれば、それは私たちの責任である。

深層学習という分野は、まだ若い。この講義で触れた内容の多くは、10年後には古びているかもしれない。しかし、「空間の形を問う」という姿勢は、古びないと信じている。

地図を閉じ、コンパスを手に、未知の大陸へ踏み出そう。

---

## 参考文献

本講義全体に関わる基本文献を挙げる。各回の詳細な文献リストは、該当回を参照されたい。

### 深層学習と幾何学の接続

- Amari, S. (2016). *Information Geometry and Its Applications*. Springer.
- Bronstein, M. et al. (2021). "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges." arXiv:2104.13478.

### Transformer とAttention

- Vaswani, A. et al. (2017). "Attention Is All You Need." NeurIPS.

### 拡散モデル

- Ho, J. et al. (2020). "Denoising Diffusion Probabilistic Models." NeurIPS.
- Song, Y. et al. (2021). "Score-Based Generative Modeling through Stochastic Differential Equations." ICLR.

### Mixture of Experts

- Fedus, W. et al. (2022). "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity." JMLR.
- Dai, D. et al. (2024). "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models." arXiv:2401.06066.

### 双曲幾何学とAI

- Nickel, M. & Kiela, D. (2017). "Poincaré Embeddings for Learning Hierarchical Representations." NeurIPS.
- Chami, I. et al. (2019). "Hyperbolic Graph Convolutional Neural Networks." NeurIPS.

### Topological Data Analysis

- Carlsson, G. (2009). "Topology and Data." *Bulletin of the AMS* 46(2):255-308.

---

**本講義はこれで終了する。次の講義「情報幾何学とAIの動態論」で再会しよう。**
