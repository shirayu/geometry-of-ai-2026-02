# 第0回：幾何学という言語 ～この講義の羅針盤～

## 注意事項

本回は数学的道具の導入であり、特定のモデルへの適用条件はまだ問わない。
ただし「多様体」「測地線」等の概念を深層学習に適用する際は、各回で改めて「どの条件下で成り立つか」を確認する。

## 導入：なぜ幾何学なのか

深層学習の手法は急速に変化する。2年前の最先端は今日の常識となり、明日には陳腐化するかもしれない。しかし、その背後にある「空間の形」についての問いは、驚くほど普遍的である。

本講義は、個々の手法（How）ではなく、なぜその設計が有効なのか（Why）を問う。その答えを与えるのが **幾何学** という言語である。

幾何学は、空間の「形」を記述する数学だ。平らな紙の上で成り立つ定理が、地球儀の上では崩れる。深層学習の表現空間も同様に、私たちの素朴な直感が通用しない「曲がった空間」であることが多い。この講義では、その曲がり方を理解し、設計に活かすための道具を整える。

## 多様体：局所と大域の二重構造

### 多様体とは何か

**多様体（manifold）** とは、局所的にはユークリッド空間に見えるが、大域的には異なる構造を持つ空間のことである。

最も身近な例は地球である。あなたが立っている地面は、目に見える範囲では平らに見える。地図を広げれば、近所の道路は直線的に描かれ、距離も正確に測れる。しかし、地球全体を見れば、それは球面である。平らな地図をいくら貼り合わせても、球面を完全に覆うことはできない。必ずどこかに歪みや切れ目が生じる。

これが多様体の本質だ：

| 視点 | 地球の場合 | 深層学習の表現空間 |
| --- | --- | --- |
| **局所的** | 近所は平らに見える | 小さな摂動は線形近似できる |
| **大域的** | 実際は球面 | 全体の構造は非線形で複雑 |

> [!NOTE]
> **多様体とリーマン多様体の区別：** 厳密には、多様体とは「局所的にユークリッド空間と同相」な空間であり、距離や角度は定義されていない。距離・角度・曲率を語るには、リーマン計量を入れた **リーマン多様体** が必要である。本講義では、深層学習の文脈で自然に計量が入る（内積が定義される）場合を主に扱うため、両者を厳密に区別しないことが多い。

### なぜ多様体が重要か

深層学習において、データは高次元空間に埋め込まれるが、その「意味のある部分」は多くの場合、より低次元の多様体上に集中している。これを **多様体仮説（manifold hypothesis）** と呼ぶ。

例えば、顔画像のデータセットを考えよう。各画像は数百万画素のベクトルとして表現されるが、「顔らしさ」を決める自由度（表情、向き、照明など）はせいぜい数十から数百程度である。つまり、顔画像は高次元空間の中の低次元多様体上に存在すると考えられる。

この多様体の「形」を理解することが、表現学習の本質である。

> [!NOTE]
> **多様体仮説の限界：** 多様体仮説は多くの実験的証拠に支持されているが、実データが「厳密な多様体」であるとは限らない。(i) ノイズや離散性により滑らかな多様体にならない、(ii) 局所次元が場所によって変わる、(iii) クラス境界付近で自己交差する、といった状況が起こりうる。実務では「多様体」というより **「低次元構造っぽさ」** として捉える方が安全である。後続回では、「多様体と見なせるためにどの仮定が必要か」を明示する。

## 測地線：曲がった空間での「最短」

### 測地線とは何か

平らな空間では、2点間の最短経路は直線である。では、曲がった空間ではどうか。

**測地線（geodesic）** とは、多様体上で「局所的に長さが停留（極小）」となる曲線である。地球上では、これは **大円（great circle）** に対応する。東京からニューヨークへの最短航路が、地図上では奇妙に曲がって見えるのは、球面上の測地線を平面に投影しているからだ。

> [!NOTE]
> 測地線は「最短経路」と同値ではない。正確には「長さの変分が0になる（停留点）」という条件で定義される。球面上の2点を結ぶ大円は2本の弧を与えるが、短い方だけが「最短」であり、長い方は測地線ではあるが最短ではない。また、球面の反対点（対蹠点）では、どの大円も同じ長さになり、測地線が一意に定まらない。

| 空間 | 測地線の形 | 例 |
| --- | --- | --- |
| ユークリッド空間 | 直線 | 紙の上で定規を引く |
| 球面 | 大円 | 飛行機の国際線ルート |
| 双曲空間 | 双曲線の一部 | 第12回で詳しく扱う |

### 深層学習における測地線

表現空間が球面（単位球面上に正規化されたベクトル）として **明示的に設計されている** 場合、2つの表現間の「自然な補間」は測地線に沿って行われるべきである。

単純な線形補間（ユークリッド空間での直線）では、中間点が球面から外れてしまう。これに対し、**球面線形補間（Slerp: Spherical Linear Interpolation）** は、球面上の測地線に沿った補間を与える（Shoemake, 1985）。

> [!CAUTION]
> **Slerpが適切なのは「球面制約が設計に組み込まれている場合」に限られる。** 埋め込みが正規化されていない場合、あるいはコサイン類似度を使っていても学習目的が球面幾何と整合しない場合は、Slerpが「意味的に自然な補間」を与える保証はない。「意味の線形性」は幾何というよりタスクや損失関数の設計に依存する。

```python
import torch

def slerp(v0, v1, t):
    """球面線形補間（Slerp）
    
    Args:
        v0, v1: 単位ベクトル（正規化済み）
        t: 補間パラメータ（0から1）
    
    Returns:
        球面上の測地線に沿った補間点
    """
    # 内積からなす角を計算
    dot = torch.clamp(torch.sum(v0 * v1), -1.0, 1.0)
    theta = torch.acos(dot)
    
    # 特殊ケース：ほぼ同じ方向
    if theta.abs() < 1e-6:
        return v0
    
    # Slerp公式
    sin_theta = torch.sin(theta)
    return (torch.sin((1 - t) * theta) / sin_theta) * v0 + \
           (torch.sin(t * theta) / sin_theta) * v1
```

> [!CAUTION]
> Slerpは $v_0$ と $v_1$ が反対方向（内積≈-1）のとき数値的に不安定になる。この場合、測地線は一意に定まらない（どの大円でも同じ長さ）。実装では、内積が-1に近い場合の例外処理が必要。

## 曲率：空間の「曲がり具合」

### 曲率とは何か

**曲率（curvature）** は、空間がどれだけ「平ら」から逸脱しているかを定量化する。直感的には、以下のように区別できる：

| 曲率 | 特徴 | 例 | 深層学習での用途 |
| --- | --- | --- | --- |
| **正（positive）** | 三角形の内角の和が180°より大きい | 球面 | 第3回：球面埋め込み |
| **ゼロ（zero）** | 三角形の内角の和がちょうど180° | 平面 | 古典的なユークリッド空間 |
| **負（negative）** | 三角形の内角の和が180°より小さい | 双曲面（鞍型） | 第12回：階層構造の表現 |

### 曲率の直感的理解

球面上で三角形を描いてみよう。北極から赤道上の2点へ線を引くと、赤道上の辺と合わせて「三角形」ができる。この三角形の内角の和は、180°を超える。極端な例では、北極から赤道まで下り、赤道を90°移動し、北極へ戻ると、各角が90°の三角形ができる。内角の和は270°だ。

これが **正の曲率** の特徴である。空間が「膨らんでいる」ために、三角形が「太る」。

逆に、鞍型の曲面（プリングルスの形）では、三角形は「痩せる」。これが **負の曲率** である。

### 曲率と平行移動

曲がった空間のもう一つの特徴は、ベクトルを「平行に」移動しても、元に戻ったときに向きがズレることだ。

地球上で北極に立ち、南を向いた矢印を持っているとしよう。この矢印を「向きを変えずに」赤道まで運び、赤道に沿って90°移動し、北極へ戻る。すると、矢印は最初の向きから90°ズレている。

この「ズレ」の量が曲率を反映している。深層学習の文脈では、表現空間の曲率が学習ダイナミクスに影響を与える（第4回で詳述）。

## リーマン計量：「距離の測り方」を定める

### リーマン計量とは何か

**リーマン計量（Riemannian metric）** は、多様体上で「距離」や「角度」を測るための数学的装置である。より正確には、各点における接空間（その点での「平らな近似」）に内積を定めるものだ。

平らな空間では、距離は単純なユークリッドノルムで測れる：

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\| = \sqrt{\sum_i (x_i - y_i)^2}$$

しかし、曲がった空間では「どの方向に進むか」によって距離の測り方が変わりうる。リーマン計量は、この「方向依存の距離感」を定式化する。

### 深層学習での例：フィッシャー情報行列

確率分布の空間を考えよう。パラメータ $\theta$ で決まる分布 $p(x; \theta)$ の「近さ」をどう測るか。

素朴にはパラメータ空間でのユークリッド距離 $\|\theta_1 - \theta_2\|$ を使いたくなる。しかし、これは問題がある。同じパラメータの「幅」でも、分布の変化量は異なるからだ。

例えば、正規分布 $N(\mu, \sigma^2)$ で、 $\mu$ を0.1動かすのと、 $\sigma$ を0.1動かすのでは、分布の「見た目」の変化量が全く違う。

**フィッシャー情報行列（Fisher Information Matrix）** は、この「分布の変化しやすさ」を反映したリーマン計量を与える：

$$g_{ij}(\theta) = \mathbb{E}_{p(x;\theta)}\left[\frac{\partial \log p(x;\theta)}{\partial \theta_i} \frac{\partial \log p(x;\theta)}{\partial \theta_j}\right]$$

この計量を使えば、パラメータ空間での「1歩」が、分布空間での「同じ大きさの変化」に対応するようになる。

## 情報幾何学：確率分布の多様体

### 甘利俊一の情報幾何学

**情報幾何学（Information Geometry）** は、確率分布の空間を多様体として扱う学問である。甘利俊一によって体系化され、統計学・機械学習・物理学に広く応用されている。

その核心的なアイデアは：

> **確率分布の空間にリーマン計量を入れると、統計的推論が幾何学的な言葉で記述できる。**

具体的には：

| 統計的概念 | 幾何学的解釈 |
| --- | --- |
| 最尤推定 | 多様体への射影 |
| KLダイバージェンス | 非対称な「距離」（ブレグマンダイバージェンス） |
| 自然勾配 | リーマン計量に沿った勾配 |
| 十分統計量 | 多様体の座標系の選び方 |

### 自然勾配と学習の効率

通常の勾配降下法は、パラメータ空間でのユークリッド勾配を使う。しかし、パラメータの「1歩」が分布の「どれだけの変化」に対応するかは、現在地によって異なる。

**自然勾配（Natural Gradient）** は、フィッシャー情報行列を使って勾配を補正することで、分布空間での「等しい変化量」を実現する（Amari, 1998）：

$$\tilde{\nabla}_ \theta \mathcal{L} = G(\theta)^{-1} \nabla_ \theta \mathcal{L}$$

ここで $G(\theta)$ はフィッシャー情報行列。

直感的には、「地形に応じて歩幅を調整する」ようなものだ。急な坂（分布が敏感に変わる方向）では小さく、緩やかな坂（分布があまり変わらない方向）では大きく歩く。

> [!NOTE]
> 自然勾配の計算は、フィッシャー情報行列の逆行列を必要とするため、大規模モデルでは直接計算が困難である。Adam（Kingma & Ba, 2015）等の適応的オプティマイザは、勾配の2次モーメントを座標ごとに正規化する「前処理付き勾配法」であり、特定の仮定のもとで自然勾配に **似た形** になることがある。ただし、両者は同一ではなく、「Adam＝自然勾配の近似」と言い切るのは過剰主張である。

### 本講義の立ち位置

本講義は、情報幾何学の精神を受け継ぎつつ、深層学習の具体的な幾何構造に焦点を当てる。特に：

- **球面**（第3回〜第5回）：正規化された表現空間
- **双曲空間**（第12回）：階層構造の表現
- **位相**（第14回）：形ではなく「穴」を測る

これらは、情報幾何学の一般論というより、実践的な設計指針として扱う。

## 本講義の地図：平坦から曲がった世界へ

### 三つの世界

本講義は、表現空間の「形」を軸に、以下の三つの世界を旅する：

```txt
平坦（古典ML）→ 球面（現代DL）→ その先（双曲・トポロジー）
```

| 世界 | 曲率 | 典型的な手法 | 講義での位置 |
| --- | --- | --- | --- |
| **平坦** | ゼロ | PCA, SVM, Word2Vec（初期） | 第1回〜第2回 |
| **球面** | 正 | nGPT, ArcFace, CLIP | 第3回〜第7回 |
| **双曲** | 負 | Poincaré Embedding, HypLoRA | 第12回 |
| **位相** | - | TDA, Persistent Homology | 第14回 |

> [!NOTE]
> **表の分類軸について：** ここでの「球面」「双曲」は、**表現空間の設計において明示的にその幾何構造が導入されている** ことを指す。例えばnGPTは単位球面上での正規化を明示的に行い、Poincaré Embeddingは双曲空間での埋め込みを前提とする。ただし、これらの手法が「本質的に」その幾何を必要とするかどうかは議論の余地がある。各回で、何が幾何学的制約であり何がそうでないかを区別する。

### 連続と離散の界面

深層学習には、避けて通れない緊張がある。**連続空間での「旅」が、最終的に離散トークン（または離散的な決定）に「着地」しなければならない** という点だ。

Softmaxは、連続的なロジットを確率分布に変換する。Attentionは、連続的な類似度を重みに変換する。これらの操作は、連続と離散の界面で起きている。

第4回では、この界面をSoftmaxと情報幾何学の観点から再検討する。

### スパース性：高次元空間の「ほとんどが空」

高次元空間には、直感に反する性質がある。第13回で詳しく扱うが、ここで予告しておこう：

> 高次元空間では、ランダムに選んだ2つのベクトルは、ほぼ確実に直交に近い。

この性質は、呪いにも祝福にもなりうる。

- **呪い**：距離の区別がつきにくくなる（kNNの困難）
- **祝福**：互いに干渉しない表現を大量に詰め込める可能性

Mixture of Experts（MoE）は、高次元空間を活用するアーキテクチャの一例である。各Expertが異なる部分空間を担当し、入力に応じて適切なExpertだけを活性化する（疎な活性化）。

> [!IMPORTANT]
> **MoEの設計原理の中心は「直交性」ではない。** MoEが成立する主な要因は、(1) 条件付き計算（疎な活性化）による計算効率、(2) 容量分割によるパラメータ効率、(3) ルーティング学習の安定化技術、である（Shazeer et al., 2017; Fedus et al., 2022）。高次元での直交性は「Expert間の干渉が起きにくい」という **直感の補助** にはなるが、MoEの本質を幾何だけで説明することはできない。

## 幾何学の限界：形は測れても、正しさは測れない

### 本講義が扱わないこと

ここで、重要な限界を明示しておく。

幾何学は「空間の形」を測る道具だが、**「その空間に注ぎ込まれたデータの真偽」** については沈黙している。

例えば、「1+1=2」と「1+1=3」の埋め込みベクトルは、どちらも正規化すれば単位球面上の点になる。幾何学的には、両者は単に「異なる方向を向いた単位ベクトル」に過ぎない。どちらが「正しい」かは、幾何学の範疇外である。

| 幾何学が見るもの | 真偽判定に必要なもの |
| --- | --- |
| 距離・曲率・位相（内在的性質） | 世界知識・論理規則・観測事実（外部参照） |

この限界については、[Appendix 2「多様体の純度問題」](appendix.2.md) で詳しく議論する。

> [!IMPORTANT]
> 正規化は「形式の安定」を与えるが、「内容の真偽」には中立である。幾何学に真偽判定を期待するのは、定規に善悪の判断を期待するようなものだ。

### 外部参照の必要性

現時点で、データの真偽を判定するには、幾何学の外にある道具が必要である：

- **Tool Use**：計算エンジンや検索エンジンで事実を検証
- **合成データ**：論理エンジンで「正しい」データを生成
- **人手ラベリング**：人間が「良い/悪い」を判定

これらはいずれも「幾何学的手法」ではない。本講義は、幾何学が解決できる問題と、解決できない問題を区別することを重視する。

## 続編への接続：静的から動的へ

本講義「統一視点」は、**空間の形**（どこに配置されているか）を扱う。しかし、深層学習には「形」だけでは捉えられない側面がある。**情報がどう流れるか**（どう移動するか）という動的な視点だ。

続編「動態論」では、この動的な視点を導入する：

| 講義 | 問い | 主語 | 典型的な対象 |
| --- | --- | --- | --- |
| 統一視点（本講義） | 空間の形は何か | 表現（embedding） | 球面、双曲空間、情報幾何 |
| 動態論（続編） | 情報はどう流れるか | 軌道（trajectory） | 拡散、フロー、推論過程 |

例えば、拡散モデルは「ノイズからデータへの旅」を記述する。この旅を理解するには、**連続の式**（質量保存）、**速度場**（どの方向に流れるか）、**作用**（どの経路が最適か）といった、流体力学の言葉が必要になる。

動態論の第0回では、これらの道具を導入する。本講義で「空間の形」を理解した後、続編で「情報の流れ」を学ぶことで、深層学習の幾何学的な全体像が見えてくる。

## 実装ノート：幾何学的直感を検証する

> [!NOTE]
> 以下のコードは PyTorch >= 1.9 を前提とする。再現性のために `torch.manual_seed(42)` 等でシードを固定することを推奨。

### 高次元での直交性の確認

高次元空間でランダムベクトルがほぼ直交することを、実験で確認しよう：

```python
import torch
import matplotlib.pyplot as plt

def check_random_orthogonality(dims, n_pairs=1000):
    """高次元でランダムベクトルの内積分布を調べる"""
    results = []
    
    for d in dims:
        # 単位球面上の一様ランダムベクトル
        v1 = torch.randn(n_pairs, d)
        v1 = v1 / v1.norm(dim=1, keepdim=True)
        v2 = torch.randn(n_pairs, d)
        v2 = v2 / v2.norm(dim=1, keepdim=True)
        
        # 内積の計算
        dots = (v1 * v2).sum(dim=1)
        results.append({
            'dim': d,
            'mean': dots.mean().item(),
            'std': dots.std().item()
        })
    
    return results

# 実験
dims = [10, 100, 1000, 10000]
results = check_random_orthogonality(dims)

for r in results:
    print(f"d={r['dim']:5d}: mean={r['mean']:+.4f}, std={r['std']:.4f}")

# 期待される結果：
# d=   10: mean≈0, std≈0.32
# d=  100: mean≈0, std≈0.10
# d= 1000: mean≈0, std≈0.032
# d=10000: mean≈0, std≈0.010
# → 次元が上がると標準偏差が 1/√d で減少
```

### 測地線（Slerp）と線形補間の違い

```python
import torch
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def slerp(v0, v1, t):
    """球面線形補間"""
    dot = torch.clamp(torch.sum(v0 * v1), -1.0, 1.0)
    theta = torch.acos(dot)
    if theta.abs() < 1e-6:
        return v0
    sin_theta = torch.sin(theta)
    return (torch.sin((1 - t) * theta) / sin_theta) * v0 + \
           (torch.sin(t * theta) / sin_theta) * v1

def lerp(v0, v1, t):
    """線形補間（正規化なし）"""
    return (1 - t) * v0 + t * v1

def lerp_normalized(v0, v1, t):
    """線形補間して正規化"""
    v = (1 - t) * v0 + t * v1
    return v / v.norm()

# 3次元で可視化
v0 = torch.tensor([1.0, 0.0, 0.0])
v1 = torch.tensor([0.0, 1.0, 0.0])
ts = torch.linspace(0, 1, 20)

slerp_points = torch.stack([slerp(v0, v1, t) for t in ts])
lerp_points = torch.stack([lerp(v0, v1, t) for t in ts])
lerp_norm_points = torch.stack([lerp_normalized(v0, v1, t) for t in ts])

# 結果：
# - slerp_points: 球面上の大円に沿った点列
# - lerp_points: 球面を突き抜ける直線（中点でノルム≈0.71）
# - lerp_norm_points: slerpとほぼ同じ（この場合）
```

> [!NOTE]
> 2点間の角度が小さい場合、Slerpと正規化付き線形補間の差は小さい。角度が大きくなると差が顕著になる。

## まとめ

| 概念 | 定義 | 深層学習での役割 |
| --- | --- | --- |
| **多様体** | 局所的に平坦、大域的に曲がった空間 | 表現空間の構造を記述 |
| **測地線** | 曲がった空間での「最短経路」 | 表現間の自然な補間（Slerp等） |
| **曲率** | 空間の曲がり具合の定量化 | 正規化・埋め込み設計の指針 |
| **リーマン計量** | 距離・角度の測り方 | 自然勾配、学習ダイナミクスの理解 |
| **情報幾何学** | 確率分布を多様体として扱う | Softmax、KL、温度の幾何学的解釈 |

### ゴール

**数式は後から付いてくる。まずは「幾何学的直感」を獲得する。**

この講義を通じて獲得してほしいのは、以下のような問いを自然に発することができる直感である：

- 「この表現空間は、どんな形をしているのか」
- 「この正規化は、空間をどう変形しているのか」
- 「この距離関数は、何を『近い』と定義しているのか」
- 「この手法の本質は、幾何学的に何をしているのか」

### 次回予告

第1回「かつての地図」では、古典的な機械学習が前提としていたユークリッド空間観を振り返る。PCA、SVM、LDAといった手法が、いかに「平らな世界」を暗黙の前提としていたかを確認し、見落とされていた構造の兆しを掘り起こす。

## 参考文献

### 情報幾何学・リーマン幾何学（数学的基礎）

- Amari, S. (2016). *Information Geometry and Its Applications*. Applied Mathematical Sciences, Vol. 194. Springer Japan. DOI: [10.1007/978-4-431-55978-8](https://doi.org/10.1007/978-4-431-55978-8)
    - 情報幾何学の標準的教科書。フィッシャー情報行列、自然勾配、双対構造などを体系的に扱う。
- Amari, S. (1998). Natural Gradient Works Efficiently in Learning. *Neural Computation*, 10(2), 251–276. DOI: [10.1162/089976698300017746](https://doi.org/10.1162/089976698300017746)
    - 自然勾配の一次文献。パラメータ空間の計量構造が学習効率に与える影響を示した古典。
- Lee, J. M. (2018). *Introduction to Riemannian Manifolds* (2nd ed.). Graduate Texts in Mathematics, Vol. 176. Springer. DOI: [10.1007/978-3-319-91755-9](https://doi.org/10.1007/978-3-319-91755-9)
    - リーマン幾何学の標準的教科書。測地線、曲率、平行移動などの厳密な定義を参照できる。
    - 著者公式ページ（正誤表・補足）: [https://sites.math.washington.edu/~lee/Books/RM/](https://sites.math.washington.edu/~lee/Books/RM/)

### 多様体仮説

- Fefferman, C., Mitter, S., & Narayanan, H. (2016). Testing the Manifold Hypothesis. *Journal of the American Mathematical Society*, 29(4), 983–1049. DOI: [10.1090/jams/852](https://doi.org/10.1090/jams/852)
    - 多様体仮説を理論的にテストする立場の代表的論文。
    - arXiv版: [arXiv:1310.0425](https://arxiv.org/abs/1310.0425) (2013)

### 幾何学的深層学習（サーベイ）

- Bronstein, M. M., Bruna, J., Cohen, T., & Veličković, P. (2021). Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. *arXiv:2104.13478*.
    - 幾何学的深層学習の統一的サーベイ。対称性・不変性・同変性の観点から深層学習を俯瞰する。
    - Proto-book（章立て・図版が充実）: [https://geometricdeeplearning.com/](https://geometricdeeplearning.com/)

### 最適化（Adam・自然勾配との関係）

- Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. *ICLR 2015*. arXiv: [arXiv:1412.6980](https://arxiv.org/abs/1412.6980) (2014).
    - Adamオプティマイザの一次文献。本文で「自然勾配に似た形になることがある」と述べた際の参照先。

### Mixture of Experts（MoE）

- Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. *ICLR 2017*. arXiv: [arXiv:1701.06538](https://arxiv.org/abs/1701.06538).
    - 現代的なMoEの基礎論文。スパースゲーティングと条件付き計算の設計を導入。
- Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. JMLR, 23(120), 1–39. (arXiv:[arXiv:2101.03961](https://arxiv.org/abs/2101.03961), 2021)
    - MoEの訓練安定化技術（Load Balancing Loss等）を扱う。本文で「MoEの設計原理の中心」として言及した内容の参照先。

### 球面補間（Slerp）

- Shoemake, K. (1985). Animating Rotation with Quaternion Curves. *SIGGRAPH '85 Proceedings*, 245–254. DOI: [10.1145/325334.325242](https://doi.org/10.1145/325334.325242)
    - 球面線形補間（Slerp）の古典的文献。四元数の文脈で導入されたが、一般の単位球面にも適用可能。
