# 第0回：幾何学という言語 ～この講義の羅針盤～

## 注意事項

本回は数学的道具の導入であり、特定のモデルへの適用条件はまだ問わない。
ただし「多様体」「測地線」等の概念を深層学習に適用する際は、各回で改めて「どの条件下で成り立つか」を確認する。

## Arsenal（この回の武器庫）

| 項目 | 内容 |
| --- | --- |
| **Key Concepts** | 多様体、測地線、曲率、リーマン計量、情報幾何学 |
| **Key Metaphor** | 地球儀と世界地図（局所的には平面、大域的には曲がっている） |
| **Key Question** | 「どの空間で考えるか」が、なぜ設計の根幹を決めるのか |
| **Key Limitation** | 幾何学は「形」を測るが、「意味の正しさ」は測れない |

## トピック

必要最小限の数学的道具と講義の見取り図

---

## 導入：なぜ幾何学なのか

深層学習の手法は急速に変化する。2年前の最先端は今日の常識となり、明日には陳腐化するかもしれない。しかし、その背後にある「空間の形」についての問いは、驚くほど普遍的である。

本講義は、個々の手法（How）ではなく、なぜその設計が有効なのか（Why）を問う。その答えを与えるのが **幾何学** という言語である。

幾何学は、空間の「形」を記述する数学だ。平らな紙の上で成り立つ定理が、地球儀の上では崩れる。深層学習の表現空間も同様に、私たちの素朴な直感が通用しない「曲がった空間」であることが多い。この講義では、その曲がり方を理解し、設計に活かすための道具を整える。

---

## 多様体：局所と大域の二重構造

### 多様体とは何か

**多様体（manifold）** とは、局所的にはユークリッド空間に見えるが、大域的には異なる構造を持つ空間のことである。

最も身近な例は地球である。あなたが立っている地面は、目に見える範囲では平らに見える。地図を広げれば、近所の道路は直線的に描かれ、距離も正確に測れる。しかし、地球全体を見れば、それは球面である。平らな地図をいくら貼り合わせても、球面を完全に覆うことはできない。必ずどこかに歪みや切れ目が生じる。

これが多様体の本質だ：

| 視点 | 地球の場合 | 深層学習の表現空間 |
| --- | --- | --- |
| **局所的** | 近所は平らに見える | 小さな摂動は線形近似できる |
| **大域的** | 実際は球面 | 全体の構造は非線形で複雑 |

### なぜ多様体が重要か

深層学習において、データは高次元空間に埋め込まれるが、その「意味のある部分」は多くの場合、より低次元の多様体上に集中している。これを **多様体仮説（manifold hypothesis）** と呼ぶ。

例えば、顔画像のデータセットを考えよう。各画像は数百万画素のベクトルとして表現されるが、「顔らしさ」を決める自由度（表情、向き、照明など）はせいぜい数十から数百程度である。つまり、顔画像は高次元空間の中の低次元多様体上に存在すると考えられる。

この多様体の「形」を理解することが、表現学習の本質である。

> [!NOTE]
> 多様体仮説は多くの実験的証拠に支持されているが、すべてのデータセットで成り立つわけではない。また、「低次元」の程度はデータによって大きく異なる。

---

## 測地線：曲がった空間での「最短」

### 測地線とは何か

平らな空間では、2点間の最短経路は直線である。では、曲がった空間ではどうか。

**測地線（geodesic）** とは、多様体上の2点を結ぶ「局所的に最短」な曲線である。地球上では、これは **大円（great circle）** に対応する。東京からニューヨークへの最短航路が、地図上では奇妙に曲がって見えるのは、球面上の測地線を平面に投影しているからだ。

| 空間 | 測地線の形 | 例 |
| --- | --- | --- |
| ユークリッド空間 | 直線 | 紙の上で定規を引く |
| 球面 | 大円 | 飛行機の国際線ルート |
| 双曲空間 | 双曲線の一部 | 第12回で詳しく扱う |

### 深層学習における測地線

表現空間が球面（単位球面上に正規化されたベクトル）である場合、2つの表現間の「自然な補間」は測地線に沿って行われるべきである。

単純な線形補間（ユークリッド空間での直線）では、中間点が球面から外れてしまう。これに対し、**球面線形補間（Slerp: Spherical Linear Interpolation）** は、球面上の測地線に沿った補間を与える。

```python
import torch

def slerp(v0, v1, t):
    """球面線形補間（Slerp）
    
    Args:
        v0, v1: 単位ベクトル（正規化済み）
        t: 補間パラメータ（0から1）
    
    Returns:
        球面上の測地線に沿った補間点
    """
    # 内積からなす角を計算
    dot = torch.clamp(torch.sum(v0 * v1), -1.0, 1.0)
    theta = torch.acos(dot)
    
    # 特殊ケース：ほぼ同じ方向
    if theta.abs() < 1e-6:
        return v0
    
    # Slerp公式
    sin_theta = torch.sin(theta)
    return (torch.sin((1 - t) * theta) / sin_theta) * v0 + \
           (torch.sin(t * theta) / sin_theta) * v1
```

> [!CAUTION]
> Slerpは $v_0$ と $v_1$ が反対方向（内積≈-1）のとき不安定になる。この場合、経路は一意に定まらない（どの大円でも最短）。実装では例外処理が必要。

---

## 曲率：空間の「曲がり具合」

### 曲率とは何か

**曲率（curvature）** は、空間がどれだけ「平ら」から逸脱しているかを定量化する。直感的には、以下のように区別できる：

| 曲率 | 特徴 | 例 | 深層学習での用途 |
| --- | --- | --- | --- |
| **正（positive）** | 三角形の内角の和が180°より大きい | 球面 | 第3回：球面埋め込み |
| **ゼロ（zero）** | 三角形の内角の和がちょうど180° | 平面 | 古典的なユークリッド空間 |
| **負（negative）** | 三角形の内角の和が180°より小さい | 双曲面（鞍型） | 第12回：階層構造の表現 |

### 曲率の直感的理解

球面上で三角形を描いてみよう。北極から赤道上の2点へ線を引くと、赤道上の辺と合わせて「三角形」ができる。この三角形の内角の和は、180°を超える。極端な例では、北極から赤道まで下り、赤道を90°移動し、北極へ戻ると、各角が90°の三角形ができる。内角の和は270°だ。

これが **正の曲率** の特徴である。空間が「膨らんでいる」ために、三角形が「太る」。

逆に、鞍型の曲面（プリングルスの形）では、三角形は「痩せる」。これが **負の曲率** である。

### 曲率と平行移動

曲がった空間のもう一つの特徴は、ベクトルを「平行に」移動しても、元に戻ったときに向きがズレることだ。

地球上で北極に立ち、南を向いた矢印を持っているとしよう。この矢印を「向きを変えずに」赤道まで運び、赤道に沿って90°移動し、北極へ戻る。すると、矢印は最初の向きから90°ズレている。

この「ズレ」の量が曲率を反映している。深層学習の文脈では、表現空間の曲率が学習ダイナミクスに影響を与える（第4回で詳述）。

---

## リーマン計量：「距離の測り方」を定める

### リーマン計量とは何か

**リーマン計量（Riemannian metric）** は、多様体上で「距離」や「角度」を測るための数学的装置である。より正確には、各点における接空間（その点での「平らな近似」）に内積を定めるものだ。

平らな空間では、距離は単純なユークリッドノルムで測れる：

$$d(\mathbf{x}, \mathbf{y}) = \|\mathbf{x} - \mathbf{y}\| = \sqrt{\sum_i (x_i - y_i)^2}$$

しかし、曲がった空間では「どの方向に進むか」によって距離の測り方が変わりうる。リーマン計量は、この「方向依存の距離感」を定式化する。

### 深層学習での例：フィッシャー情報行列

確率分布の空間を考えよう。パラメータ $\theta$ で決まる分布 $p(x; \theta)$ の「近さ」をどう測るか。

素朴にはパラメータ空間でのユークリッド距離 $\|\theta_1 - \theta_2\|$ を使いたくなる。しかし、これは問題がある。同じパラメータの「幅」でも、分布の変化量は異なるからだ。

例えば、正規分布 $N(\mu, \sigma^2)$ で、$\mu$ を0.1動かすのと、$\sigma$ を0.1動かすのでは、分布の「見た目」の変化量が全く違う。

**フィッシャー情報行列（Fisher Information Matrix）** は、この「分布の変化しやすさ」を反映したリーマン計量を与える：

$$g_{ij}(\theta) = \mathbb{E}_{p(x;\theta)}\left[\frac{\partial \log p(x;\theta)}{\partial \theta_i} \frac{\partial \log p(x;\theta)}{\partial \theta_j}\right]$$

この計量を使えば、パラメータ空間での「1歩」が、分布空間での「同じ大きさの変化」に対応するようになる。

---

## 情報幾何学：確率分布の多様体

### 甘利俊一の情報幾何学

**情報幾何学（Information Geometry）** は、確率分布の空間を多様体として扱う学問である。甘利俊一によって体系化され、統計学・機械学習・物理学に広く応用されている。

その核心的なアイデアは：

> **確率分布の空間にリーマン計量を入れると、統計的推論が幾何学的な言葉で記述できる。**

具体的には：

| 統計的概念 | 幾何学的解釈 |
| --- | --- |
| 最尤推定 | 多様体への射影 |
| KLダイバージェンス | 非対称な「距離」（ブレグマンダイバージェンス） |
| 自然勾配 | リーマン計量に沿った勾配 |
| 十分統計量 | 多様体の座標系の選び方 |

### 自然勾配と学習の効率

通常の勾配降下法は、パラメータ空間でのユークリッド勾配を使う。しかし、パラメータの「1歩」が分布の「どれだけの変化」に対応するかは、現在地によって異なる。

**自然勾配（Natural Gradient）** は、フィッシャー情報行列を使って勾配を補正することで、分布空間での「等しい変化量」を実現する：

$$\tilde{\nabla}_\theta \mathcal{L} = G(\theta)^{-1} \nabla_\theta \mathcal{L}$$

ここで $G(\theta)$ はフィッシャー情報行列。

直感的には、「地形に応じて歩幅を調整する」ようなものだ。急な坂（分布が敏感に変わる方向）では小さく、緩やかな坂（分布があまり変わらない方向）では大きく歩く。

> [!NOTE]
> 自然勾配の計算は、フィッシャー情報行列の逆行列を必要とするため、大規模モデルでは近似が必要。Adam等のオプティマイザは、自然勾配の近似と解釈できる側面がある。

### 本講義の立ち位置

本講義は、情報幾何学の精神を受け継ぎつつ、深層学習の具体的な幾何構造に焦点を当てる。特に：

- **球面**（第3回〜第5回）：正規化された表現空間
- **双曲空間**（第12回）：階層構造の表現
- **位相**（第14回）：形ではなく「穴」を測る

これらは、情報幾何学の一般論というより、実践的な設計指針として扱う。

---

## 本講義の地図：平坦から曲がった世界へ

### 三つの世界

本講義は、表現空間の「形」を軸に、以下の三つの世界を旅する：

```
平坦（古典ML）→ 球面（現代DL）→ その先（双曲・トポロジー）
```

| 世界 | 曲率 | 典型的な手法 | 講義での位置 |
| --- | --- | --- | --- |
| **平坦** | ゼロ | PCA, SVM, Word2Vec（初期） | 第1回〜第2回 |
| **球面** | 正 | nGPT, ArcFace, CLIP | 第3回〜第7回 |
| **双曲** | 負 | Poincaré Embedding, HypLoRA | 第12回 |
| **位相** | - | TDA, Persistent Homology | 第14回 |

### 連続と離散の界面

深層学習には、避けて通れない緊張がある。**連続空間での「旅」が、最終的に離散トークン（または離散的な決定）に「着地」しなければならない** という点だ。

Softmaxは、連続的なロジットを確率分布に変換する。Attentionは、連続的な類似度を重みに変換する。これらの操作は、連続と離散の界面で起きている。

第4回では、この界面をSoftmaxと情報幾何学の観点から再検討する。

### スパース性：高次元空間の「ほとんどが空」

高次元空間には、直感に反する性質がある。第13回で詳しく扱うが、ここで予告しておこう：

> 高次元空間では、ランダムに選んだ2つのベクトルは、ほぼ確実に直交に近い。

これは呪いであると同時に祝福でもある。

- **呪い**：距離の区別がつきにくくなる（kNNの困難）
- **祝福**：互いに干渉しない表現を大量に詰め込める（MoEの設計原理）

Mixture of Experts（MoE）は、この祝福を積極的に活用するアーキテクチャである。各Expertが高次元空間の異なる部分空間を担当し、入力に応じて適切なExpertだけを活性化する。

---

## 幾何学の限界：形は測れても、正しさは測れない

### 本講義が扱わないこと

ここで、重要な限界を明示しておく。

幾何学は「空間の形」を測る道具だが、**「その空間に注ぎ込まれたデータの真偽」** については沈黙している。

例えば、「1+1=2」と「1+1=3」の埋め込みベクトルは、どちらも正規化すれば単位球面上の点になる。幾何学的には、両者は単に「異なる方向を向いた単位ベクトル」に過ぎない。どちらが「正しい」かは、幾何学の範疇外である。

| 幾何学が見るもの | 真偽判定に必要なもの |
| --- | --- |
| 距離・曲率・位相（内在的性質） | 世界知識・論理規則・観測事実（外部参照） |

この限界については、[Appendix 2「多様体の純度問題」](appendix.2.md) で詳しく議論する。

> [!IMPORTANT]
> 正規化は「形式の安定」を与えるが、「内容の真偽」には中立である。幾何学に真偽判定を期待するのは、定規に善悪の判断を期待するようなものだ。

### 外部参照の必要性

現時点で、データの真偽を判定するには、幾何学の外にある道具が必要である：

- **Tool Use**：計算エンジンや検索エンジンで事実を検証
- **合成データ**：論理エンジンで「正しい」データを生成
- **人手ラベリング**：人間が「良い/悪い」を判定

これらはいずれも「幾何学的手法」ではない。本講義は、幾何学が解決できる問題と、解決できない問題を区別することを重視する。

---

## 続編への接続：静的から動的へ

本講義「統一視点」は、**空間の形**（どこに配置されているか）を扱う。しかし、深層学習には「形」だけでは捉えられない側面がある。**情報がどう流れるか**（どう移動するか）という動的な視点だ。

続編「動態論」では、この動的な視点を導入する：

| 講義 | 問い | 主語 | 典型的な対象 |
| --- | --- | --- | --- |
| 統一視点（本講義） | 空間の形は何か | 表現（embedding） | 球面、双曲空間、情報幾何 |
| 動態論（続編） | 情報はどう流れるか | 軌道（trajectory） | 拡散、フロー、推論過程 |

例えば、拡散モデルは「ノイズからデータへの旅」を記述する。この旅を理解するには、**連続の式**（質量保存）、**速度場**（どの方向に流れるか）、**作用**（どの経路が最適か）といった、流体力学の言葉が必要になる。

動態論の第0回では、これらの道具を導入する。本講義で「空間の形」を理解した後、続編で「情報の流れ」を学ぶことで、深層学習の幾何学的な全体像が見えてくる。

---

## 実装ノート：幾何学的直感を検証する

### 高次元での直交性の確認

高次元空間でランダムベクトルがほぼ直交することを、実験で確認しよう：

```python
import torch
import matplotlib.pyplot as plt

def check_random_orthogonality(dims, n_pairs=1000):
    """高次元でランダムベクトルの内積分布を調べる"""
    results = []
    
    for d in dims:
        # 単位球面上の一様ランダムベクトル
        v1 = torch.randn(n_pairs, d)
        v1 = v1 / v1.norm(dim=1, keepdim=True)
        v2 = torch.randn(n_pairs, d)
        v2 = v2 / v2.norm(dim=1, keepdim=True)
        
        # 内積の計算
        dots = (v1 * v2).sum(dim=1)
        results.append({
            'dim': d,
            'mean': dots.mean().item(),
            'std': dots.std().item()
        })
    
    return results

# 実験
dims = [10, 100, 1000, 10000]
results = check_random_orthogonality(dims)

for r in results:
    print(f"d={r['dim']:5d}: mean={r['mean']:+.4f}, std={r['std']:.4f}")

# 期待される結果：
# d=   10: mean≈0, std≈0.32
# d=  100: mean≈0, std≈0.10
# d= 1000: mean≈0, std≈0.032
# d=10000: mean≈0, std≈0.010
# → 次元が上がると標準偏差が 1/√d で減少
```

### 測地線（Slerp）と線形補間の違い

```python
import torch
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def slerp(v0, v1, t):
    """球面線形補間"""
    dot = torch.clamp(torch.sum(v0 * v1), -1.0, 1.0)
    theta = torch.acos(dot)
    if theta.abs() < 1e-6:
        return v0
    sin_theta = torch.sin(theta)
    return (torch.sin((1 - t) * theta) / sin_theta) * v0 + \
           (torch.sin(t * theta) / sin_theta) * v1

def lerp(v0, v1, t):
    """線形補間（正規化なし）"""
    return (1 - t) * v0 + t * v1

def lerp_normalized(v0, v1, t):
    """線形補間して正規化"""
    v = (1 - t) * v0 + t * v1
    return v / v.norm()

# 3次元で可視化
v0 = torch.tensor([1.0, 0.0, 0.0])
v1 = torch.tensor([0.0, 1.0, 0.0])
ts = torch.linspace(0, 1, 20)

slerp_points = torch.stack([slerp(v0, v1, t) for t in ts])
lerp_points = torch.stack([lerp(v0, v1, t) for t in ts])
lerp_norm_points = torch.stack([lerp_normalized(v0, v1, t) for t in ts])

# 結果：
# - slerp_points: 球面上の大円に沿った点列
# - lerp_points: 球面を突き抜ける直線（中点でノルム≈0.71）
# - lerp_norm_points: slerpとほぼ同じ（この場合）
```

> [!NOTE]
> 2点間の角度が小さい場合、Slerpと正規化付き線形補間の差は小さい。角度が大きくなると差が顕著になる。

---

## まとめ

| 概念 | 定義 | 深層学習での役割 |
| --- | --- | --- |
| **多様体** | 局所的に平坦、大域的に曲がった空間 | 表現空間の構造を記述 |
| **測地線** | 曲がった空間での「最短経路」 | 表現間の自然な補間（Slerp等） |
| **曲率** | 空間の曲がり具合の定量化 | 正規化・埋め込み設計の指針 |
| **リーマン計量** | 距離・角度の測り方 | 自然勾配、学習ダイナミクスの理解 |
| **情報幾何学** | 確率分布を多様体として扱う | Softmax、KL、温度の幾何学的解釈 |

### ゴール

**数式は後から付いてくる。まずは「幾何学的直感」を獲得する。**

この講義を通じて獲得してほしいのは、以下のような問いを自然に発することができる直感である：

- 「この表現空間は、どんな形をしているのか」
- 「この正規化は、空間をどう変形しているのか」
- 「この距離関数は、何を『近い』と定義しているのか」
- 「この手法の本質は、幾何学的に何をしているのか」

### 次回予告

第1回「かつての地図」では、古典的な機械学習が前提としていたユークリッド空間観を振り返る。PCA、SVM、LDAといった手法が、いかに「平らな世界」を暗黙の前提としていたかを確認し、見落とされていた構造の兆しを掘り起こす。

---

## 参考文献

- Amari, S. (2016). *Information Geometry and Its Applications*. Springer.
- Lee, J. M. (2018). *Introduction to Riemannian Manifolds* (2nd ed.). Springer.
- Fefferman, C., Mitter, S., & Narayanan, H. (2016). Testing the Manifold Hypothesis. *Journal of the AMS*.
- Bronstein, M. M. et al. (2021). Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. *arXiv:2104.13478*.
