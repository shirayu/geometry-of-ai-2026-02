# 第6回：Transformerという測量士 ～動的な接続～

## 注意事項【重要】

- 標準Transformerの $Q,K$ は単位ノルム化されていない。LayerNorm/RMSNormは $L_2$ 正規化とは別物。
- したがって、標準Transformerで「 $Q^T K = \cos\theta$ 」は成り立たない（ノルムの影響が混在）。
- 「球面上のAttention」解釈が厳密に成り立つのは、Cosine AttentionやnGPTなど明示的に $L_2$ 正規化する設計の場合。
- それでも「角度的な見方」は、正規化設計を理解するための導線として有用。

## トピック

Attention機構の幾何学的解釈

## 内容

- **カーネル法の限界を振り返る:**
    - カーネル法（RBF, Polynomial）：固定された特徴空間への写像
    - 「地図」は入力に関わらず静的
    - 柔軟性に欠ける
- **Attentionの革新:**
    - 入力に応じて**空間自体が歪む**（ように振る舞う）
    - Query-Key-Value機構 = 動的な測量システム
- **標準Transformerの計算ステップ（厳密寄り）:**

    1. **Query ( $\mathbf{Q}$ ) と Key ( $\mathbf{K}$ ) の内積:**
        - Scaled Dot-Product: $\frac{\mathbf{Q}^T \mathbf{K}}{\sqrt{d}}$
        - **重要:** 標準Transformerでは **スケーリング**が入り、かつ **Q,Kは単位正規化が前提ではない**
        - LayerNorm/RMSNormは入っているが、これは **平均・分散の正規化**であり **単位ノルム化（L2正規化）とは別物**
        - したがって、 $\mathbf{Q}^T \mathbf{K}$ は **ノルムの影響も受ける**
    2. **球面解釈が成り立つ条件（条件付き主張）:**
        - **条件A:** $\mathbf{Q}$ と $\mathbf{K}$ が単位ノルムに正規化されている（ $|\mathbf{Q}| = |\mathbf{K}| = 1$ ）
        - **条件B:** この場合に限り $\mathbf{Q}^T \mathbf{K} = \cos\theta$
        - **nGPTや一部のモデル**（Cosine Attention等）がこれを満たしうる
        - **標準Transformer（一般的なBERT/GPT等）はこの条件を常には満たさない**
    3. **それでも幾何学的解釈は有用:**
        - たとえ厳密に $cos\theta$ でなくても、内積は「方向の類似性」の指標として有用
        - 正規化しない場合は「方向 + 大きさ」の混合情報
        - それでも「角度中心の見方」は、**cosine attention / 正規化設計**を理解する導線として有用
- **Softmax による確率的重み付け:**
    - 内積（互換度）を確率に変換
    - 「近い（互換度が高い）ほど強く照らす」
- **Value ( $\mathbf{V}$ ) の加重和:**
    - 重み付き平均として情報を統合
    - 「星から星へ、光のラインで情報を運ぶ」
- **Multi-head Attention の意味:**
    - プラネタリウムに**複数の星座レイヤー**を重ね合わせる
    - 各ヘッド = 異なる「見方」（異なる部分空間への射影）
    - LDAの動的・連続版としての再解釈（比喩）
        - LDA：固定されたトピック空間
        - Multi-head：入力ごとに変化する"見方"の集合
- **RoPE（Rotary Positional Embeddings）の幾何学:**
    - 天球の回転: 絶対的な位置ではなく、星と星の「角距離（角度の差）」で関係を測る。地球の自転（位置の変化）にかかわらず、北斗七星の形が変わらないように、文の中の相対的な位置関係を「回転」として保存する仕組み。
    - 現代LLM（Llama, Qwen, Mistral等）の標準的な位置埋め込み手法
    - **核心:** 絶対位置ではなく、**回転**によって相対位置を表現
    - Query/Keyベクトルに位置依存の回転行列を適用:
        - $\mathbf{q}'_m = R_m \mathbf{q}$ , $\mathbf{k}'_n = R_n \mathbf{k}$
        - 内積 $\mathbf{q}'^T_m \mathbf{k}'_n$ は $(m - n)$ のみに依存 → **相対位置**
    - **幾何学的解釈:**
        - 位置 = 球面上での回転角
        - 相対位置 = 回転角の差
        - 講義テーマ（角度・回転）と完璧に整合する設計
    - ※参考: Su et al., "RoFormer: Enhanced Transformer with Rotary Position Embedding" (2024)
- **Self-Attention = 多様体の自己測量（比喩として）:**
    - 入力系列内の各点が、他のすべての点との関係を測る
    - 「どの星とどの星を結ぶべきか」を自動発見

## 一般的な説明との違い

- 一般的な説明: 「Queryは検索クエリ、Keyはインデックス、Valueは内容である（辞書形式のメタファー）」
- 「答えが元からそこにあるか」か、それとも「観測によって答えが生成されるか」の違い

### 1. 「完全一致」ではなく「空間の重なり」

辞書の検索は、基本的には「AならB」という1対1の対応。
しかし、TransformerのQとKは、**「どれだけ同じ方向を向いているか（余弦）」**を計算。

- **辞書的:** 「リンゴ」を探して、あればその情報を出す。
- **測量的:** Qという観測者が「赤くて丸いもの」という方向を向いたとき、空間内のK（トークン）たちがどれだけその視線上に「重なって見えるか」を計算する。

### 2. 辞書にはない「歪み」の許容

辞書は「不正確な検索」に弱いが、測量（Attention）は**「空間がどう歪んでいるか」**を計算に含める。
たとえば、「彼はペンを持った」と「彼は自信を持った」では、「持った」の意味が違う。

- **辞書的:** 「持った」という項を引くだけ。
- **測量的:** 周囲のトークンの影響で空間が歪み、QとKの測量結果（内積）が動的に変化する。結果として、同じ単語でも**「観測される座標」が変わる**。

### 3. 「情報の統合」は平均化（加重和）である

辞書は「一つの正解」を取り出すが、Transformerは測量したすべてのKeyの重みに基づいて、Valueを**「混ぜ合わせ（加重和）」**。

測量士の言葉で言えば、**「周囲のあらゆる基準点（トークン）から、自分の今の立ち位置（文脈）を逆算して合成する」**というプロセス。
辞書のように「これ！」と決めるのではなく、周囲との関係性の総和として自分の意味を定義。

### なぜ「辞書」という説明が広まったのか？

初期のメモリネットワークや、エンジニアにとって馴染み深い「Key-Valueストア」という概念に名前を借りたから。
しかし、これは理解を助けるための単純化。
計算実態は $\mathbf{Q}^T \mathbf{K}$ という純粋な線形代数（幾何学）であり、
これは検索エンジンよりも**「干渉縞の観測」や「測量」に近い振る舞い**。

## 実装ノート

- 標準Attention: `torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)`
- Cosine Attention: `torch.matmul(F.normalize(Q, dim=-1), F.normalize(K, dim=-1).transpose(-2, -1))`
    - `dim=-1`の明示が重要（最後の次元で正規化）

- nGPT: すべてのベクトルが常にノルム1（設計として）

## 結論

Transformerを幾何学的に見る視点は有用だが、「どの幾何学か」は設計次第
