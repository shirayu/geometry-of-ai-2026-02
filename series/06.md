# 第6回：Transformerという測量士 ～動的な接続～

## 注意事項

- 標準Transformerの $Q, K$ は単位ノルム化されていない。LayerNorm/RMSNormは平均・分散の正規化であり、 $L_2$ 正規化（単位ノルム化）とは別物である。
- したがって、標準Transformerで「 $Q^\top K = \cos\theta$ 」は成り立たない（ノルムの影響が混在する）。
- 「球面上のAttention」解釈が厳密に成り立つのは、Cosine AttentionやnGPTなど明示的に $L_2$ 正規化する設計の場合に限られる。
- それでも「角度的な見方」は、正規化設計を理解するための導線として有用である。

## Transformerの全体像

### なぜここで全体像を示すか

本講義はTransformerの解説書ではない。しかし、本回以降の議論——Attention、正規化、位置埋め込み、FFN——はすべてTransformerの構成要素に対する幾何学的再解釈である。全体像を持たずに個別の要素を語ると、地図なしで建物の内装を見学することになる。

本セクションでは、Transformerの構成要素を最小限の粒度で整理し、各要素が本講義のどこで幾何学的に再解釈されるかを示す。詳細な計算は各回に譲る。

### 構成要素

Transformerは、以下の要素が積み重なった構造である。1ブロック（層）の処理を簡略化すると、次のようになる：

```
入力トークン列
  ↓
[トークン埋め込み + 位置埋め込み]  ... 離散記号を連続ベクトルに変換
  ↓
┌─────────── Transformer Block（×N層）───────────┐
│                                                 │
│  [Multi-Head Attention]  ... 「どこを見るか」     │
│       ↓                                         │
│  [残差接続 + 正規化]                              │
│       ↓                                         │
│  [FFN]  ... 「何を変換するか」                     │
│       ↓                                         │
│  [残差接続 + 正規化]                              │
│                                                 │
└─────────────────────────────────────────────────┘
  ↓
[出力層（線形 + Softmax等）]
```

各要素の役割を一文で述べる：

| 要素 | 役割 | 一言で言えば |
| --- | --- | --- |
| **トークン埋め込み** | 離散トークンを $d$ 次元ベクトルに変換 | 「記号を空間に配置する」 |
| **位置埋め込み** | トークンの順序情報を付与 | 「星に座標を与える」 |
| **Multi-Head Attention** | 入力に応じてトークン間の関係を動的に計算 | 「望遠鏡で星空を測量する」 |
| **FFN（Feed-Forward Network）** | 各トークンの表現を非線形に変換 | 「観測データを処理する」 |
| **残差接続** | 入力をショートカットで加算し、勾配の流れを安定させる | 「元の情報を保持しながら更新する」 |
| **正規化（LayerNorm等）** | 表現のスケールを安定させる | 「計器のキャリブレーション」 |
| **出力層** | タスクに応じた最終変換（分類ならSoftmax等） | 「測量結果を報告する」 |

> [!IMPORTANT]
> **AttentionとFFNの分業：** Attentionは「トークン間の情報交換（どの情報を集めるか）」を担い、FFNは「各トークンの表現変換（集めた情報をどう加工するか）」を担う。この分業構造がTransformerの基本設計である。第3回で見たnGPTは、この両方の出力に球面制約を課す設計だった。また、近年のMoE（Mixture of Experts）ではFFN部分を動的に切り替える設計も登場している（第13回で触れる）。

### 本講義との対応マップ

各構成要素が本講義のどの回で幾何学的に再解釈されるかを示す。この表は「予告」であり、各回を読む際の見通しとして使ってほしい。

| 構成要素 | 幾何学的な問い | 対応する回 |
| --- | --- | --- |
| トークン埋め込み | ベクトルはどの空間に住むべきか？ | 第2回（ノルム）、第3回（球面） |
| 位置埋め込み | 順序をどう幾何学に埋め込むか？ | 第6回（RoPE：本回） |
| Attention（QKV） | 内積は何を測っているか？ | 第6回（本回） |
| Softmax | 確率分布空間でどう位置づけるか？ | 第4回（情報幾何学） |
| FFN | 表現の非線形変換とは何か？ | 第8回（ベクトル場） |
| 残差接続 | なぜ「足す」のか？ | 第8回（Neural ODE） |
| 正規化 | LayerNormと球面制約はどう違うか？ | 第3回（nGPT） |
| 出力層（分類ヘッド） | マージンの幾何学 | 第5回（ArcFace） |

> [!NOTE]
> **Encoder-DecoderとDecoder-only：** Transformerには大きく分けてEncoder-Decoder型（T5など）とDecoder-only型（GPTなど）がある。上の構成要素は両者に共通する。本講義では特定のアーキテクチャに限定せず、共通する幾何学的構造に注目する。Decoder-onlyモデルでは、Attentionに因果マスク（未来のトークンを見ない制約）が加わるが、幾何学的な本質は変わらない。

## 導入：静的な地図から動的な測量へ

第1回で見たように、古典的な機械学習手法は「固定された特徴空間」を前提としていた。PCAは固定された主成分軸を、SVMは固定されたカーネル関数を使う。入力が何であれ、空間の構造は変わらない。

Transformerは、この前提を根本から覆した。

Attention機構は、入力に応じて「どこを見るか」を動的に決定する。これは、固定された地図を読む代わりに、その場で測量を行うようなものだ。入力が変われば、測量結果も変わり、空間の見え方自体が変化する。

本回では、このAttention機構を幾何学的に再解釈する。特に、正規化の有無が解釈にどう影響するかを明確にしながら、Query-Key-Value機構の本質に迫る。

## カーネル法の限界：固定された特徴空間

### カーネル法とは何だったか

第1回で触れたカーネル法を、もう少し詳しく振り返ろう。

カーネル法は、データを高次元の特徴空間に写像し、その空間で線形な操作を行う。カーネル関数 $k(\mathbf{x}, \mathbf{y})$ は、この高次元空間での内積を直接計算する：

$$k(\mathbf{x}, \mathbf{y}) = \langle \phi(\mathbf{x}), \phi(\mathbf{y}) \rangle$$

RBFカーネル $k(\mathbf{x}, \mathbf{y}) = \exp(-\gamma \|\mathbf{x} - \mathbf{y}\|^2)$ は「近いものは類似度が高い」という直感を反映し、多項式カーネルは特徴の交互作用を捉える。

### 固定性という制約

カーネル法の根本的な制約は、**特徴空間の構造が入力に依存しない**ことである。

RBFカーネルは常に「ユークリッド距離に基づく類似度」を計算する。入力がテキストであろうと画像であろうと、同じ関数形を使う。これは、すべての入力に対して同じ「地図」を適用しているようなものだ。

| カーネル法 | Attention |
| --- | --- |
| 固定された特徴空間 | 入力依存の動的な空間 |
| 事前に設計されたカーネル関数 | 学習されたQuery-Key-Value |
| 入力に関わらず同じ構造 | 入力ごとに異なる重み付け |

## Attention機構：動的な測量システム

### Query-Key-Valueの直感

Attention機構を理解するために、**測量**のメタファーを導入しよう。

測量士は、未知の地形を測定するために、複数の基準点との距離や角度を測る。その測定結果を統合して、自分の位置や地形の構造を推定する。

Attention機構も同様のことを行う：

- **Query（問い）**：「私は今、何を知りたいのか」という測量士の意図
- **Key（鍵）**：各基準点が「自分はここにいる」と示す標識
- **Value（値）**：各基準点が持つ情報の内容

測量士（Query）は、各標識（Key）との整合度を計算し、整合度が高い基準点（Value）からより多くの情報を取り入れる。

### 標準Transformerの計算ステップ

標準的なTransformerのSelf-Attentionは、以下のステップで計算される。

**Step 1: 線形射影**

入力 $\mathbf{X} \in \mathbb{R}^{n \times d}$ を、学習された重み行列で射影する：

$$\mathbf{Q} = \mathbf{X} W_Q, \quad \mathbf{K} = \mathbf{X} W_K, \quad \mathbf{V} = \mathbf{X} W_V$$

**Step 2: Scaled Dot-Product Attention**

QueryとKeyの内積を計算し、スケーリングする：

$$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_k}}\right) \mathbf{V}$$

ここで $\sqrt{d_k}$ によるスケーリングは、内積の分散を安定させるためのものである。

> [!IMPORTANT]
> **標準TransformerとL2正規化の違い：** 標準Transformerでは、LayerNormやRMSNormが使われるが、これらは **平均・分散の正規化** であり、ベクトルを単位ノルムにする **L2正規化とは異なる**。したがって、 $\mathbf{Q}^\top \mathbf{K}$ は純粋な角度情報（ $\cos\theta$ ）ではなく、**方向と大きさの両方の影響を受ける**。

> [!NOTE]
> **高次元でのLayerNormの挙動：** ただし、高次元空間においてLayerNormを適用されたベクトルは、確率的に「ある程度の範囲のノルムに集中する」傾向がある（高次元の集中現象の一種）。そのため、標準Attentionでも「実質的には角度に近いものを見ている」という近似的解釈は、現場の直感としては完全に間違いではない。本資料では厳密な区別を維持しつつ、角度的な見方を正規化設計への導線として位置づけている。

### 球面解釈が成り立つ条件

「Attentionは角度を計算している」という解釈は、特定の条件下でのみ厳密に成り立つ。

**条件：** $\mathbf{Q}$ と $\mathbf{K}$ の各行ベクトルが単位ノルムに正規化されている場合、

$$\mathbf{q}_i ^\top \mathbf{k}_j = \|\mathbf{q}_i\| \|\mathbf{k}_j\| \cos\theta _{ij} = \cos\theta _{ij}$$

となり、内積は純粋に角度情報を反映する。

| 設計 | L2正規化 | 内積の意味 |
| --- | --- | --- |
| 標準Transformer | なし | 方向 + 大きさの混合 |
| Cosine Attention | Q, Kを正規化 | 純粋な角度（ $\cos\theta$ ） |
| nGPT | すべて正規化 | 純粋な角度（ $\cos\theta$ ） |

> [!NOTE]
> **それでも幾何学的解釈は有用：** たとえ厳密に $\cos\theta$ でなくても、内積は「方向の類似性」の指標として機能する。正規化設計（Cosine Attention, nGPT）を理解するための導線として、角度的な見方は有用である。

## 天体観測メタファー：QKVの再解釈

### なぜ「辞書」ではなく「天体観測」か

Attention機構の説明として、「Queryは検索クエリ、Keyはインデックス、Valueは内容」という**辞書メタファー**がよく使われる。これは初期のメモリネットワークや、エンジニアに馴染み深いKey-Valueストアからの類推である。

しかし、この比喩には限界がある。辞書は「AならB」という1対1の対応を前提とするが、Attentionは**連続的な類似度**に基づいて**すべてのValueを混合**する。

より適切なメタファーとして、**天体観測**を提案する。

### 星空としてのトークン列

トークン列を、宇宙に浮かぶ星々として捉えよう。

**Key（星の輝き・色）**：各トークンが「自分は何者か」を周囲に示す特性。星が特定の波長の光を放つように、各トークンは自身の意味的特性をKeyとして発信する。

**Value（星の質量・組成）**：各トークンが持つ情報のエネルギー。Attentionによって選ばれたとき、この情報が伝達される。

**Query（望遠鏡の向きとフィルター）**：観測者が「今、何を探しているか」という意図。特定の波長だけを通すフィルターをセットして、夜空を見渡す行為に相当する。

**Attention Score（光の受信）**：観測者のフィルター（Query）と星の輝き（Key）が一致したとき、その星は強く輝いて見える。Softmaxによって、特に明るい星々が選び出され、意味のある「星座」として認識される。

### 辞書メタファーとの本質的な違い

天体観測メタファーは、辞書メタファーでは説明しにくい現象を自然に捉える。

**1. 完全一致ではなく空間の重なり**

辞書検索は「リンゴ」を探して、あればその情報を返す。Attentionは、Queryという「方向」に対して、各Keyがどれだけ「重なって見えるか」を連続的に計算する。

**2. 文脈による「歪み」の許容**

「彼はペンを持った」と「彼は自信を持った」では、「持った」の意味が異なる。辞書は「持った」という項を引くだけだが、Attentionでは周囲のトークンの影響で空間が「歪み」、同じ単語でも異なる座標が観測される。

**3. 情報の統合は加重和**

辞書は「一つの正解」を取り出すが、Attentionは測量したすべてのKeyの重みに基づいて、Valueを混ぜ合わせる。周囲との関係性の総和として意味が定義される。

> [!NOTE]
> **ハルシネーションの幾何学的理解：** 辞書観では「検索ミス」としか理解できないハルシネーションも、天体観測観では「観測データの密度が低い領域で、Attentionが変な方向に歪み、存在しない星（意味）を観測してしまった」と理解できる。これは「空間の歪みをどう補正するか」という本質的な議論への入口を開く。

## Multi-head Attention：複数の視点

### なぜ複数のヘッドが必要か

Single-head Attentionは、一つの「見方」しか持たない。しかし、言語には複数の側面がある。文法的な関係、意味的な関係、照応関係など、異なる種類の関係を同時に捉える必要がある。

**Multi-head Attention**は、この問題に対処するために、複数の独立したAttentionを並列に実行する：

$$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W_O$$

$$\text{head}_i = \text{Attention}(\mathbf{Q} W_Q^i, \mathbf{K} W_K^i, \mathbf{V} W_V^i)$$

### 幾何学的解釈：部分空間への射影

各ヘッドは、異なる射影行列 $W_Q^i, W_K^i, W_V^i$ を持つ。これは、高次元空間を異なる部分空間に射影し、それぞれの部分空間でAttentionを計算することに相当する。

天体観測メタファーで言えば、**複数のフィルターを通して同じ星空を観測**するようなものだ。可視光、赤外線、X線など、異なる波長で観測すると、異なる構造が見える。Multi-head Attentionは、意味空間を複数の「波長」で同時に観測している。

| 観点 | Single-head | Multi-head |
| --- | --- | --- |
| 視点の数 | 1つ | $h$ 個 |
| 捉えられる関係 | 1種類 | 複数種類 |
| 部分空間 | 全空間 | $h$ 個の部分空間 |
| 表現力 | 限定的 | 豊か |

> [!NOTE]
> **LDAとの比較（比喩として）：** 第1回で触れたLDA（Linear Discriminant Analysis）は、固定されたトピック空間への射影を行う。Multi-head Attentionは、入力ごとに変化する「見方」の集合を学習するという意味で、LDAの動的・連続版と見なせる。ただし、これは直感的な比喩であり、最適化問題として同一ではない。

## T5：空間の大統一理論

### タスク固有の幾何構造という制約

Transformerが「動的な測量システム」として登場したとき、**事前学習＋下流タスク適応**のパラダイムでは、タスクごとに異なる「出力層」が設計されることが一般的だった。

- **分類タスク**：最終層にSoftmax層を接続し、カテゴリ確率を出力
- **系列ラベリング**：各トークンに対してラベル分布を出力
- **生成タスク**：デコーダーを追加し、自己回帰的にテキストを生成

これらは、同じTransformer Encoderを共有しながらも、タスクごとに異なる「出力形式」を持っていた。この状況は、カーネル法が固定された特徴空間を前提とすることと、完全に同型ではないが、「表現形式を事前に決める必要がある」という点で共通の制約を抱えていた。

| 時代 | 設計思想 | 特徴 |
| --- | --- | --- |
| カーネル法 | 固定されたカーネル関数 | データに関わらず同じ幾何構造 |
| 初期の事前学習モデル | タスク固有の出力層 | タスクごとに異なる出力形式 |
| T5以降 | Text-to-Text | すべてを同一の出力形式に統一 |

### T5：共通インタフェースとしてのテキスト

**T5（Text-to-Text Transfer Transformer）**（Raffel et al., 2020）は、この制約を根本から再考した。

T5の核心的アイデアは、**すべてのタスクをテキスト生成として定式化する**ことである。翻訳、要約、質問応答、感情分析など、あらゆるタスクが「入力テキスト → 出力テキスト」という単一のフレームワークに統一される。

例えば：

- **翻訳**：`translate English to German: The house is wonderful.` → `Das Haus ist wunderbar.`
- **要約**：`summarize: [長文]` → `[要約文]`
- **質問応答**：`question: What is the capital of France? context: [文書]` → `Paris`
- **感情分析**：`sentiment: This movie is great!` → `positive`

数学的には、これは各タスク $\tau$ について、**入出力を文字列空間へ符号化（encoding）する**写像

$$e _\tau ^{\text{in}}: \mathcal{X} _\tau \to \Sigma ^*, \quad e _\tau ^{\text{out}}: \mathcal{Y} _\tau \to \Sigma ^{\star}$$

を用意し、学習は $\Sigma^{\star} \to \Sigma^{\star}$ という単一の写像で行うことと見なせる。ここで $\Sigma^{\star}$ は文字列（トークン列）の空間である。評価や利用時には、必要に応じて復号写像 $d_\tau^{\text{out}}: \Sigma^{\star} \to \mathcal{Y}_\tau$ により出力をタスクの形式へ解釈する。

重要なのは、これが「すべてのタスクを完全に統一した」というより、**共通の入出力インタフェース（テキスト形式）を提供した**ことである。タスク間の性能差や相性は依然として存在するが、少なくとも「同じ言語で表現できる」形に揃えることで、転移学習の土台を作りやすくなった。

### 幾何学的解釈：統一された入出力インタフェース

T5の設計思想を、天体観測メタファーで理解しよう。

**T5以前：異なる銀河ごとに望遠鏡を付け替える**

事前学習モデルの初期では、タスクごとに「異なる銀河（問題領域）」を観測していた。翻訳という銀河を見るには翻訳用の望遠鏡を、分類という銀河を見るには分類用の望遠鏡を付け替える必要があった。

観測者は複数の望遠鏡を携え、タスクが変わるたびに機材を交換しなければならない。

**T5以降：同じ全天を一つの望遠鏡で観測**

T5は、すべての銀河を**同じ観測形式（テキスト）で記録できる**ようにした。翻訳も要約も質問応答も、すべて「テキストからテキストへの写像」という同じ言語で表現される。

観測者は一つの万能望遠鏡を持ち、フィルター（タスクプレフィックス）を変えるだけで、あらゆる現象を観測できる。ただし、これは「すべてのタスクが完全に同一の構造を持つ」という意味ではなく、**共通の記録形式を持つ**ことで、知識の転移がしやすくなったと理解すべきである。

| 観点 | T5以前 | T5以降 |
| --- | --- | --- |
| 観測対象 | 異なる銀河（タスクごとに別形式） | 同じ形式で記録可能な現象 |
| 観測機器 | タスクごとに異なる望遠鏡 | 一つの万能望遠鏡 |
| フィルター | 望遠鏡自体を交換 | フィルター（プレフィックス）を変更 |
| 出力形式 | タスクごとに異なる | 統一されたテキスト形式 |

### 設計思想の転換：仕組みから哲学へ

T5は、Transformerの「仕組み（Attention機構）」に何も追加していない。QueryもKeyもValueも、Multi-headも、すべて標準的なTransformerと同じである。

T5が変えたのは、**出力空間の設計哲学**である。

- **仕組み**：Transformerは、動的な測量システムとしてAttentionを実行する
- **哲学**：T5は、あらゆる測量結果を「テキスト」という共通形式で報告する

この哲学的転換により、Transformerは **統一された入出力インタフェース（テキスト形式）** を持つようになった。同一形式に揃えることで、タスク間で知識を転移する土台が作られる。翻訳で学んだパターンが要約に活用されたり、質問応答で学んだ推論が感情分析に応用されたりする可能性が開ける。

ただし、これは「タスクの壁が完全に消えた」というより、**転移しやすい環境を整えた**と理解すべきである。タスク間の性能差や相性は依然として存在し、実際の転移効果はデータ量、タスクの近さ、事前学習の設計などに依存する。

> [!NOTE]
> **GPTとの関係：** T5のText-to-Text哲学は、GPTの「言語モデリングとしての統一」とは微妙に異なる。GPTは「次トークン予測」という単一目的ですべてを学習するが、T5は「明示的なタスクプレフィックス」を使って多様なタスクを統一する。ただし、GPT系も実際には**指示文（プロンプト）でタスク条件付け**を行うため、差は「原論文の枠組みとして全タスクを明示的にtext-to-textに整形したこと」程度である。どちらもTransformerの「仕組み」を変えず、「使い方の哲学」を変えた点で共通している。

### メッセージ：仕組みと設計思想の分離

本回は「仕組み（Attention）」が中心だが、T5は「仕組みを変えずに、設計思想だけで到達できる地平」を示している。

Attention機構という「動的な測量システム」は、T5という「設計思想」によって、 **統一された入出力形式（テキストインタフェース）** を実現した。固定された地図（カーネル法）でもなく、タスクごとに異なる地図（初期の事前学習モデル）でもなく、**一つの測量システムで多様な地形を同じ形式で記述できる**世界が開かれた。

これは、幾何学における「座標系の選択」に似ている。直交座標、極座標、球座標など、どの座標系を選んでも同じ幾何学的対象を記述できるが、問題に応じた適切な座標系を選ぶことで、計算が劇的に簡潔になる。T5は、「テキストという座標系」を選ぶことで、あらゆるタスクを統一的に記述しやすくしたのである。

## RoPE：回転による相対位置の表現

### 位置情報の必要性

Self-Attentionは、入力の順序を区別しない。「犬が猫を追いかけた」と「猫が犬を追いかけた」は、単語の集合としては同じだが、意味は異なる。位置情報を何らかの形で導入する必要がある。

初期のTransformer（Vaswani et al., 2017）は、正弦波による **絶対位置埋め込み** を使った。しかし、近年の大規模言語モデル（Llama, Qwen, Mistral等）では、**RoPE（Rotary Position Embedding）** が広く採用されている。

> [!NOTE]
> **位置埋め込みの多様性：** RoPEは近年の多くのLLMで採用されているが、唯一の選択肢ではない。ALiBi（Press et al., 2022）のように、Attention重みに位置依存のバイアスを加える方式の採用例もある。最適な位置埋め込みはタスクやモデル設計に依存する。

### RoPEの核心：回転による相対位置

RoPE（Su et al., 2021）の核心的アイデアは、位置を**回転**として表現することである。

Query と Key のベクトルに、位置依存の回転行列を適用する：

$$\mathbf{q}'_m = R_m \mathbf{q}, \quad \mathbf{k}'_n = R_n \mathbf{k}$$

ここで $R_m$ は位置 $m$ に対応する回転行列である。

この設計の妙は、内積を計算したとき：

$$\mathbf{q}'^{\top}_m \mathbf{k}'_n = \mathbf{q}^\top R _m ^\top R _n \mathbf{k} = \mathbf{q}^\top R _{n-m} \mathbf{k}$$

となり、**相対位置 $(m - n)$ のみに依存する**ことである。

### 幾何学的解釈：天球の回転

RoPEの幾何学的意味を、天体観測メタファーで理解しよう。

宇宙には絶対的な座標系はない。あるのは、星と星の間の **角度（相対位置）** だけである。地球が自転しても、北斗七星の形は変わらない。各星の絶対位置は変化するが、星同士の相対的な配置は保存される。

RoPEはこの原理を言語モデルに適用する：

- **絶対位置**：文中での位置（1番目、2番目、…）
- **回転**：位置に対応する回転操作
- **相対位置**：二つのトークン間の距離（回転角の差）

| 概念 | 天体観測 | RoPE |
| --- | --- | --- |
| 絶対位置 | 天球上の座標 | トークンの位置 $m$ |
| 回転 | 地球の自転 | 回転行列 $R_m$ |
| 相対位置 | 星同士の角距離 | 位置の差 $(m - n)$ |
| 不変量 | 星座の形 | 相対位置に基づくAttention |

> [!NOTE]
> **講義テーマとの整合：** RoPEは「角度」と「回転」を本質的に使う設計であり、本講義のテーマ（球面、角度、回転）と完璧に整合する。これは偶然ではなく、高次元空間での効率的な位置表現として、回転が自然な選択であることを示している。

### 2次元での具体例

RoPEの仕組みを、2次元の場合で具体的に見てみよう。

位置 $m$ での回転行列は：

$$R_m = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix}$$

ここで $\theta$ は基本周波数である。

Query $\mathbf{q} = (q_1, q_2)$ に回転を適用すると：

$$\mathbf{q}'_m = \begin{pmatrix} q_1 \cos(m\theta) - q_2 \sin(m\theta) \\ q_1 \sin(m\theta) + q_2 \cos(m\theta) \end{pmatrix}$$

高次元では、ベクトルを2次元ずつのペアに分割し、各ペアに異なる周波数の回転を適用する。

## Softmaxの役割：連続から離散への橋渡し

### Attentionにおけるスケーリング

第4回で見たように、Softmaxには温度パラメータがある。Attentionの文脈では、スケーリング係数 $\sqrt{d_k}$ が分布の鋭さに影響する。

$$\text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_k}}\right)$$

$\sqrt{d_k}$ によるスケーリングは、**主に内積の分散を安定化するため**に導入された（Vaswani et al., 2017）。 $d_k$ が大きいと、内積の値も大きくなりやすく、Softmaxが極端に鋭くなって勾配が消失する問題がある。 $\sqrt{d_k}$ で割ることで、次元に関わらず内積のスケールを揃える。

結果として、このスケーリングはSoftmaxの鋭さ（実効的な温度）にも影響する。スケーリングがなければ、 $d_k$ が大きいほど内積が大きくなりやすく分布が過度に鋭くなるが、 $\sqrt{d_k}$ で割ることでその効果を相殺し、鋭さを次元に対して概ね一定に保つ。

天体観測メタファーで言えば、**望遠鏡の倍率を調整**するようなものだ。高倍率（低温）では、最も明るい星だけが見える。低倍率（高温）では、微かな光も拾い上げられる。

### 情報の加重和としての出力

Attention重みが計算されると、Valueの加重和として出力が生成される：

$$\text{output}_i = \sum_j \alpha _{ij} \mathbf{v} _j$$

ここで $\alpha_{ij}$ はSoftmaxで計算されたAttention重み。

これは、辞書から「一つの正解」を引くのではなく、**すべての情報源からの寄与を重み付けて統合**する操作である。測量士が複数の基準点からの測定結果を統合して位置を推定するように、Attentionは複数のトークンからの情報を統合して出力を生成する。

## まとめ

| 概念 | 定義 | 本回での役割 |
| --- | --- | --- |
| **Scaled Dot-Product** | $\frac{QK^\top}{\sqrt{d_k}}$ | Attentionの基本計算 |
| **Cosine Attention** | 正規化された $Q, K$ の内積 | 純粋な角度ベースのAttention |
| **Multi-head Attention** | 複数の部分空間でのAttention | 異なる関係の同時捕捉 |
| **T5** | Text-to-Text統一フレームワーク | タスク出力形式の統一 |
| **RoPE** | 回転による相対位置表現 | 位置情報の幾何学的導入 |
| **天体観測メタファー** | Query=望遠鏡、Key=星の輝き | Attentionの直感的理解 |

### 本回のポイント

Transformerを幾何学的に見る視点は有用だが、**「どの幾何学か」は設計次第**である。

標準TransformerのAttentionは、厳密には「角度を計算している」わけではない。LayerNormやRMSNormは平均・分散の正規化であり、L2正規化とは異なる。したがって、内積には方向と大きさの両方が影響する。

しかし、Cosine AttentionやnGPTのように明示的にL2正規化する設計では、「 $Q^\top K = \cos\theta$ 」が厳密に成り立ち、球面上のAttentionとして解釈できる。

**T5は、Transformerの「仕組み」を変えずに「設計思想」だけで到達できる地平を示した。** すべてのタスクを「テキスト生成」という統一された入出力形式に揃えることで、タスク間転移の土台を作りやすくなった。これは、適切な座標系（表現形式）の選択が問題を劇的に簡潔にする例である。

RoPEは、位置情報を「回転」として表現する設計であり、本講義のテーマ（角度、回転、球面）と深く整合する。絶対位置ではなく相対位置に基づくAttentionを可能にし、近年の多くのLLMで広く採用されている。

## 次回予告

第7回「不確実性の復権」では、点表現から分布表現への移行を議論する。

これまで、埋め込みは「点」として扱ってきた。しかし、点は不確実性を表現できない。「この単語の意味は確信度が高い」「この単語は曖昧だ」といった情報を、表現自体に組み込むことはできるだろうか。分布表現がこの問いに答える。

## 実装ノート

> [!NOTE]
> 以下のコードは PyTorch >= 1.9 を前提とする。

### 標準的なScaled Dot-Product Attention

<details>
<summary>コード例: 06_scaled_dot_product_attention.py</summary>

```06_scaled_dot_product_attention.py
import math

import torch
import torch.nn.functional as F


def scaled_dot_product_attention(Q, K, V, mask=None):
    """標準的なScaled Dot-Product Attention

    Args:
        Q: Query [batch, heads, seq_len, d_k]
        K: Key [batch, heads, seq_len, d_k]
        V: Value [batch, heads, seq_len, d_v]
        mask: オプションのマスク [batch, 1, 1, seq_len] or [batch, 1, seq_len, seq_len]

    Returns:
        output: Attention出力 [batch, heads, seq_len, d_v]
        attention_weights: Attention重み [batch, heads, seq_len, seq_len]
    """
    d_k = Q.size(-1)

    # Q @ K^T / sqrt(d_k)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

    # マスクの適用（オプション）
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))

    # Softmaxで確率に変換
    attention_weights = F.softmax(scores, dim=-1)

    # Value との加重和
    output = torch.matmul(attention_weights, V)

    return output, attention_weights
```

</details>

### Cosine Attention（L2正規化版）

<details>
<summary>コード例: 06_cosine_attention.py</summary>

```06_cosine_attention.py
import math

import torch
import torch.nn.functional as F


def scaled_dot_product_attention(Q, K, V, mask=None):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights


def cosine_attention(Q, K, V, temperature=1.0, mask=None):
    """Cosine Attention（Q, Kを正規化）

    この設計では、Q^T K = cos(θ) が厳密に成り立つ。

    Args:
        Q, K, V: Query, Key, Value
        temperature: 温度パラメータ（大きいほど分布が平坦）
        mask: オプションのマスク

    Returns:
        output, attention_weights
    """
    # Q, Kを単位ノルムに正規化
    Q_norm = F.normalize(Q, dim=-1)  # dim=-1 で最後の次元を正規化
    K_norm = F.normalize(K, dim=-1)

    # 内積 = コサイン類似度
    scores = torch.matmul(Q_norm, K_norm.transpose(-2, -1)) / temperature

    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))

    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)

    return output, attention_weights


# 使用例：標準 vs Cosine の比較
batch, heads, seq_len, d_k = 2, 8, 10, 64
Q = torch.randn(batch, heads, seq_len, d_k)
K = torch.randn(batch, heads, seq_len, d_k)
V = torch.randn(batch, heads, seq_len, d_k)

# 標準Attention
out_standard, attn_standard = scaled_dot_product_attention(Q, K, V)

# Cosine Attention
out_cosine, attn_cosine = cosine_attention(Q, K, V)

print("標準Attention: scores range depends on norms")
print("Cosine Attention: scores in [-1, 1] (cosine similarity)")
```

</details>

### RoPEの実装

<details>
<summary>コード例: 06_rope.py</summary>

```06_rope.py
import math

import torch
import torch.nn.functional as F


def scaled_dot_product_attention(Q, K, V, mask=None):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights


def precompute_rope_freqs(dim, max_seq_len, theta=10000.0):
    """RoPEの周波数を事前計算

    Args:
        dim: 埋め込み次元（偶数）
        max_seq_len: 最大シーケンス長
        theta: 基本周波数

    Returns:
        freqs_cos, freqs_sin: [max_seq_len, dim/2] の周波数テンソル
    """
    # 各次元ペアの周波数
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))

    # 位置インデックス
    positions = torch.arange(max_seq_len).float()

    # 外積で [seq_len, dim/2] の角度を計算
    angles = torch.outer(positions, freqs)

    return torch.cos(angles), torch.sin(angles)


def apply_rope(x, freqs_cos, freqs_sin):
    """RoPEを適用

    Args:
        x: 入力テンソル [batch, heads, seq_len, dim]
        freqs_cos, freqs_sin: 事前計算された周波数

    Returns:
        回転が適用されたテンソル
    """
    # 次元を2つずつのペアに分割
    x_reshape = x.reshape(*x.shape[:-1], -1, 2)
    x1, x2 = x_reshape[..., 0], x_reshape[..., 1]

    # シーケンス長に合わせてスライス
    seq_len = x.size(-2)
    cos = freqs_cos[:seq_len].unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, dim/2]
    sin = freqs_sin[:seq_len].unsqueeze(0).unsqueeze(0)

    # 回転を適用: (x1, x2) -> (x1*cos - x2*sin, x1*sin + x2*cos)
    x1_rot = x1 * cos - x2 * sin
    x2_rot = x1 * sin + x2 * cos

    # 元の形状に戻す
    x_rot = torch.stack([x1_rot, x2_rot], dim=-1)
    return x_rot.reshape(*x.shape)


# 使用例
dim, max_len = 64, 512
freqs_cos, freqs_sin = precompute_rope_freqs(dim, max_len)
Q = torch.randn(2, 8, 10, dim)
K = torch.randn(2, 8, 10, dim)
V = torch.randn(2, 8, 10, dim)

# Query, Keyに適用
Q_rope = apply_rope(Q, freqs_cos, freqs_sin)
K_rope = apply_rope(K, freqs_cos, freqs_sin)

# RoPE適用後のAttention
out_rope, attn_rope = scaled_dot_product_attention(Q_rope, K_rope, V)
```

</details>

### Multi-head Attentionの完全な実装

<details>
<summary>コード例: 06_multi_head_attention.py</summary>

```06_multi_head_attention.py
import math

import torch
import torch.nn as nn
import torch.nn.functional as F


def precompute_rope_freqs(dim, max_seq_len, theta=10000.0):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))
    positions = torch.arange(max_seq_len).float()
    angles = torch.outer(positions, freqs)
    return torch.cos(angles), torch.sin(angles)


def apply_rope(x, freqs_cos, freqs_sin):
    x_reshape = x.reshape(*x.shape[:-1], -1, 2)
    x1, x2 = x_reshape[..., 0], x_reshape[..., 1]
    seq_len = x.size(-2)
    cos = freqs_cos[:seq_len].unsqueeze(0).unsqueeze(0)
    sin = freqs_sin[:seq_len].unsqueeze(0).unsqueeze(0)
    x1_rot = x1 * cos - x2 * sin
    x2_rot = x1 * sin + x2 * cos
    x_rot = torch.stack([x1_rot, x2_rot], dim=-1)
    return x_rot.reshape(*x.shape)


def scaled_dot_product_attention(Q, K, V, mask=None):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights


def cosine_attention(Q, K, V, temperature=1.0, mask=None):
    Q_norm = F.normalize(Q, dim=-1)
    K_norm = F.normalize(K, dim=-1)
    scores = torch.matmul(Q_norm, K_norm.transpose(-2, -1)) / temperature
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float("-inf"))
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights


class MultiHeadAttention(nn.Module):
    """Multi-head Attention（教育目的の実装）"""

    def __init__(self, d_model, num_heads, use_rope=False, use_cosine=False):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.use_rope = use_rope
        self.use_cosine = use_cosine

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        if use_rope:
            freqs_cos, freqs_sin = precompute_rope_freqs(self.d_k, 2048)
            self.register_buffer("freqs_cos", freqs_cos)
            self.register_buffer("freqs_sin", freqs_sin)

    def forward(self, x, mask=None):
        batch_size, seq_len, _ = x.shape

        # 線形射影
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # RoPEの適用（オプション）
        if self.use_rope:
            Q = apply_rope(Q, self.freqs_cos, self.freqs_sin)
            K = apply_rope(K, self.freqs_cos, self.freqs_sin)

        # Attention計算
        if self.use_cosine:
            output, _ = cosine_attention(Q, K, V, mask=mask)
        else:
            output, _ = scaled_dot_product_attention(Q, K, V, mask=mask)

        # ヘッドを結合
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

        return self.W_o(output)
```

</details>

## 参考文献

### Transformer

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*. arXiv: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762).
    - Transformerの原論文。Scaled Dot-Product AttentionとMulti-head Attentionを導入。

### T5

- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *JMLR*, 21(140), 1-67. arXiv: [arXiv:1910.10683](https://arxiv.org/abs/1910.10683).
    - T5の原論文。すべてのNLPタスクをText-to-Textフレームワークに統一する設計を提案。

### RoPE

- Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., & Liu, Y. (2024). RoFormer: Enhanced Transformer with Rotary Position Embedding. *Neurocomputing*, 568, 127063. arXiv: [arXiv:2104.09864](https://arxiv.org/abs/2104.09864).
    - RoPEの原論文。回転による相対位置表現を提案。最初の提案は2021年（arXiv）、2024年にNeurocomputing誌に掲載。
- Press, O., Smith, N. A., & Lewis, M. (2022). Train Short, Test Long: Attention with Linear Biases Enables Input Length Generalization. *ICLR 2022*. arXiv: [arXiv:2108.12409](https://arxiv.org/abs/2108.12409).
    - ALiBi（Attention with Linear Biases）の原論文。RoPEとは異なるアプローチで相対位置を導入。

### nGPTと正規化設計

- Loshchilov, I., Hsieh, C.-P., Sun, S., & Ginsburg, B. (2025). nGPT: Normalized Transformer with Representation Learning on the Hypersphere. *ICLR 2025*. arXiv: [arXiv:2410.01131](https://arxiv.org/abs/2410.01131) (2024).
    - すべての表現を単位球面上に制約するTransformer設計。arXiv 2024、ICLR 2025に採択。

### カーネル法とAttentionの関係

- Tsai, Y.-H. H., Bai, S., Yamada, M., & Morency, L.-P. (2019). Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel. *EMNLP 2019*. arXiv: [arXiv:1908.11775](https://arxiv.org/abs/1908.11775).
    - Attentionとカーネル法の関係を分析。
