# Appendix 4: 空間の「物差し」再考: 2点間から情報の密度まで

## 注意事項

本Appendixで扱う内容には、確立された数学的事実と、教育的な解釈が混在している。特に以下の点に留意されたい：

- **「点」という言葉の多義性について**：本講義では、文脈に応じて「点」の意味が変わる。
    - **第7回まで（表現空間）**：データの位置を表すベクトル $\mathbf{h}$ 。不確実性を持たないため、分布（雲）と対比された。従来の点埋め込みは $\kappa = \infty$ （確信度無限大）の極限状態と解釈できる。
    - **今回（統計多様体）**：分布のパラメータ $\theta = (\boldsymbol{\mu}, \kappa)$ 。この空間上の1点は、第7回で扱った**分布（雲）全体**を代表している。「点 $\theta$ を動かす」ことは、「雲の形や位置を変える」ことと同義である。
  
  つまり、**表現空間での「分布（レベル2）」は、パラメータ空間では「点（レベル3）」として扱われる**。これは視座の転換であり、矛盾ではない。第7回が「ズームイン（点を拡大して雲を見る）」なら、今回は「ズームアウト（雲を操るコントローラーの座標を見る）」である。

- **「距離」と「ダイバージェンス」の区別**：本資料では数学的に厳密な距離（対称・三角不等式を満たす）と、非対称な「隔たりの尺度」であるダイバージェンスを明確に区別する。KLダイバージェンスは対称性を持たないため、数学的には距離ではない。
- **「情報密度」の用語定義**：本Appendixで用いる「情報密度」は、データ点の密度やベクトル空間の分布ではなく、 **フィッシャー情報行列が表す「パラメータ変化に対する予測分布の感度」** を指す教育的比喩である。この用語は機械学習文脈で複数の意味を持ちうるため、本資料では上記の定義に限定して使用する。
- **フィッシャー情報行列の解釈**：「空間の密度」「ジャングルと砂漠」は教育的メタファーであり、厳密には「パラメータに対する予測分布の感度」を表す計量テンソルである。
- **自然勾配法の効果**：自然勾配法が「常に通常勾配より優れる」わけではない。計算コスト、近似精度、タスク特性により実用性が変わる。大規模モデルでは近似手法（K-FAC等）が必要。
- **情報密度とAttention/MoEの関係**：「Attentionが情報密度の高い場所に重みを置く」という記述は、直感的理解を助けるための**比喩的対応づけ**である。Attentionは内積幾何に基づく類似度計算、Fisherは統計モデルの計量であり、厳密には別概念である。また、因果関係は双方向的で、学習によって特定領域の情報密度（感度）が**形成される**側面もある。

## 導入：「物差し」が世界を定義する

### 幾何学とは「測る」こと

空間を理解するとは、そこで使える **「物差し（計量、metric）」** を知ることである。

小学校で習う算数では、2点間の距離は定規で測る。しかし、その「定規」自体が、どのような空間にいるかによって変わる。平らな紙の上では直線距離（ユークリッド距離）が自然だが、地球儀の上では大円に沿った測地距離が適切だ。

深層学習における表現空間も同様である。ベクトルの「近さ」を測る方法は一つではない：

| 測り方 | 何を測るか | 特性 | 深層学習での例 |
| --- | --- | --- | --- |
| **ユークリッド距離** | 2点間の直線距離 | 対称、三角不等式を満たす | Word2Vec（初期）、PCA |
| **内積（dot product）** | ベクトルの大きさと方向 | 対称 | Attention（scaled dot-product） |
| **コサイン類似度** | ベクトルの方向の近さ | 対称、長さを無視 | 正規化された埋め込み（L2正規化後の内積） |
| **KLダイバージェンス** | 分布の「隔たり」 | 非対称 | 損失関数、VAE |
| **フィッシャー情報** | 分布の変化しやすさ（感度） | 計量テンソル | 自然勾配法、パラメータ空間の幾何 |

### 「点」の正体：スカラではなく分布の代表

測定の前に、まず「何を測るのか」を明確にする必要がある。情報幾何学において、**「点」は単なる数値（スカラ）や座標ではない**。

数学的には、情報幾何学で扱う「点」は以下の3つの階層を持つ：

| 階層 | 数学的実体 | 講義での位置づけ | 日常的なイメージ | 不確実性 |
| --- | --- | --- | --- | --- |
| **レベル1：データ点** | $\mathbf{x} \in \mathbb{R}^n$ | 観測データそのもの | 夜空の光点 | — |
| **レベル2：表現ベクトル** | $\mathbf{h} \in \mathbb{R}^d$ | **第7回で拡張した対象**。<br>従来の点埋め込みは、分布埋め込みの視点では $\kappa \to \infty$ （確信度無限大）の極限に相当すると見なせる。<br>第7回では、これを有限の $\kappa$ を持つ分布に拡張した。 | 望遠鏡で見る前の星<br>（点 or 霧） | 点： $\kappa = \infty$ <br>分布： $\kappa$ 有限 |
| **レベル3：統計多様体の点** | $\theta = (\boldsymbol{\mu}, \kappa)$ | **今回扱う対象**。<br>分布（雲）の形状を決めるパラメータ座標。<br>**この「点」は、レベル2の「分布全体」を代表する。** | 星雲カタログの登録番号<br>（雲の設計図） | 次の階層で議論 |

**重要なのは第3の階層である。** 統計多様体の各点は、ある確率分布に対応し、 $\theta$ はその点を指定する **座標（パラメータ）** である。つまり：

- **一般的な直感：** 点 = 位置を表すスカラや座標
- **第7回（表現空間）：** 点（ $\kappa = \infty$ ） vs 分布（ $\kappa$ 有限）の対比
- **今回（統計多様体）：** 点 $\theta$ = ある確率分布を代表し、その背後に第7回で扱った「雲」全体が広がっている

たとえば、vMF分布族は $(\boldsymbol{\mu}, \kappa)$ という2つのパラメータで指定される多様体を成す。各点（パラメータ $(\boldsymbol{\mu}, \kappa)$ ）は一つのvMF分布に対応し、その背後には**球面上の全ての可能な $\mathbf{h}$ に対する確率密度という関数的な構造**が広がっている。

第7回では「点埋め込み（ $\kappa = \infty$ ）を分布埋め込み（ $\kappa$ 有限）に拡張する」ことで解像度を上げた。今回は、その**分布を操るパラメータ空間**に視座を移す。「雲の形（ $\kappa$ ）」や「雲の位置（ $\boldsymbol{\mu}$ ）」を変えることは、パラメータ空間上で点 $\theta$ を動かすことに対応する。

> [!IMPORTANT]
> **次元の注意：** 統計多様体自体は通常**有限次元**（パラメータ数に等しい）である。正規分布族は2次元、カテゴリカル（ $k$ クラス）は、単体の内部 ( $p_i>0$ , $\sum_i p_i=1$ ) では $k-1$ 次元の **滑らかな多様体** となる（境界 $p_i=0$ を含めると幾何が特異になりうる）。分布そのものは関数なので背後に豊かな構造があるが、多様体として扱うときは $\theta$ の次元数だけの有限次元空間である。

> [!TIP]
> **比喩的理解：点は「天体のカタログ番号」**<br>
> 夜空を見上げると、星が点として見える。しかし、その一つ一つの「点」は、実際には巨大な天体であり、惑星や衛星や無数の構造を内包している。
>
> 第7回では、この「点」を望遠鏡で拡大し、「星雲（分布）」として観察した。霧の濃さ（ $\kappa$ ）や中心の位置（ $\boldsymbol{\mu}$ ）という、より詳細な構造が見えるようになった。
>
> 今回（統計多様体）では、さらに視座を変える。天文学者が星雲を「カタログ番号」で管理するように、我々は分布を**パラメータ $\theta = (\boldsymbol{\mu}, \kappa)$ という座標**で指定する。この座標は、まさに「設計図」や「レシピ」である：
>
> - 座標 $\theta_1 = (\boldsymbol{\mu}_1, \kappa_1)$ → 「こういう形の雲を作れ」
> - 座標 $\theta_2 = (\boldsymbol{\mu}_2, \kappa_2)$ → 「別の形の雲を作れ」
>
> 座標 $\theta$ という「位置」の背後に、分布という「形」が存在する。**パラメータ空間上で点を動かすことは、表現空間上で雲の形を変えることに対応する。** この視点転換が、情報幾何学の出発点である。
>
### AIにとっての多層的な定規

AIにとっての空間は、単なる物理的な「長さ」だけでなく、以下のような多層的な定規で構成されている：

1. **配置の定規**：どこに点を置くか（対称的な距離・内積）
2. **学習の定規**：どの方向に進むか（非対称なダイバージェンス）
3. **感度の定規**：空間のどこが「硬い」か（フィッシャー情報行列）

本Appendixでは、これら3つの視点から「物差し」を再考し、情報幾何学の核心的な概念を、数式を最小限に抑えつつ直感的に理解することを目指す。

> [!TIP]
> **読者の幾何学的直感の拡張**：物理的な「長さ」という直感から、情報的な「量」へと視野を広げることが本Appendixの目標である。距離を測ることは、単に「どれだけ離れているか」を知るだけでなく、「どう変化するか」「どこが重要か」を知ることでもある。そして何よりも、**「点」という存在が、実は巨大な構造を内包している**ことを理解することが重要だ。

## 対称的な尺度（距離）：配置と構造の固定

### 内積：方向と大きさを測る基本道具

最も基本的な「測り方」は **内積（inner product / dot product）** である。2つのベクトル $\mathbf{u}, \mathbf{v}$ の内積は：

$$\mathbf{u} \cdot \mathbf{v} = \sum_i u_i v_i = \|\mathbf{u}\| \|\mathbf{v}\| \cos\theta$$

この式は、内積が以下の2つの情報を含むことを示している：

- **大きさ**（ノルム $\|\mathbf{u}\|, \|\mathbf{v}\|$ ）
- **方向の近さ**（角度 $\theta$ のコサイン）

### コサイン類似度：方向だけを見る

両方のベクトルを正規化（単位長に）すると、内積は純粋に方向だけを測る **コサイン類似度** になる：

$$\text{cosine similarity} = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \cos\theta$$

これは、ベクトルの大きさ（ノルム）を無視し、向きだけに注目する尺度である。

### Attentionとの関係

第6回で扱ったAttention機構（標準的なscaled dot-product attention）は、QueryとKeyの **内積（dot product）** を基本としている：

$$\text{score}(q_i, k_j) = \frac{q_i^\top k_j}{\sqrt{d_k}}$$

この内積は、**ベクトルの大きさと方向の両方**を反映する。ただし、実装によっては：

- **L2正規化されたAttention**：Query/Keyを事前に正規化すれば、内積はコサイン類似度と等価になる
- **標準的なAttention**：正規化なしでは、内積はベクトルの大きさも考慮する

```appendix4_cosine_similarity.py
import torch


def dot_product(u, v):
    """内積の計算（標準的なAttentionに相当）"""
    return torch.dot(u, v)


def cosine_similarity(u, v):
    """コサイン類似度の計算（正規化後の内積）"""
    u_norm = u / u.norm()
    v_norm = v / v.norm()
    return torch.dot(u_norm, v_norm)


# 例：同じ方向だが大きさが異なるベクトル
u = torch.tensor([1.0, 2.0, 3.0])
v = torch.tensor([2.0, 4.0, 6.0])  # uの2倍

print(f"内積: {dot_product(u, v):.2f}")
print(f"コサイン類似度: {cosine_similarity(u, v):.4f}")
# 内積: 28.00  ← 大きさも反映される
# コサイン類似度: 1.0000  ← 完全に同じ方向（大きさは無視）
```

> [!NOTE]
> **第6回との接続**：標準的なAttentionは内積を使用し、Softmaxで正規化する。これは「どのKeyがQueryと高い内積を持つか」を測定し、その値に応じて情報を集約する操作である。内積はベクトルの大きさも考慮するため、純粋な「方向の類似度」とは異なる。一部のモデル（例：nGPTなど）では明示的にL2正規化を行い、コサイン類似度ベースのAttentionを実装している。

### 双曲距離：階層構造の深さを測る

ユークリッド空間や球面空間とは異なる計量として、**双曲距離（hyperbolic distance）** がある。これは負の曲率を持つ空間での距離である。

第12回で詳しく扱ったが、双曲空間の重要な特徴は：

- **中心から離れるほど空間が指数的に広がる**
- **階層構造（木構造）の埋め込みに適している**

例えば、Poincaré球モデルでは、距離は以下のように定義される：

$$d(\mathbf{u}, \mathbf{v}) = \text{arcosh}\left(1 + 2\frac{\|\mathbf{u} - \mathbf{v}\|^2}{(1-\|\mathbf{u}\|^2)(1-\|\mathbf{v}\|^2)}\right)$$

この距離は、中心（原点）の近くでは「親」を表し、境界に近いほど「深い子孫」を表現できる。

> [!NOTE]
> **実装上の注意**：Poincaré球モデルでは、すべての点が $\|\mathbf{u}\| < 1$ を満たす必要がある（開球内）。実装時には数値安定性のため、以下の対策が推奨される：
>
> 1. **境界クリッピング**：境界 $\|\mathbf{u}\| = 1$ に近づきすぎないよう、例えば $\|\mathbf{u}\| < 1 - \epsilon$ （ $\epsilon = 10^{-5}$ ）でクリップ
> 2. **arcoshの引数の保証**：浮動小数点誤差で arcosh の引数が 1 未満になり得るため、`arg = arg.clamp(min=1 + eps)` のような処理が有用
> 3. **ゼロ除算回避**：分母 $(1-\|\mathbf{u}\|^2)(1-\|\mathbf{v}\|^2)$ がゼロにならないよう、上記のクリッピングと合わせて対策する

| 空間 | 曲率 | 距離の特徴 | 適した構造 | 深層学習での例 |
| --- | --- | --- | --- | --- |
| **ユークリッド** | 0 | 一様 | 特定の構造なし | PCA、初期のWord2Vec |
| **球面** | 正 | 中心への回帰性 | 方向の多様性 | 正規化埋め込み（第3回） |
| **双曲** | 負 | 外に向かって拡張 | 階層・木構造 | Poincaré Embedding（第12回） |

> [!IMPORTANT]
> **静的な配置としての距離**：これらの距離（内積、コサイン類似度、双曲距離）はすべて **対称的** である。つまり、点Aから点Bへの距離と、点Bから点Aへの距離が等しい。これらは **「どこに配置するか」を決める静的な地図（Static Map）** のための定規である。しかし、学習（パラメータ更新）の方向性を決めるには、非対称な尺度が必要になる。

> [!NOTE]
> **Attentionと内積**：第6回で扱う標準的なAttention（scaled dot-product attention）は、内積を基本とする。内積はベクトルの大きさと方向の両方を反映するため、純粋な「方向の類似度」（コサイン類似度）とは異なる。一部のモデル（nGPTなど）では明示的にL2正規化を行い、コサイン類似度ベースのAttentionを実装している。

## 非対称な尺度（ダイバージェンス）：学習の駆動力

### 距離では学習の方向が見えない

対称的な距離は、2点間の「隔たり」を測るが、**どちらからどちらに向かうべきか**という方向性は持たない。

学習とは、モデルの分布 $Q$ を真の分布 $P$ に **近づける** プロセスである。この「近づく」という方向性を表現するには、非対称な尺度が必要だ。

### KLダイバージェンス：分布の非対称な隔たり

**KLダイバージェンス（Kullback-Leibler divergence）** は、2つの確率分布 $P$ と $Q$ の「隔たり」を測るが、行きと帰りで値が異なる：

$$D_{\text{KL}}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}$$

$$D_{\text{KL}}(Q \| P) = \sum_x Q(x) \log \frac{Q(x)}{P(x)}$$

一般に、 $D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)$ である。

### 非対称性の幾何学的意味

なぜ非対称なのか？ それは、**「どちらの視点で測るか」が異なる**からである。

- $D_{\text{KL}}(P \| Q)$ ：「真の分布 $P$ から見て、モデル $Q$ がどれだけズレているか」
    - $P$ の確率が高い場所で $Q$ の確率が低いと、大きなペナルティ
    - **Forward KL** とも呼ばれる
- $D_{\text{KL}}(Q \| P)$ ：「モデル $Q$ から見て、真の分布 $P$ がどれだけズレているか」
    - $Q$ の確率が高い場所で $P$ の確率が低いと、大きなペナルティ
    - **Reverse KL** とも呼ばれる

```appendix4_kl_asymmetry.py
import torch

# 真の分布 P と モデル Q
P = torch.tensor([0.1, 0.6, 0.3])
Q = torch.tensor([0.4, 0.3, 0.3])


# KLダイバージェンスの計算（手計算版）
def kl_divergence(p, q):
    """KL(P||Q) を計算"""
    return (p * torch.log(p / q)).sum()


kl_pq = kl_divergence(P, Q)
kl_qp = kl_divergence(Q, P)

print(f"KL(P||Q): {kl_pq:.4f}")
print(f"KL(Q||P): {kl_qp:.4f}")
print(f"非対称性: {abs(kl_pq - kl_qp):.4f}")
# 出力例:
# KL(P||Q): 0.2332
# KL(Q||P): 0.1596
# 非対称性: 0.0736
```

> [!NOTE]
> **PyTorchの実装**：PyTorchの `F.kl_div` は内部で対数を取るため、引数の順序に注意が必要。`F.kl_div(Q.log(), P, reduction='sum')` は $D_{\text{KL}}(P \| Q)$ を計算する。

### 非対称性が学習を駆動する

この非対称性こそが、**損失関数として学習（パラメータ更新）を駆動するエネルギー**となる。

例えば、クロスエントロピー損失は、KLダイバージェンスと密接に関連している：

$$\mathcal{L} _{\text{CE}} = -\sum _x P(x) \log Q(x) = D _{\text{KL}}(P \| Q) + H(P)$$

ここで $H(P)$ は $P$ のエントロピー（定数）。つまり、クロスエントロピー損失を最小化することは、Forward KL $D_{\text{KL}}(P \| Q)$ を最小化することと等価である。

| 尺度 | 対称性 | 用途 | 講義での位置 |
| --- | --- | --- | --- |
| **距離（内積・コサイン）** | 対称 | 配置の決定 | 第6回（Attention） |
| **双曲距離** | 対称 | 階層構造の表現 | 第12回 |
| **KLダイバージェンス** | 非対称 | 学習の駆動 | 第4回（Softmax） |

> [!IMPORTANT]
> **第4回との接続**：第4回で扱ったSoftmax/Cross-Entropyの幾何学的意味は、まさにこの非対称なKLダイバージェンスにある。モデルの分布を真の分布に「引き寄せる」力が、学習の勾配となる。対称的な距離では、この引力の方向が定義できない。

## 計量としてのフィッシャー情報：空間の感度を測る

### 「局所と大局」の比喩とその限界

情報幾何学を説明する際、しばしば「局所的な構造」と「大局的な構造」という比喩が用いられる。この比喩は、空間の性質を理解する手がかりとして有用である。しかし、**この比喩自体が局所的にしか成立しない**ことに注意が必要だ。

なぜなら、「局所的」と「大局的」という言葉自体が、ある特定の文脈（ユークリッド幾何的直感）に依存しているからである。情報幾何学における「曲がった空間」では、この二分法が必ずしも適切でない場合がある。

> [!WARNING]
> **比喩の適用範囲**：「局所 vs 大局」という対比は、平坦な空間の直感から出発した比喩である。曲率が大きい空間や、統計多様体の境界付近では、この比喩が破綻する可能性がある。比喩はあくまで理解の入り口であり、厳密な数学的定義に立ち返ることが重要だ。

### フィッシャー情報行列：分布の感度を測る

**フィッシャー情報行列（Fisher Information Matrix）** は、パラメータ $\theta$ を少し変えたとき、予測分布 $p(x; \theta)$ がどれだけ敏感に変化するかを測る尺度である。

数学的には、スコア関数（対数尤度の勾配）の共分散として定義される：

$$F_{ij}(\theta) = \mathbb{E}_{p(x;\theta)}\left[\frac{\partial \log p(x;\theta)}{\partial \theta_i} \frac{\partial \log p(x;\theta)}{\partial \theta_j}\right]$$

この行列は、パラメータ空間上の **計量テンソル（metric tensor）** として機能する。つまり、フィッシャー情報行列は、パラメータ空間における「距離の測り方」を定義する。

#### KLダイバージェンスとの関係

フィッシャー情報行列の幾何学的意味は、**KLダイバージェンスの局所2次近似**として最も明快に理解できる。パラメータを $\theta$ から $\theta + d\theta$ に微小変化させたとき、分布 $p_\theta(x) = p(x;\theta)$ 間のKLダイバージェンスは以下のように展開される：

$$D_{\text{KL}}(p_\theta \| p_{\theta+d\theta}) \approx \frac{1}{2} d\theta^\top F(\theta) d\theta + O(|d\theta|^3)$$

ここで1次項が消えるのは、**同一パラメータ点ではスコア関数（対数尤度の勾配）の期待値（ $p(x;\theta)$ に関する期待値）が0になる**ためである。その結果、2次の項が支配的になる。

重要なのは、**逆向きのKLダイバージェンス $D_{\text{KL}}(p_{\theta+d\theta} \| p_\theta)$ も、（正則性条件の下で）2次の項は同じフィッシャー情報行列になる**点である。つまり、フィッシャー情報行列は、分布の「局所的な隔たりの2次形式」を表している。

これにより、フィッシャー情報行列は以下の2つの顔を持つ：

1. **統計的解釈**：パラメータ推定の精度（クラメール・ラオの下限）
2. **幾何学的解釈**：分布空間における「距離の2次形式」

### 直感的理解：ジャングルと砂漠のメタファー

フィッシャー情報行列を直感的に理解するための比喩として、**「情報密度の地形」** がある：

- **情報密度が高い場所（ジャングル）**：パラメータを少し動かすと、分布が大きく変化する領域。学習が難しく、慎重な更新が必要。
- **情報密度が低い場所（砂漠）**：パラメータを大きく動かしても、分布があまり変わらない領域。学習が容易で、大胆な更新が可能。

> [!NOTE]
> **メタファーの限界**：「ジャングルと砂漠」は教育的比喩であり、厳密には「パラメータに対する予測分布の感度」を表す。実際の統計多様体の幾何は、この比喩よりも複雑である。

### 数値例：正規分布の場合

正規分布 $\mathcal{N}(\mu, \sigma^2)$ のフィッシャー情報行列は、解析的に求められる：

$$F = \begin{pmatrix} \frac{1}{\sigma^2} & 0 \\\\ 0 & \frac{2}{\sigma^4} \end{pmatrix}$$

これは以下を意味する：

- **分散 $\sigma^2$ が小さいほど、平均 $\mu$ への感度が高い**（ $F_{\mu\mu} = 1/\sigma^2$ ）
- **分散 $\sigma^2$ への感度は、さらに急峻に増加する**（ $F_{\sigma\sigma} = 2/\sigma^4$ ）

分散が小さい（鋭い）分布ほど、パラメータの変化に敏感である。

#### 第7回との接続：vMF分布と $\kappa$ による直感

第7回で導入した $\kappa$ （集中度）を思い出してほしい。vMF分布のフィッシャー情報行列は、この $\kappa$ に強く依存する。

**なぜ「曲がっている」と言えるのか？**

- $\kappa$ が大きい（高確信度）場合：分布は鋭く尖っている。このとき、中心方向 $\boldsymbol{\mu}$ を少しでもズラすと、確率密度は劇的に変化する。つまり**感度（フィッシャー情報量）が高い**。パラメータ空間のこの領域は「ジャングル」——慎重な更新が必要な、情報密度の高い場所。
  
- $\kappa$ が小さい（低確信度）場合：分布はボヤけている。中心 $\boldsymbol{\mu}$ を多少ズラしても、確率密度はあまり変わらない。つまり**感度（フィッシャー情報量）が低い**。パラメータ空間のこの領域は「砂漠」——大胆な更新が可能な、情報密度の低い場所。

このように、分布の「形（ $\kappa$ ）」によって、パラメータを動かしたときの影響度（距離感）が変わる。これが**「空間が歪んでいる」ことの正体**である。

正規分布の場合と同様、vMF分布でも：

$$F_{\boldsymbol{\mu}\boldsymbol{\mu}} \propto \kappa \quad \text{（高次元極限）}$$

つまり、集中度 $\kappa$ が大きいほど、平均方向 $\boldsymbol{\mu}$ への感度が高くなる。第7回で「解像度を上げる（ $\kappa$ を無限大から有限に）」ことで得た不確実性の情報は、パラメータ空間の幾何（フィッシャー計量）に直接反映されている。

> [!NOTE]
> **表現空間とパラメータ空間の対応**：
>
> - 第7回（表現空間）：「 $\kappa$ が小さい → 霧が濃い → 不確実性が高い」
> - 今回（パラメータ空間）：「 $\kappa$ が小さい → $F_{\boldsymbol{\mu}\boldsymbol{\mu}}$ が小さい → 感度が低い（砂漠）」
>
> 両者は同じコインの裏表である。表現空間で「確信が持てない（霧が濃い）」領域は、パラメータ空間では「どう動いても影響が小さい（砂漠）」領域に対応する。

### 自然勾配法：曲がった空間での最適化

通常の勾配降下法は、パラメータ空間を **平坦なユークリッド空間** として扱う。しかし、統計多様体は一般に曲がっている。

**自然勾配法（Natural Gradient Descent）** は、フィッシャー情報行列を計量として用いることで、曲がった空間での「真の勾配」を計算する：

$$\theta_{t+1} = \theta_t - \eta F(\theta_t)^{-1} \nabla_\theta \mathcal{L}$$

ここで $F(\theta)^{-1}$ は、フィッシャー情報行列の逆行列である。

#### 座標不変性：パラメータ化によらない学習

自然勾配法の最も重要な動機は、**座標不変性（coordinate invariance）** である。

通常の勾配降下法は、パラメータの定義の仕方（座標系の選び方）に依存してしまう。例えば、同じ分布を表すパラメータでも、 $\theta$ を使うか $\phi = f(\theta)$ を使うかで、勾配の方向が変わってしまう。

しかし、学習の軌道は本来、**パラメータの表し方に左右されるべきではない**。自然勾配法は、フィッシャー情報行列を使って勾配を補正することで、**微小ステップの極限において、どのようなパラメータ化を選んでも分布空間での同じ方向に進む**ことを実現する。

### 自然勾配の幾何学的意味：「雲」をどう動かすか

自然勾配法は、以下のような幾何学的解釈を持つ：

- **通常の勾配**：パラメータ空間（地図）での最急降下方向（ユークリッド計量）
- **自然勾配**：統計多様体（実際の地形）での最急降下方向（フィッシャー計量）

第7回との関連で言い換えれば：

**通常の勾配法**は、「パラメータの数値（ $\boldsymbol{\mu}$ や $\kappa$ という座標）をどれだけ変えるか」を基準にする。地図上で等距離に見える2つの移動が選ばれる。

**自然勾配法**は、「表現空間での分布の形（雲）がどれだけ変わるか」を基準にする。つまり、**KLダイバージェンスの意味で一定の変化量**を実現するように、パラメータの更新幅を調整する。

これにより：

- $\kappa$ **が大きくて敏感な場所（ジャングル）** ：少しの更新でも分布が大きく変わるため、 $F^{-1}$ が小さくなり、更新幅が抑制される（慎重に進む）
- $\kappa$ **が小さくて鈍感な場所（砂漠）** ：大きな更新をしても分布があまり変わらないため、 $F^{-1}$ が大きくなり、更新幅が拡大される（大胆に進む）

つまり、自然勾配法は、**情報密度の高い場所では小さく、低い場所では大きく更新する**ことで、効率的な学習を実現する。パラメータの数値を変えるのではなく、**分布の形を直接操作している**と解釈できる。

| 性質 | 通常の勾配 | 自然勾配 |
| --- | --- | --- |
| **何を測るか** | パラメータ座標の変化量 | 分布の形の変化量（KLダイバージェンス） |
| **パラメータ化への依存** | あり（座標系に依存） | なし（座標不変） |
| **空間の扱い** | 平坦と仮定 | 曲がりを考慮 |
| **第7回との関係** | 「座標を動かす」 | 「雲を動かす」 |
| **収束速度** | 情報密度の高い場所で遅い | 情報幾何の意味で合理的（条件により改善） |
| **計算コスト** | 低い | 高い（ $F^{-1}$ の計算） |

> [!TIP]
> **AdamやRMSpropとの類似性**：Adamなどの適応的学習率手法は、各パラメータに対して異なる学習率を使う。これは、暗黙的に「空間の歪み」を補正していると解釈できる。完全なフィッシャー情報行列を使わないが、対角成分（座標ごとの感度）を近似的に考慮している。理想的には真のフィッシャー計量を使いたいが、計算コストとの兼ね合いで対角近似やK-FACなどの近似手法が実用化されている。

### 実装上の課題

自然勾配法の主な課題は、**フィッシャー情報行列の計算コスト**である：

- パラメータ数が $d$ のとき、 $F$ は $d \times d$ 行列
- 大規模モデル（数億〜数兆パラメータ）では、 $F$ の計算・保持・逆行列計算が現実的でない

このため、実用的には以下の近似手法が用いられる：

- **K-FAC（Kronecker-Factored Approximate Curvature）**：行列をブロック対角化し、各ブロックをKronecker積で近似
- **対角近似**：行列の対角成分のみを使用（Adam等と類似）

### AttentionとMoEとの比喩的対応

> [!IMPORTANT]
> **比喩的理解としての注意**：以下の記述は、直感的理解を助けるための**比喩的対応づけ**である。Attention機構やMoEの設計原理は、フィッシャー情報行列そのものではなく、内積幾何やルーティング機構に基づいている。

Attention機構やMoE（Mixture of Experts）は、計算資源を動的に配分する仕組みである。これを「情報密度」との比喩的対応で理解することができる：

- **Attention**：情報密度（内積による類似度）の高いTokenに重みを置く
- **MoE**：情報密度（入力の特徴）に応じて、適切なExpertに計算を振り分ける

ただし、因果関係は双方向的である：

1. **順方向**：学習済みモデルは、情報密度の高い場所に注意を向ける
2. **逆方向**：学習プロセスが、特定領域の情報密度（感度）を**形成する**

つまり、Attentionが「情報密度を読み取る」だけでなく、学習によって「情報密度を作り出す」側面もある。

> [!NOTE]
> **第13回との接続**：第13回で扱うMoEは、入力に応じて計算を動的に配分する。これは、「情報のある場所に計算を集中させる」という意味で、フィッシャー情報の比喩と緩やかに対応する。ただし、MoEのルーティング機構は、厳密にはフィッシャー情報に基づいていない。

## 付録：コード例

### KLダイバージェンスの非対称性の可視化

<details>
<summary>コード例: appendix4_kl_visualization.py</summary>

```appendix4_kl_visualization.py
import matplotlib.pyplot as plt
import numpy as np
import torch


def kl_divergence(p, q):
    """KL(P||Q) を計算（0除算回避付き）"""
    epsilon = 1e-10
    return (p * torch.log((p + epsilon) / (q + epsilon))).sum()


# 2つの分布を定義
P = torch.tensor([0.7, 0.2, 0.1])
Q_list = []
kl_pq_list = []
kl_qp_list = []

# Qの最初の要素を変化させる
for q0 in np.linspace(0.1, 0.9, 50):
    Q = torch.tensor([q0, (1 - q0) * 0.6, (1 - q0) * 0.4])
    Q_list.append(q0)
    kl_pq_list.append(kl_divergence(P, Q).item())
    kl_qp_list.append(kl_divergence(Q, P).item())

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(Q_list, kl_pq_list, label="KL(P||Q)", linewidth=2)
plt.plot(Q_list, kl_qp_list, label="KL(Q||P)", linewidth=2, linestyle="--")
plt.xlabel("Q[0]")
plt.ylabel("KL Divergence")
plt.title("KL Divergence Asymmetry")
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig("kl_asymmetry.png", dpi=150, bbox_inches="tight")
print("KLダイバージェンスの非対称性を kl_asymmetry.png に保存")
```

</details>

### フィッシャー情報行列の数値計算（正規分布）

<details>
<summary>コード例: appendix4_fisher_gaussian.py</summary>

```appendix4_fisher_gaussian.py
import torch


def fisher_information_gaussian(mu, sigma):
    """正規分布のフィッシャー情報行列（解析的）

    N(mu, sigma^2) のフィッシャー情報行列:
    [[1/sigma^2, 0],
     [0, 2/sigma^4]]
    """
    return torch.tensor([[1 / sigma**2, 0], [0, 2 / sigma**4]])


# 異なる分散での比較
sigmas = [0.5, 1.0, 2.0]

for sigma in sigmas:
    fisher = fisher_information_gaussian(mu=0.0, sigma=sigma)
    print(f"σ={sigma}:")
    print(f"  F_μμ = {fisher[0, 0]:.4f} (平均への感度)")
    print(f"  F_σσ = {fisher[1, 1]:.4f} (分散への感度)")
    print()

# 出力例:
# σ=0.5:
#   F_μμ = 4.0000 (平均への感度)  ← 分散が小さいほど感度が高い
#   F_σσ = 32.0000 (分散への感度)
#
# σ=1.0:
#   F_μμ = 1.0000
#   F_σσ = 2.0000
#
# σ=2.0:
#   F_μμ = 0.2500  ← 分散が大きいと感度が低い
#   F_σσ = 0.1250
```

</details>

### 自然勾配と通常勾配の比較（玩具問題）

<details>
<summary>コード例: appendix4_natural_vs_standard.py</summary>

```appendix4_natural_vs_standard.py
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F


# 簡単な2パラメータのロジスティック回帰
class SimpleLogistic(nn.Module):
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.tensor([0.0, 0.0]))

    def forward(self, x):
        return torch.sigmoid(x @ self.w)


# データ生成（線形分離可能）
torch.manual_seed(42)
X = torch.randn(100, 2)
y = (X[:, 0] + X[:, 1] > 0).float()

# 通常の勾配降下
model_sgd = SimpleLogistic()
optimizer_sgd = torch.optim.SGD(model_sgd.parameters(), lr=0.1)
losses_sgd = []

for _ in range(100):
    optimizer_sgd.zero_grad()
    pred = model_sgd(X)
    loss = F.binary_cross_entropy(pred, y)
    loss.backward()
    optimizer_sgd.step()
    losses_sgd.append(loss.item())

# 適応的前処理（Adamで近似的に）
model_adam = SimpleLogistic()
optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.1)
losses_adam = []

for _ in range(100):
    optimizer_adam.zero_grad()
    pred = model_adam(X)
    loss = F.binary_cross_entropy(pred, y)
    loss.backward()
    optimizer_adam.step()
    losses_adam.append(loss.item())

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(losses_sgd, label="SGD (Standard Gradient)", linewidth=2)
plt.plot(losses_adam, label="Adam (Adaptive Preconditioning)", linewidth=2)
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("Convergence: Standard Gradient vs Adaptive Preconditioning")
plt.legend()
plt.grid(True, alpha=0.3)
plt.yscale("log")
plt.savefig("gradient_comparison.png", dpi=150, bbox_inches="tight")
print("収束の比較を gradient_comparison.png に保存")
```

</details>

> [!NOTE]
> 上記の例では、Adamを **適応的前処理（座標ごとのスケーリング）** として扱っている。これは特定条件下で自然勾配的な効果を持つことがあるが、厳密には自然勾配法（Fisher計量に基づく最適化）とは異なる。真の自然勾配法を実装するには、K-FACなどの専用手法が必要。

## まとめ

| 概念 | 定義 | 深層学習での役割 |
| --- | --- | --- |
| **内積（dot product）** | ベクトルの大きさと方向の積 | Attention（scaled dot-product）の基礎 |
| **コサイン類似度** | 正規化後の内積（方向のみ） | 正規化埋め込み、正規化Attention（nGPTなど） |
| **双曲距離** | 負曲率空間での距離 | 階層構造の表現（第12回） |
| **KLダイバージェンス** | 分布の非対称な隔たり | 損失関数、学習の駆動 |
| **フィッシャー情報行列** | パラメータ空間の計量 | 自然勾配法、空間の感度 |
| **自然勾配法** | 曲がった空間での最適化 | 効率的な学習（理論上） |

### 第7回との統合：「点」の二重性

第7回では「点埋め込み（ $\kappa = \infty$ ）から分布埋め込み（ $\kappa$ 有限）へ」という**ズームイン**を行った。本Appendixでは、その分布を操るパラメータ空間という**ズームアウト**を行った。

両者を統合すると、以下の二重の視点が得られる：

| 視点 | 「点」とは何か | 変化とは何か | 測り方 |
| --- | --- | --- | --- |
| **第7回（表現空間）** | ベクトル $\mathbf{h}$ 。 $\kappa = \infty$ なら点、 $\kappa$ 有限なら分布（雲）。 | 星の位置や霧の濃さが変わる | コサイン類似度、測地距離 |
| **今回（パラメータ空間）** | パラメータ $\theta = (\boldsymbol{\mu}, \kappa)$ 。**表現空間での分布全体を代表する座標。** | 雲の形を決める設計図が変わる | KLダイバージェンス、フィッシャー計量 |

**重要な認識**：パラメータ空間の「点」 $\theta$ は、表現空間の「分布（雲）」そのものを指し示している。この階層構造を理解することで、「点」という言葉が文脈によって異なる実体を指すことが明確になる。

- 表現空間で「 $\kappa$ を大きくする」= パラメータ空間で「 $\theta$ を動かして分布を鋭くする」
- 表現空間で「霧が濃い」= パラメータ空間で「フィッシャー情報量が低い（砂漠）」

両者は同じ現象を、異なる視座から見ているに過ぎない。

### ゴール

**「測る」ことは「理解する」ことの第一歩である。そして「何を測るか」を知ることは、さらに根本的である。**

本Appendixを通じて、以下のような問いを自然に発することができる直感を獲得してほしい：

- 「この空間では、何を『近い』と定義しているのか」
- 「この損失関数は、どちらからどちらへの『隔たり』を測っているのか」
- 「この最適化手法は、空間のどんな性質を利用しているのか」
- 「AIモデルは、情報の地形のどこに計算資源を投じているのか」
- **「この座標 $\theta$ が代表する分布は、どんな構造を持っているのか」**

最後の問いは、情報幾何学の核心である。パラメータという「座標」の背後に、確率分布という豊かな構造が広がっている。位置という「点」の先に、情報という「形」が存在する。この多層的な視点こそ、AIの本質を理解する鍵となる。

### 講義本編との接続

本Appendixで扱った概念は、以下の回と密接に関連している：

- **第4回（Softmax/情報幾何）**：KLダイバージェンス、自然勾配法の応用
- **第6回（Attention）**：内積（scaled dot-product）による類似度計算。標準的なAttentionは内積を使用し、ベクトルの大きさも考慮する。正規化Attentionではコサイン類似度に相当。
- **第7回（不確実性の復権）**：本Appendixと最も強く結びついている。第7回が「表現空間での解像度向上（点→分布）」なら、本Appendixは「パラメータ空間での幾何学（分布を操る座標系）」。vMF分布の $\kappa$ がフィッシャー情報行列に与える影響を具体例として扱った。
- **第12回（双曲幾何）**：双曲距離による階層構造の表現
- **第13回（高次元/MoE）**：計算資源の動的配分（Fisher情報との**比喩的対応**として理解可能）

これらの回を読む際、本Appendixで得た「物差し」の視点を思い出すことで、より深い理解が得られるだろう。ただし、**AttentionやMoEの設計原理はFisher情報そのものではなく、内積幾何やルーティング機構に基づいている**点を念頭に置くこと。

### リーマンの先へ：フィンスラー計量とシンプレクティック形式

本Appendixではリーマン計量（フィッシャー情報行列）を中心に扱ったが、リーマン幾何学はあくまで「物差しの一族」の中核であって、全体ではない。ここでは、近年の深層学習研究で存在感を増しつつある2つの幾何学的構造に簡潔に触れておく。

#### フィンスラー計量：方向によって変わる物差し

リーマン計量の最大の制約は、**距離が方向に依存しない**（ $d(A, B) = d(B, A)$ ）ことである。しかし、現実の多くの系は本質的に非対称だ。

**フィンスラー計量（Finsler metric）** は、リーマン計量の一般化であり、距離の測り方が**方向によって異なる**ことを許容する。リーマン計量が各点に「円形の単位球」を置くのに対し、フィンスラー計量では「歪んだ凸体」が単位球となる。いわば「向かい風のある空間」——進む方向によって同じ距離が「遠く」も「近く」もなる計量である。

深層学習との接点は明確である：

- **言語の遷移確率の非対称性**：「猫」の次に「鳴く」が来る確率と、「鳴く」の次に「猫」が来る確率は大きく異なる。この方向依存性は、対称なリーマン計量では自然に記述できないが、フィンスラー計量であれば空間の構造そのものに組み込める。
- **順伝播と逆伝播の非対称性**：ニューラルネットワークにおける情報の流れは、順方向と逆方向で異なる構造を持つ。この非対称性をフィンスラー的な視点で捉えることで、より少ないパラメータで複雑な情報の流れをモデル化できる可能性がある。

> [!NOTE]
> **KLダイバージェンスとの関係**：本Appendixで扱ったKLダイバージェンスの非対称性（ $D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)$ ）は、まさにフィンスラー的な現象である。フィッシャー情報行列は「KLの2次近似」として対称に見えるが、3次以降の項を含めると非対称性（双対接続の構造）が現れる。フィンスラー幾何はこの非対称性を計量レベルで正面から扱う枠組みと言える。

#### シンプレクティック形式：情報量の保存則

もう一つの重要な構造は、**シンプレクティック形式（symplectic form）** である。

リーマン計量が「長さ」を測る道具だとすれば、シンプレクティック形式は「面積」を保存する道具である。古典力学において、ハミルトン系は位相空間の体積（リウヴィルの定理）を保存しながら時間発展する。シンプレクティック形式はこの保存則の数学的基盤であり、エネルギーや情報が「漏れない」変換を特徴づける。

深層学習との接点は、特に物理シミュレーションAIの領域で具体化している：

- **Hamiltonian Neural Networks（HNN）**：ニューラルネットワーク自体にシンプレクティック構造を組み込むことで、エネルギー保存則を厳密に満たす物理シミュレーションを実現する。従来の手法ではエネルギーが数値誤差で少しずつ「漏れる」問題があったが、シンプレクティック積分器を用いることでこの問題を構造的に回避できる。
- **可逆ネットワーク（Reversible Networks）**：情報を消失させずに変換するアーキテクチャ（RevNet、Neural ODEなど）は、シンプレクティック的な「保存則」の思想と通底している。

> [!TIP]
> **本Appendixとの接続**：本Appendixで扱ったリーマン計量（フィッシャー情報行列）は、「空間のどこが硬いか（感度）」を教えてくれる。フィンスラー計量は「どの方向に硬いか」を加え、シンプレクティック形式は「変換しても失われないものは何か」を教えてくれる。三者は排他的ではなく、相補的な視点を提供している。

| 幾何学的構造 | 測るもの | 一言で言えば | 深層学習での応用例 |
| --- | --- | --- | --- |
| **リーマン計量** | 長さ（感度） | 空間の硬さ | 自然勾配法、フィッシャー情報 |
| **フィンスラー計量** | 方向依存の長さ | 向かい風の計量 | 非対称な遷移・情報の流れ |
| **シンプレクティック形式** | 面積（保存量） | 情報量の保存則 | HNN、可逆ネットワーク |

これらの構造は、本講義の「動態論」編で扱う**情報の流れ**の理論的基盤ともなる。特にシンプレクティック幾何は、拡散モデルやフローマッチングにおけるエネルギー保存の議論と直結している。

### 次のステップ：動態論へ

本講義「統一視点」は、**空間の形**（どこに配置されているか）を扱った。しかし、情報幾何学のもう一つの顔は、**情報の流れ**（どう移動するか）である。

続編「動態論」では、拡散モデルやフローマッチングにおける「情報の軌道」、推論過程における「思考の連鎖」など、動的な視点を導入する。そこでは、本Appendixで学んだ「物差し」が、**時間発展する系**においてどう機能するかを見ることになる。

情報幾何学は、静止画ではなく、動画として理解されるべきである。

## 参考文献

### 情報幾何学（基礎）

- Amari, S. (2016). *Information Geometry and Its Applications*. Applied Mathematical Sciences, Vol. 194. Springer Japan. DOI: [10.1007/978-4-431-55978-8](https://doi.org/10.1007/978-4-431-55978-8)
    - 情報幾何学の標準的教科書。フィッシャー情報行列、KLダイバージェンス、双対構造などを体系的に扱う。
- Amari, S., & Nagaoka, H. (2000). *Methods of Information Geometry*. Translations of Mathematical Monographs, Vol. 191. American Mathematical Society. DOI: [10.1090/mmono/191](https://doi.org/10.1090/mmono/191)
    - 情報幾何学の古典。数学的に厳密な定式化を提供。

### 自然勾配法

- Amari, S. (1998). Natural Gradient Works Efficiently in Learning. *Neural Computation*, 10(2), 251–276. DOI: [10.1162/089976698300017746](https://doi.org/10.1162/089976698300017746)
    - 自然勾配法の一次文献。パラメータ空間の計量構造が学習効率に与える影響を示した古典。
- Martens, J., & Grosse, R. (2015). Optimizing Neural Networks with Kronecker-factored Approximate Curvature. *ICML 2015*. arXiv: [arXiv:1503.05671](https://arxiv.org/abs/1503.05671)
    - K-FAC（Kronecker-Factored Approximate Curvature）の提案。大規模モデルでの自然勾配法の近似手法。

### KLダイバージェンスと損失関数

- Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. *The Annals of Mathematical Statistics*, 22(1), 79–86. DOI: [10.1214/aoms/1177729694](https://doi.org/10.1214/aoms/1177729694)
    - KLダイバージェンスの一次文献。情報理論における基礎概念。
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. ISBN: 978-0387310732.
    - 機械学習におけるKLダイバージェンス、クロスエントロピー、最尤推定の関係を詳しく解説。

### 双曲幾何学（階層構造）

- Nickel, M., & Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. *NeurIPS 2017*. arXiv: [arXiv:1705.08039](https://arxiv.org/abs/1705.08039)
    - 双曲空間（Poincaré球モデル）での埋め込み手法。階層構造の表現に適していることを実験的に示した。
- Ganea, O., Bécigneul, G., & Hofmann, T. (2018). Hyperbolic Neural Networks. *NeurIPS 2018*. arXiv: [arXiv:1805.09112](https://arxiv.org/abs/1805.09112)
    - 双曲空間でのニューラルネットワークの一般化。

### Attention と内積

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*. arXiv: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
    - Transformer と Scaled Dot-Product Attention の一次文献。

### フィンスラー幾何学とシンプレクティック幾何学

- Bao, D., Chern, S.-S., & Shen, Z. (2000). *An Introduction to Riemann-Finsler Geometry*. Graduate Texts in Mathematics, Vol. 200. Springer. DOI: [10.1007/978-1-4612-1268-3](https://doi.org/10.1007/978-1-4612-1268-3)
    - フィンスラー幾何学の標準的教科書。リーマン計量の一般化としての定式化を体系的に扱う。
- Greydanus, S., Dzamba, M., & Yosinski, J. (2019). Hamiltonian Neural Networks. *NeurIPS 2019*. arXiv: [arXiv:1906.01563](https://arxiv.org/abs/1906.01563)
    - シンプレクティック構造をニューラルネットワークに組み込んだHNNの提案。エネルギー保存則を構造的に満たす学習手法。
- Chen, Z., Zhang, J., Arjovsky, M., & Bottou, L. (2020). Symplectic Recurrent Neural Networks. *ICLR 2020*. arXiv: [arXiv:1909.13334](https://arxiv.org/abs/1909.13334)
    - シンプレクティック積分器をリカレントネットワークに導入し、長期安定性を実現。

### MoE（Mixture of Experts）

- Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. *ICLR 2017*. arXiv: [arXiv:1701.06538](https://arxiv.org/abs/1701.06538)
    - 現代的MoEの基礎論文。スパースゲーティングと条件付き計算の設計。
- Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. *JMLR*, 23(120), 1–39. arXiv: [arXiv:2101.03961](https://arxiv.org/abs/2101.03961)
    - MoEの訓練安定化技術（Load Balancing Loss等）を扱う。
