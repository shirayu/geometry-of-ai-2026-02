# Appendix 4: 空間の「物差し」再考: 2点間から情報の密度まで

## 注意事項

本Appendixで扱う内容には、確立された数学的事実と、教育的な解釈が混在している。特に以下の点に留意されたい：

- **「距離」と「ダイバージェンス」の区別**：本資料では数学的に厳密な距離（対称・三角不等式を満たす）と、非対称な「隔たりの尺度」であるダイバージェンスを明確に区別する。KLダイバージェンスは対称性を持たないため、数学的には距離ではない。
- **「情報密度」の用語定義**：本Appendixで用いる「情報密度」は、データ点の密度やベクトル空間の分布ではなく、 **フィッシャー情報行列が表す「パラメータ変化に対する予測分布の感度」** を指す教育的比喩である。この用語は機械学習文脈で複数の意味を持ちうるため、本資料では上記の定義に限定して使用する。
- **フィッシャー情報行列の解釈**：「空間の密度」「ジャングルと砂漠」は教育的メタファーであり、厳密には「パラメータに対する予測分布の感度」を表す計量テンソルである。
- **自然勾配法の効果**：自然勾配法が「常に通常勾配より優れる」わけではない。計算コスト、近似精度、タスク特性により実用性が変わる。大規模モデルでは近似手法（K-FAC等）が必要。
- **情報密度とAttention/MoEの関係**：「Attentionが情報密度の高い場所に重みを置く」という記述は、直感的理解を助けるための**比喩的対応づけ**である。Attentionは内積幾何に基づく類似度計算、Fisherは統計モデルの計量であり、厳密には別概念である。また、因果関係は双方向的で、学習によって特定領域の情報密度（感度）が**形成される**側面もある。

## 導入：「物差し」が世界を定義する

### 幾何学とは「測る」こと

空間を理解するとは、そこで使える **「物差し（計量、metric）」** を知ることである。

小学校で習う算数では、2点間の距離は定規で測る。しかし、その「定規」自体が、どのような空間にいるかによって変わる。平らな紙の上では直線距離（ユークリッド距離）が自然だが、地球儀の上では大円に沿った測地距離が適切だ。

深層学習における表現空間も同様である。ベクトルの「近さ」を測る方法は一つではない：

| 測り方 | 何を測るか | 特性 | 深層学習での例 |
| --- | --- | --- | --- |
| **ユークリッド距離** | 2点間の直線距離 | 対称、三角不等式を満たす | Word2Vec（初期）、PCA |
| **内積（dot product）** | ベクトルの大きさと方向 | 対称 | Attention（scaled dot-product） |
| **コサイン類似度** | ベクトルの方向の近さ | 対称、長さを無視 | 正規化された埋め込み（L2正規化後の内積） |
| **KLダイバージェンス** | 分布の「隔たり」 | 非対称 | 損失関数、VAE |
| **フィッシャー情報** | 分布の変化しやすさ（感度） | 計量テンソル | 自然勾配法、パラメータ空間の幾何 |

### 「点」の正体：スカラではなく分布の代表

測定の前に、まず「何を測るのか」を明確にする必要がある。情報幾何学において、**「点」は単なる数値（スカラ）や座標ではない**。

数学的には、情報幾何学で扱う「点」は以下の3つの階層を持つ：

| 階層 | 数学的実体 | 日常的なイメージ | 情報の性質 |
| --- | --- | --- | --- |
| **レベル1：データ点** | $\mathbf{x} \in \mathbb{R}^n$ | 観測された数値の羅列 | 確定的（決定論的） |
| **レベル2：表現ベクトル** | $\mathbf{h} \in \mathbb{R}^d$ | 高次元空間内の向きと長さ | 特徴量としての埋め込み（確定的） |
| **レベル3：統計多様体の点** | $\theta$ に対応する $p(x; \theta)$ | ある確率分布 | 統計的構造全体（平均、分散、形状） |

**重要なのは第3の階層である。** 統計多様体の各点は、ある確率分布に対応し、 $\theta$ はその点を指定する **座標（パラメータ）** である。つまり：

- **一般的な直感：** 点 = 位置を表すスカラや座標
- **情報幾何学：** 点 = ある確率分布を代表し、 $\theta$ はその座標

たとえば、正規分布族 $\mathcal{N}(\mu, \sigma^2)$ は2次元の統計多様体を成す。各点（パラメータ $(\mu, \sigma^2)$ ）は一つの正規分布に対応し、その背後には**全ての可能な $x$ に対する確率密度関数という関数的な構造**が広がっている。

> [!IMPORTANT]
> **次元の注意：** 統計多様体自体は通常**有限次元**（パラメータ数に等しい）である。正規分布族は2次元、カテゴリカル（ $k$ クラス）は、単体の内部 ( $p_i>0$ , $\sum_i p_i=1$ ) では $k-1$ 次元の **滑らかな多様体** となる（境界 $p_i=0$ を含めると幾何が特異になりうる）。分布そのものは関数なので背後に豊かな構造があるが、多様体として扱うときは $\theta$ の次元数だけの有限次元空間である。

> [!TIP]
> **比喩的理解：点は「天体」**<br>
> 夜空を見上げると、星が点として見える。しかし、その一つ一つの「点」は、実際には巨大な天体であり、惑星や衛星や無数の構造を内包している。情報幾何学における「点」も同様である。パラメータ空間上の一つの「点」 $\theta$ は、その背後に**確率分布という構造**を代表している。
>
> 座標 $\theta$ という「位置」の背後に、分布という「形」が存在する。この視点転換が、情報幾何学の出発点である。
>
### AIにとっての多層的な定規

AIにとっての空間は、単なる物理的な「長さ」だけでなく、以下のような多層的な定規で構成されている：

1. **配置の定規**：どこに点を置くか（対称的な距離・内積）
2. **学習の定規**：どの方向に進むか（非対称なダイバージェンス）
3. **感度の定規**：空間のどこが「硬い」か（フィッシャー情報行列）

本Appendixでは、これら3つの視点から「物差し」を再考し、情報幾何学の核心的な概念を、数式を最小限に抑えつつ直感的に理解することを目指す。

> [!TIP]
> **読者の幾何学的直感の拡張**：物理的な「長さ」という直感から、情報的な「量」へと視野を広げることが本Appendixの目標である。距離を測ることは、単に「どれだけ離れているか」を知るだけでなく、「どう変化するか」「どこが重要か」を知ることでもある。そして何よりも、**「点」という存在が、実は巨大な構造を内包している**ことを理解することが重要だ。

## 対称的な尺度（距離）：配置と構造の固定

### 内積：方向と大きさを測る基本道具

最も基本的な「測り方」は **内積（inner product / dot product）** である。2つのベクトル $\mathbf{u}, \mathbf{v}$ の内積は：

$$\mathbf{u} \cdot \mathbf{v} = \sum_i u_i v_i = \|\mathbf{u}\| \|\mathbf{v}\| \cos\theta$$

この式は、内積が以下の2つの情報を含むことを示している：

- **大きさ**（ノルム $\|\mathbf{u}\|, \|\mathbf{v}\|$ ）
- **方向の近さ**（角度 $\theta$ のコサイン）

### コサイン類似度：方向だけを見る

両方のベクトルを正規化（単位長に）すると、内積は純粋に方向だけを測る **コサイン類似度** になる：

$$\text{cosine similarity} = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \cos\theta$$

これは、ベクトルの大きさ（ノルム）を無視し、向きだけに注目する尺度である。

### Attentionとの関係

第6回で扱ったAttention機構（標準的なscaled dot-product attention）は、QueryとKeyの **内積（dot product）** を基本としている：

$$\text{score}(q_i, k_j) = \frac{q_i^\top k_j}{\sqrt{d_k}}$$

この内積は、**ベクトルの大きさと方向の両方**を反映する。ただし、実装によっては：

- **L2正規化されたAttention**：Query/Keyを事前に正規化すれば、内積はコサイン類似度と等価になる
- **標準的なAttention**：正規化なしでは、内積はベクトルの大きさも考慮する

```appendix4_cosine_similarity.py
import torch


def dot_product(u, v):
    """内積の計算（標準的なAttentionに相当）"""
    return torch.dot(u, v)


def cosine_similarity(u, v):
    """コサイン類似度の計算（正規化後の内積）"""
    u_norm = u / u.norm()
    v_norm = v / v.norm()
    return torch.dot(u_norm, v_norm)


# 例：同じ方向だが大きさが異なるベクトル
u = torch.tensor([1.0, 2.0, 3.0])
v = torch.tensor([2.0, 4.0, 6.0])  # uの2倍

print(f"内積: {dot_product(u, v):.2f}")
print(f"コサイン類似度: {cosine_similarity(u, v):.4f}")
# 内積: 28.00  ← 大きさも反映される
# コサイン類似度: 1.0000  ← 完全に同じ方向（大きさは無視）
```

> [!NOTE]
> **第6回との接続**：標準的なAttentionは内積を使用し、Softmaxで正規化する。これは「どのKeyがQueryと高い内積を持つか」を測定し、その値に応じて情報を集約する操作である。内積はベクトルの大きさも考慮するため、純粋な「方向の類似度」とは異なる。一部のモデル（例：nGPTなど）では明示的にL2正規化を行い、コサイン類似度ベースのAttentionを実装している。

### 双曲距離：階層構造の深さを測る

ユークリッド空間や球面空間とは異なる計量として、**双曲距離（hyperbolic distance）** がある。これは負の曲率を持つ空間での距離である。

第12回で詳しく扱ったが、双曲空間の重要な特徴は：

- **中心から離れるほど空間が指数的に広がる**
- **階層構造（木構造）の埋め込みに適している**

例えば、Poincaré球モデルでは、距離は以下のように定義される：

$$d(\mathbf{u}, \mathbf{v}) = \text{arcosh}\left(1 + 2\frac{\|\mathbf{u} - \mathbf{v}\|^2}{(1-\|\mathbf{u}\|^2)(1-\|\mathbf{v}\|^2)}\right)$$

この距離は、中心（原点）の近くでは「親」を表し、境界に近いほど「深い子孫」を表現できる。

> [!NOTE]
> **実装上の注意**：Poincaré球モデルでは、すべての点が $\|\mathbf{u}\| < 1$ を満たす必要がある（開球内）。実装時には数値安定性のため、以下の対策が推奨される：
>
> 1. **境界クリッピング**：境界 $\|\mathbf{u}\| = 1$ に近づきすぎないよう、例えば $\|\mathbf{u}\| < 1 - \epsilon$ （ $\epsilon = 10^{-5}$ ）でクリップ
> 2. **arcoshの引数の保証**：浮動小数点誤差で arcosh の引数が 1 未満になり得るため、`arg = arg.clamp(min=1 + eps)` のような処理が有用
> 3. **ゼロ除算回避**：分母 $(1-\|\mathbf{u}\|^2)(1-\|\mathbf{v}\|^2)$ がゼロにならないよう、上記のクリッピングと合わせて対策する

| 空間 | 曲率 | 距離の特徴 | 適した構造 | 深層学習での例 |
| --- | --- | --- | --- | --- |
| **ユークリッド** | 0 | 一様 | 特定の構造なし | PCA、初期のWord2Vec |
| **球面** | 正 | 中心への回帰性 | 方向の多様性 | 正規化埋め込み（第3回） |
| **双曲** | 負 | 外に向かって拡張 | 階層・木構造 | Poincaré Embedding（第12回） |

> [!IMPORTANT]
> **静的な配置としての距離**：これらの距離（内積、コサイン類似度、双曲距離）はすべて **対称的** である。つまり、点Aから点Bへの距離と、点Bから点Aへの距離が等しい。これらは **「どこに配置するか」を決める静的な地図（Static Map）** のための定規である。しかし、学習（パラメータ更新）の方向性を決めるには、非対称な尺度が必要になる。

> [!NOTE]
> **Attentionと内積**：第6回で扱う標準的なAttention（scaled dot-product attention）は、内積を基本とする。内積はベクトルの大きさと方向の両方を反映するため、純粋な「方向の類似度」（コサイン類似度）とは異なる。一部のモデル（nGPTなど）では明示的にL2正規化を行い、コサイン類似度ベースのAttentionを実装している。

## 非対称な尺度（ダイバージェンス）：学習の駆動力

### 距離では学習の方向が見えない

対称的な距離は、2点間の「隔たり」を測るが、**どちらからどちらに向かうべきか**という方向性は持たない。

学習とは、モデルの分布 $Q$ を真の分布 $P$ に **近づける** プロセスである。この「近づく」という方向性を表現するには、非対称な尺度が必要だ。

### KLダイバージェンス：分布の非対称な隔たり

**KLダイバージェンス（Kullback-Leibler divergence）** は、2つの確率分布 $P$ と $Q$ の「隔たり」を測るが、行きと帰りで値が異なる：

$$D_{\text{KL}}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}$$

$$D_{\text{KL}}(Q \| P) = \sum_x Q(x) \log \frac{Q(x)}{P(x)}$$

一般に、 $D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)$ である。

### 非対称性の幾何学的意味

なぜ非対称なのか？ それは、**「どちらの視点で測るか」が異なる**からである。

- $D_{\text{KL}}(P \| Q)$ ：「真の分布 $P$ から見て、モデル $Q$ がどれだけズレているか」
    - $P$ の確率が高い場所で $Q$ の確率が低いと、大きなペナルティ
    - **Forward KL** とも呼ばれる
- $D_{\text{KL}}(Q \| P)$ ：「モデル $Q$ から見て、真の分布 $P$ がどれだけズレているか」
    - $Q$ の確率が高い場所で $P$ の確率が低いと、大きなペナルティ
    - **Reverse KL** とも呼ばれる

```appendix4_kl_asymmetry.py
import torch

# 真の分布 P と モデル Q
P = torch.tensor([0.1, 0.6, 0.3])
Q = torch.tensor([0.4, 0.3, 0.3])


# KLダイバージェンスの計算（手計算版）
def kl_divergence(p, q):
    """KL(P||Q) を計算"""
    return (p * torch.log(p / q)).sum()


kl_pq = kl_divergence(P, Q)
kl_qp = kl_divergence(Q, P)

print(f"KL(P||Q): {kl_pq:.4f}")
print(f"KL(Q||P): {kl_qp:.4f}")
print(f"非対称性: {abs(kl_pq - kl_qp):.4f}")
# 出力例:
# KL(P||Q): 0.2332
# KL(Q||P): 0.1596
# 非対称性: 0.0736
```

> [!NOTE]
> **PyTorchの実装**：PyTorchの `F.kl_div` は内部で対数を取るため、引数の順序に注意が必要。`F.kl_div(Q.log(), P, reduction='sum')` は $D_{\text{KL}}(P \| Q)$ を計算する。

### 非対称性が学習を駆動する

この非対称性こそが、**損失関数として学習（パラメータ更新）を駆動するエネルギー**となる。

例えば、クロスエントロピー損失は、KLダイバージェンスと密接に関連している：

$$\mathcal{L} _{\text{CE}} = -\sum _x P(x) \log Q(x) = D _{\text{KL}}(P \| Q) + H(P)$$

ここで $H(P)$ は $P$ のエントロピー（定数）。つまり、クロスエントロピー損失を最小化することは、Forward KL $D_{\text{KL}}(P \| Q)$ を最小化することと等価である。

| 尺度 | 対称性 | 用途 | 講義での位置 |
| --- | --- | --- | --- |
| **距離（内積・コサイン）** | 対称 | 配置の決定 | 第6回（Attention） |
| **双曲距離** | 対称 | 階層構造の表現 | 第12回 |
| **KLダイバージェンス** | 非対称 | 学習の駆動 | 第4回（Softmax） |

> [!IMPORTANT]
> **第4回との接続**：第4回で扱ったSoftmax/Cross-Entropyの幾何学的意味は、まさにこの非対称なKLダイバージェンスにある。モデルの分布を真の分布に「引き寄せる」力が、学習の勾配となる。対称的な距離では、この引力の方向が定義できない。

## フィッシャー情報行列：空間の「密度」と「感度」

### 空間は一様ではない：曲がり具合の定量化

これまで見てきた距離やダイバージェンスは、**個別の2点間**の隔たりを測るものだった。しかし、空間全体を見渡すと、**場所によって「硬さ」や「密度」が異なる**ことがある。

平らな紙の上では、どこを歩いても同じ労力で進める。しかし、山岳地帯では、急な坂を登るのは大変だが、平坦な道は楽に進める。

確率分布の空間（パラメータ空間）も同様である。パラメータを少し動かしたとき、予測分布が **激変する場所** と、**ほとんど変わらない場所** がある。

### フィッシャー情報行列：計量テンソルとしての定義

**フィッシャー情報行列（Fisher Information Matrix）** は、この「空間の曲がり具合」を定量化する：

$$G_{ij}(\theta) = \mathbb{E}_{p(x;\theta)}\left[\frac{\partial \log p(x;\theta)}{\partial \theta_i} \frac{\partial \log p(x;\theta)}{\partial \theta_j}\right]$$

これは、パラメータ空間の各点における **計量テンソル（metric tensor）** である。

> [!NOTE]
> **数学的背景**：フィッシャー情報行列は、確率分布のパラメータ空間にリーマン計量を導入する。これにより、パラメータ空間が「曲がった空間（リーマン多様体）」になる。この計量を使うと、分布の「本質的な変化量」を正しく測ることができる。

### 直感的解釈：ジャングルと砂漠

数式を離れて、直感的に理解しよう。

パラメータ空間を、起伏のある地形として想像する：

- **ジャングル（情報密度が高い）**：パラメータを少し動かすと、予測分布が大きく変わる場所。フィッシャー情報が大きい。
- **砂漠（情報密度が低い）**：パラメータを動かしても、予測分布がほとんど変わらない場所。フィッシャー情報が小さい。

この「情報密度」という概念は、**「点」が代表する分布の構造と深く関わっている**。

思い出してほしい。統計多様体の点は、単なるスカラ座標 $\theta$ ではなく、ある確率分布 $p(x; \theta)$ に対応していた。フィッシャー情報行列は、この「点が代表する分布の構造」が、パラメータの微小変化に対してどれだけ敏感に反応するかを測定している。

| 概念 | 数学的定義 | 直感的イメージ |
| --- | --- | --- |
| **点** | 分布 $p(x; \theta)$ | ある確率分布 |
| **座標** $\theta$ | パラメータベクトル | その分布を指定する方法 |
| **フィッシャー情報** $G(\theta)$ | 分布の局所的感度（計量） | 「分布がどれだけ敏感に動くか」 |

※ 厳密には、同一の分布を与える異なる $\theta$ が存在しうるため（非同定性）、「点」は分布の同値類として捉える。

> [!NOTE]
> **フィッシャー情報とKLダイバージェンスの関係：**<br>
> フィッシャー情報行列は、KLダイバージェンスの **局所2次近似** の係数として現れる。
> （向きが $D_{\mathrm{KL}}(p_\theta \\,\\|\\, p_{\theta + d\theta})$ でも $D_{\mathrm{KL}}(p_{\theta + d\theta} \\,\\|\\, p_\theta)$ でも、2次の項は同じフィッシャーになる）

つまり、フィッシャー情報行列の **（ある方向の）固有値** が大きい場所では、点が代表する分布が、パラメータの微小な揺れに対して敏感に変化する（局所的に感度が高い）。逆に、固有値が小さい方向では、パラメータが多少動いても分布はほとんど変わらない（局所的に鈍感）。

> [!CAUTION]
> **「脆い／頑健」は局所的な比喩：**<br>
> ここでの「脆い／頑健」は、あくまで **「局所的に分布がどれだけ敏感に動くか」という意味での比喩** である。学習の良し悪しや汎化性能に直結するわけではない。
>
> また、フィッシャー情報は計量としては座標に依らない（＝同じ幾何学的対象を表す）が、行列成分としての表示は座標（パラメータ化）によってテンソル則に従って変わる。だからこそ計量テンソルとして扱い、座標によらない幾何学的性質を抽出できる。
>
> **裏を返せば、通常の勾配降下法は「座標（パラメータの取り方）」に依存してしまう。本来、学習の軌道はパラメータの定義の仕方に左右されるべきではない。この「座標不変性」を満たすために、フィッシャー情報行列を用いた補正（自然勾配法）が必要となるのである。**
> さらに、ニューラルネットワークのような過剰パラメータモデルでは、フィッシャー情報行列が退化（ランク落ち）することがあり、その場合は逆行列が定義できない。しかし、自然勾配法の実装においては、行列の対角要素に微小な値を加えるダンピング（Damping）や、ムーア・ペンローズの擬似逆行列を用いることで、数値的な安定性を確保するのが一般的だ。退化していると、厳密には「リーマン計量（正定値）」というより半正定値の計量（擬計量）として扱うことになる。また、KLの局所2次近似としてフィッシャーが現れるには、微分と積分の交換などの正則性条件が暗に仮定されている。

> [!TIP]
> **点の「重み」と「密度」の違い**<br>
>
> - **点の重み（スカラ的な量）**：データ点の個数や、ベクトルのノルム（大きさ）
> - **点の密度（分布の局所感度）**：確率分布の集中度や、パラメータ変化への感度
>
> フィッシャー情報は後者を測る。座標 $\theta$ という「位置」の背後にある分布が、どれだけ「変化しやすい」かを定量化する。

```txt
パラメータ空間の地形（概念図）:

    ジャングル（急峻）    砂漠（平坦）
    ⛰️  ⛰️  ⛰️          🏜️ 🏜️ 🏜️
    情報密度: 高         情報密度: 低
    少しの移動で分布が激変   大きく移動しても分布はほぼ不変
```

この「地形」を知ることは、効率的な学習にとって本質的に重要である。

### 具体例：正規分布のパラメータ

正規分布 $N(\mu, \sigma^2)$ を考えよう。フィッシャー情報行列は：

$$G = \begin{pmatrix} 1/\sigma^2 & 0 \\ 0 & 2/\sigma^4 \end{pmatrix}$$

これは何を意味するか？

- **平均 $\mu$ の感度**： $1/\sigma^2$ に比例。分散が小さいほど、平均の変化に敏感。
- **分散 $\sigma^2$ の感度**： $2/\sigma^4$ に比例。分散が小さいほど、さらに敏感。

つまり、「鋭い分布（小さい $\sigma$ ）」の方が、パラメータの変化に対して敏感である。

> [!TIP]
> **物理的アナロジー**：フィッシャー情報は、物理学における「有効質量」に似ている。重い物体を動かすには大きな力が必要だが、軽い物体は小さな力で動く。情報密度の高い場所では、分布を動かすのに「大きな勾配」が必要になる。

## 自然勾配法：曲がった空間の歩き方

### 通常の勾配法の問題：空間の歪みを無視

通常の勾配降下法は、パラメータ空間が **平坦（ユークリッド空間）** だと仮定して進む：

$$\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}$$

しかし、実際のパラメータ空間は曲がっている。特に、情報密度の高い場所（ジャングル）では、同じ大きさの勾配でも、実際の分布の変化量が異なる。

これは、地図（平面）と実際の地形（曲面）を混同するようなものだ。地図上で1cmの移動が、平地では100mに対応するが、山岳地帯では50mにしか対応しないかもしれない。

### 自然勾配：空間の歪みを補正する

**自然勾配法（Natural Gradient Descent）** は、フィッシャー情報行列を使って勾配を補正する：

$$\tilde{\nabla} _\theta \mathcal{L} = G(\theta)^{-1} \nabla _\theta \mathcal{L}$$

$$\theta_{t+1} = \theta_t - \eta \tilde{\nabla}_\theta \mathcal{L}$$

この補正により、**分布空間での「等しい変化量」を実現する**最短経路を進むことができる。

| 手法 | 使う勾配 | 空間の扱い | 利点 | 欠点 |
| --- | --- | --- | --- | --- |
| **通常勾配** | $\nabla_\theta \mathcal{L}$ | 平坦と仮定 | 計算が軽い | 密度の高い場所で停滞 |
| **自然勾配** | $G^{-1} \nabla_\theta \mathcal{L}$ | 曲がりを考慮 | 収束が速い | $G^{-1}$ の計算が重い |

### 実装上の課題と近似

フィッシャー情報行列の逆行列 $G^{-1}$ を計算するのは、大規模モデルでは計算量的に困難である（パラメータ数が $n$ のとき、 $O(n^2)$ のメモリと $O(n^3)$ の計算）。

そのため、実用的には以下のような近似手法が使われる：

- **K-FAC (Kronecker-Factored Approximate Curvature)**：フィッシャー情報行列をKronecker積で近似
- **対角近似**：非対角要素を無視し、対角成分のみを使う簡易版
- **Adam等の適応的オプティマイザ**：勾配の2次モーメントを座標ごとにスケーリングする「前処理」として、特定条件下で自然勾配的な振る舞いをすることがある

> [!NOTE]
> **Adamと自然勾配の関係**：Adamは勾配を座標ごとに再スケールする前処理であり、これが特定の条件（対角Fisher近似、定常分布など）下で自然勾配に類似した効果を持つ場合がある。しかし、Adamが「自然勾配の近似」と言い切るのは過剰主張である。両者は設計思想が異なり、Adam は Fisher 計量を明示的に計算していない。

```appendix4_natural_gradient_concept.py
import torch

# 【重要】以下は「自然勾配法の形式」を示す教育的疑似コードであり、
# Fisher情報行列を単位行列で代用しているため、自然勾配の性質を何も示さない。
# 実際には単なる学習率の再スケール（(1+damping)倍）に過ぎない。


def natural_gradient_step(model, loss, lr=0.01, damping=1e-4):
    """自然勾配法の"形式"を示す疑似コード（実用不可）

    警告: 本実装はFisher情報を単位行列で代用しており、
    自然勾配の本質（モデル依存の計量）が完全に失われている。
    fisher_approx = (1+damping)*I なので、これは単なるスカラー倍であり、
    自然勾配が提供する「空間の歪みの補正」は一切行われない。

    真の実装には、モデルの出力分布からFisherを計算する必要がある。
    """
    # 通常の勾配
    loss.backward()

    # パラメータと勾配を1次元ベクトルに
    params = torch.cat([p.flatten() for p in model.parameters()])
    grads = torch.cat([p.grad.flatten() for p in model.parameters()])

    # 【問題点】Fisherを単位行列で代用 → これでは自然勾配にならない
    # fisher_approx = I + damping*I = (1+damping)*I
    # つまり、恒等変換のスカラー倍 = 学習率を (1+damping) 倍するだけ
    # 実際には、出力のlog probabilityの勾配の外積の期待値を計算する必要がある
    fisher_approx = torch.eye(len(params)) + damping * torch.eye(len(params))

    # "自然勾配のような形" = Fisher^{-1} @ grad
    # （ただしFisher = (1+damping)*I なので、実質 grad / (1+damping)）
    natural_grad = torch.linalg.solve(fisher_approx, grads)

    # パラメータ更新
    idx = 0
    for p in model.parameters():
        p_len = p.numel()
        p.data -= lr * natural_grad[idx : idx + p_len].view_as(p)
        idx += p_len
        p.grad.zero_()


# 【推奨】実用にはK-FAC等の専用ライブラリ、または対角Fisher近似を用いるべき
```

> [!CAUTION]
> 上記は**自然勾配法そのものではなく、その"形式"を示す教育的疑似コード**である。Fisher情報行列を $(1+\text{damping}) \cdot I$ で代用しているため、これは単なる学習率の再スケールに等しく、自然勾配が提供する「空間の歪みに応じた方向補正」は一切行われない。真の自然勾配法を実装するには、K-FAC（Martens & Grosse, 2015）などの専用手法、または少なくとも対角Fisher近似（モデル出力の分布から各パラメータのFisher対角成分を計算）が必要である。

> [!NOTE]
> **第4回との接続**：第4回で自然勾配法に触れた際、それがSoftmaxやクロスエントロピーの幾何学的意味と結びついていることを示唆した。本Appendixでは、その理論的背景であるフィッシャー情報行列を明示的に導入し、自然勾配法を **空間の曲がりを補正する最適化手法** として位置づけた。

## 結論：AttentionとMoEへの接続

### 情報密度の地図を測量するAI（比喩的理解）

これまで見てきた「物差し」の概念を、具体的なモデルの挙動と**比喩的に対応づけて**理解してみよう。

**Attentionが特定のトークンに重みを置くこと**、**MoEが特定のExpertを選ぶこと** は、その領域が **「重要」または「感度が高い」** と判断していることと、直感的に対応づけられる。

- **Attention**：QueryとKeyの内積（scaled dot-product）が高い = その方向が「重要」
- **MoE**：（典型的には）入力特徴に対する線形スコア（内積に相当）が高いExpertを選択 = その部分空間が「重要」

ただし、**厳密には**：

- **Attentionは内積幾何に基づく類似度計算**（表現空間上の操作）
- **Fisher情報は統計モデルの計量**（確率分布のパラメータ空間上の操作）
- **対象空間が異なる**：「重要度」は表現（埋め込み）上の概念、Fisherの「感度」は確率モデル（出力分布）上の概念

ここでの「情報密度」は、教育的な比喩として捉えるべきである。

どちらも、**高次元空間のどの方向・部分空間に計算資源を割り当てるか**を、動的に決定している点で共通している。

### 学習による情報の地形の形成（双方向的な関係）

さらに重要なのは、この「情報の地形」は **学習によって形成される** という点である。因果関係は一方向ではなく、相互作用的である。

初期化直後のモデルは、情報密度（Fisher情報）がほぼ一様な「平らな砂漠」のような空間かもしれない。しかし、学習が進むにつれて：

1. **勾配（KLダイバージェンス）** が、モデルを真の分布に引き寄せる
2. **特定の方向・領域の重要性が高まる**（データや損失関数に応じて）
3. **結果として、Fisher情報が大きくなる方向が現れる**（ジャングルが形成される）
4. **Attention/MoEは、学習された表現空間で「類似度が高い」方向に重みを置く**

つまり、「Attentionが情報密度の高い場所に重みを置く」と「学習によって特定方向の感度が高まる」は、**相互に影響し合う双方向の関係**にある。一方が他方を決定するのではなく、学習プロセス全体で共進化する。

```txt
学習による情報地形の変化（概念図）:

初期状態（平坦）:
🏜️ 🏜️ 🏜️ 🏜️ 🏜️ 🏜️
すべての方向が等価

     ↓ 学習

学習後（起伏あり）:
⛰️  ⛰️  🏜️ ⛰️  🏜️ 🏜️
重要な方向に情報密度が集中
```

### 3つの物差しの統合

本Appendixで扱った3つの「物差し」を統合すると：

| 物差し | 対称性 | 測るもの | AIモデルでの役割 | 備考 |
| --- | --- | --- | --- | --- |
| **内積** | 対称 | ベクトルの大きさと方向 | Attention（scaled dot-product） | 内積幾何に基づく |
| **コサイン類似度** | 対称 | ベクトルの方向（正規化後） | 正規化埋め込み、正規化Attention | L2正規化後の内積 |
| **KLダイバージェンス** | 非対称 | 分布の隔たりと方向 | 学習の駆動力（損失関数） | 統計的距離（非対称） |
| **フィッシャー情報** | 計量テンソル | 空間の感度・密度 | 最適化の効率化（自然勾配） | Attention/MoEとは**比喩的対応** |

これらは独立した概念ではなく、**情報幾何学**という統一的な枠組みの中で、異なる側面を照らし出している。

> [!IMPORTANT]
> **比喩と厳密性の区別**：本Appendixでは「AttentionやMoEが情報密度の高い場所に計算資源を投じる」という表現を用いたが、これは**直感的理解を助けるための比喩的対応づけ**である。厳密には、Attentionは標準的にはQuery-Keyの内積（scaled dot-product）、MoEは（典型的には）入力とゲートベクトルの内積に基づいており、Fisher情報（統計モデルのパラメータ空間の計量）とは別物である。両者の関係は、「重要度・感度が高い方向への資源配分」という抽象レベルでの類似性であり、数学的同一性ではない。

## 実装ノート：情報の地形を可視化する

### KLダイバージェンスの非対称性の確認

<details>
<summary>コード例: appendix4_kl_asymmetry_viz.py</summary>

```appendix4_kl_asymmetry_viz.py
import matplotlib.pyplot as plt
import numpy as np
import torch


def kl_divergence(p, q):
    """KL(P||Q) を計算（0除算回避付き）"""
    epsilon = 1e-10
    return (p * torch.log((p + epsilon) / (q + epsilon))).sum()


# 2つの分布を定義
P = torch.tensor([0.7, 0.2, 0.1])
Q_list = []
kl_pq_list = []
kl_qp_list = []

# Qの最初の要素を変化させる
for q0 in np.linspace(0.1, 0.9, 50):
    Q = torch.tensor([q0, (1 - q0) * 0.6, (1 - q0) * 0.4])
    Q_list.append(q0)
    kl_pq_list.append(kl_divergence(P, Q).item())
    kl_qp_list.append(kl_divergence(Q, P).item())

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(Q_list, kl_pq_list, label="KL(P||Q)", linewidth=2)
plt.plot(Q_list, kl_qp_list, label="KL(Q||P)", linewidth=2, linestyle="--")
plt.xlabel("Q[0]")
plt.ylabel("KL Divergence")
plt.title("KL Divergence Asymmetry")
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig("kl_asymmetry.png", dpi=150, bbox_inches="tight")
print("KLダイバージェンスの非対称性を kl_asymmetry.png に保存")
```

</details>

### フィッシャー情報行列の数値計算（正規分布）

<details>
<summary>コード例: appendix4_fisher_gaussian.py</summary>

```appendix4_fisher_gaussian.py
import torch


def fisher_information_gaussian(mu, sigma):
    """正規分布のフィッシャー情報行列（解析的）

    N(mu, sigma^2) のフィッシャー情報行列:
    [[1/sigma^2, 0],
     [0, 2/sigma^4]]
    """
    return torch.tensor([[1 / sigma**2, 0], [0, 2 / sigma**4]])


# 異なる分散での比較
sigmas = [0.5, 1.0, 2.0]

for sigma in sigmas:
    fisher = fisher_information_gaussian(mu=0.0, sigma=sigma)
    print(f"σ={sigma}:")
    print(f"  F_μμ = {fisher[0, 0]:.4f} (平均への感度)")
    print(f"  F_σσ = {fisher[1, 1]:.4f} (分散への感度)")
    print()

# 出力例:
# σ=0.5:
#   F_μμ = 4.0000 (平均への感度)  ← 分散が小さいほど感度が高い
#   F_σσ = 32.0000 (分散への感度)
#
# σ=1.0:
#   F_μμ = 1.0000
#   F_σσ = 2.0000
#
# σ=2.0:
#   F_μμ = 0.2500  ← 分散が大きいと感度が低い
#   F_σσ = 0.1250
```

</details>

### 自然勾配と通常勾配の比較（玩具問題）

<details>
<summary>コード例: appendix4_natural_vs_standard.py</summary>

```appendix4_natural_vs_standard.py
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F


# 簡単な2パラメータのロジスティック回帰
class SimpleLogistic(nn.Module):
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.tensor([0.0, 0.0]))

    def forward(self, x):
        return torch.sigmoid(x @ self.w)


# データ生成（線形分離可能）
torch.manual_seed(42)
X = torch.randn(100, 2)
y = (X[:, 0] + X[:, 1] > 0).float()

# 通常の勾配降下
model_sgd = SimpleLogistic()
optimizer_sgd = torch.optim.SGD(model_sgd.parameters(), lr=0.1)
losses_sgd = []

for _ in range(100):
    optimizer_sgd.zero_grad()
    pred = model_sgd(X)
    loss = F.binary_cross_entropy(pred, y)
    loss.backward()
    optimizer_sgd.step()
    losses_sgd.append(loss.item())

# 適応的前処理（Adamで近似的に）
model_adam = SimpleLogistic()
optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.1)
losses_adam = []

for _ in range(100):
    optimizer_adam.zero_grad()
    pred = model_adam(X)
    loss = F.binary_cross_entropy(pred, y)
    loss.backward()
    optimizer_adam.step()
    losses_adam.append(loss.item())

# 可視化
plt.figure(figsize=(10, 6))
plt.plot(losses_sgd, label="SGD (Standard Gradient)", linewidth=2)
plt.plot(losses_adam, label="Adam (Adaptive Preconditioning)", linewidth=2)
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("Convergence: Standard Gradient vs Adaptive Preconditioning")
plt.legend()
plt.grid(True, alpha=0.3)
plt.yscale("log")
plt.savefig("gradient_comparison.png", dpi=150, bbox_inches="tight")
print("収束の比較を gradient_comparison.png に保存")
```

</details>

> [!NOTE]
> 上記の例では、Adamを **適応的前処理（座標ごとのスケーリング）** として扱っている。これは特定条件下で自然勾配的な効果を持つことがあるが、厳密には自然勾配法（Fisher計量に基づく最適化）とは異なる。真の自然勾配法を実装するには、K-FACなどの専用手法が必要。

## まとめ

| 概念 | 定義 | 深層学習での役割 |
| --- | --- | --- |
| **内積（dot product）** | ベクトルの大きさと方向の積 | Attention（scaled dot-product）の基礎 |
| **コサイン類似度** | 正規化後の内積（方向のみ） | 正規化埋め込み、正規化Attention（nGPTなど） |
| **双曲距離** | 負曲率空間での距離 | 階層構造の表現（第12回） |
| **KLダイバージェンス** | 分布の非対称な隔たり | 損失関数、学習の駆動 |
| **フィッシャー情報行列** | パラメータ空間の計量 | 自然勾配法、空間の感度 |
| **自然勾配法** | 曲がった空間での最適化 | 効率的な学習（理論上） |

### ゴール

**「測る」ことは「理解する」ことの第一歩である。そして「何を測るか」を知ることは、さらに根本的である。**

本Appendixを通じて、以下のような問いを自然に発することができる直感を獲得してほしい：

- 「この空間では、何を『近い』と定義しているのか」
- 「この損失関数は、どちらからどちらへの『隔たり』を測っているのか」
- 「この最適化手法は、空間のどんな性質を利用しているのか」
- 「AIモデルは、情報の地形のどこに計算資源を投じているのか」
- **「この座標 $\theta$ が代表する分布は、どんな構造を持っているのか」**

最後の問いは、情報幾何学の核心である。パラメータという「座標」の背後に、確率分布という豊かな構造が広がっている。位置という「点」の先に、情報という「形」が存在する。この多層的な視点こそ、AIの本質を理解する鍵となる。

### 講義本編との接続

本Appendixで扱った概念は、以下の回と密接に関連している：

- **第4回（Softmax/情報幾何）**：KLダイバージェンス、自然勾配法の応用
- **第6回（Attention）**：内積（scaled dot-product）による類似度計算。標準的なAttentionは内積を使用し、ベクトルの大きさも考慮する。正規化Attentionではコサイン類似度に相当。
- **第12回（双曲幾何）**：双曲距離による階層構造の表現
- **第13回（高次元/MoE）**：計算資源の動的配分（Fisher情報との**比喩的対応**として理解可能）

これらの回を読む際、本Appendixで得た「物差し」の視点を思い出すことで、より深い理解が得られるだろう。ただし、**AttentionやMoEの設計原理はFisher情報そのものではなく、内積幾何やルーティング機構に基づいている**点を念頭に置くこと。

### 次のステップ：動態論へ

本講義「統一視点」は、**空間の形**（どこに配置されているか）を扱った。しかし、情報幾何学のもう一つの顔は、**情報の流れ**（どう移動するか）である。

続編「動態論」では、拡散モデルやフローマッチングにおける「情報の軌道」、推論過程における「思考の連鎖」など、動的な視点を導入する。そこでは、本Appendixで学んだ「物差し」が、**時間発展する系**においてどう機能するかを見ることになる。

情報幾何学は、静止画ではなく、動画として理解されるべきである。

## 参考文献

### 情報幾何学（基礎）

- Amari, S. (2016). *Information Geometry and Its Applications*. Applied Mathematical Sciences, Vol. 194. Springer Japan. DOI: [10.1007/978-4-431-55978-8](https://doi.org/10.1007/978-4-431-55978-8)
    - 情報幾何学の標準的教科書。フィッシャー情報行列、KLダイバージェンス、双対構造などを体系的に扱う。
- Amari, S., & Nagaoka, H. (2000). *Methods of Information Geometry*. Translations of Mathematical Monographs, Vol. 191. American Mathematical Society. DOI: [10.1090/mmono/191](https://doi.org/10.1090/mmono/191)
    - 情報幾何学の古典。数学的に厳密な定式化を提供。

### 自然勾配法

- Amari, S. (1998). Natural Gradient Works Efficiently in Learning. *Neural Computation*, 10(2), 251–276. DOI: [10.1162/089976698300017746](https://doi.org/10.1162/089976698300017746)
    - 自然勾配法の一次文献。パラメータ空間の計量構造が学習効率に与える影響を示した古典。
- Martens, J., & Grosse, R. (2015). Optimizing Neural Networks with Kronecker-factored Approximate Curvature. *ICML 2015*. arXiv: [arXiv:1503.05671](https://arxiv.org/abs/1503.05671)
    - K-FAC（Kronecker-Factored Approximate Curvature）の提案。大規模モデルでの自然勾配法の近似手法。

### KLダイバージェンスと損失関数

- Kullback, S., & Leibler, R. A. (1951). On Information and Sufficiency. *The Annals of Mathematical Statistics*, 22(1), 79–86. DOI: [10.1214/aoms/1177729694](https://doi.org/10.1214/aoms/1177729694)
    - KLダイバージェンスの一次文献。情報理論における基礎概念。
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. ISBN: 978-0387310732.
    - 機械学習におけるKLダイバージェンス、クロスエントロピー、最尤推定の関係を詳しく解説。

### 双曲幾何学（階層構造）

- Nickel, M., & Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. *NeurIPS 2017*. arXiv: [arXiv:1705.08039](https://arxiv.org/abs/1705.08039)
    - 双曲空間（Poincaré球モデル）での埋め込み手法。階層構造の表現に適していることを実験的に示した。
- Ganea, O., Bécigneul, G., & Hofmann, T. (2018). Hyperbolic Neural Networks. *NeurIPS 2018*. arXiv: [arXiv:1805.09112](https://arxiv.org/abs/1805.09112)
    - 双曲空間でのニューラルネットワークの一般化。

### Attention と内積

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. *NeurIPS 2017*. arXiv: [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
    - Transformer と Scaled Dot-Product Attention の一次文献。

### MoE（Mixture of Experts）

- Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. *ICLR 2017*. arXiv: [arXiv:1701.06538](https://arxiv.org/abs/1701.06538)
    - 現代的MoEの基礎論文。スパースゲーティングと条件付き計算の設計。
- Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. *JMLR*, 23(120), 1–39. arXiv: [arXiv:2101.03961](https://arxiv.org/abs/2101.03961)
    - MoEの訓練安定化技術（Load Balancing Loss等）を扱う。
