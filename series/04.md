# 第4回：分類の再統一 I ～Softmaxと情報幾何学～

## 注意事項

- 「ロジット＝角度」の解釈は、重みベクトルと入力ベクトルの両方が $L_2$ 正規化されている場合に成り立つ。標準的なSoftmax分類器では、ノルムの影響も混在する。
- 情報幾何学との接続（Fisher情報＝リーマン計量）は、正則性条件（パラメータに関する微分可能性、台の一致など）の下で成り立つ標準的な対応である。本回では正則性を仮定して議論を進める。
- 自然勾配とAdamの関係は「解釈の一つ」であり、確定的な理論的基礎ではない。

## 導入：連続と離散の界面

深層学習の多くのタスクは、最終的に**離散的な決定**を要求する。画像分類なら「猫か、犬か」、言語モデルなら「次のトークンは何か」。これらは**カテゴリカル変数**の予測問題であり、出力は本質的に離散的である。

しかし第3回で見たように、意味の表現空間は**連続的**である。単位球面上の点として埋め込まれた単語やクラスは、無限の方向を持ちうる。この「連続的な意味」を「離散的な記号」に変換するには、何らかの**翻訳装置**が必要になる。

**分類とは、連続空間の離散化である**。

この翻訳は単純な閾値処理ではない。なぜなら、学習には**微分可能性**が必要だからだ。勾配降下法は連続な最適化アルゴリズムであり、離散的な出力を直接扱えない。したがって、離散(頂点)の「手前」に、連続的な確率分布という中間層を置く必要がある。

本回では、この連続→離散の翻訳を担う変換プロセスを、3つの視点から解剖する:

1. **幾何学的視点**: 確率分布の空間(シンプレックス)への写像として
2. **統計学的視点**: 最大エントロピー原理による必然性として
3. **情報幾何学的視点**: 確率空間の計量構造と最適化として

## Softmax：確率単体への幾何学的写像

第3回で構築した単位球面上の表現から、最終的な離散選択に至るまでには、いくつかの段階がある。Softmaxはその中核を担う関数である。

### シンプレックス：確率分布の空間

Softmax関数の出力空間を理解するために、まず **確率単体（probability simplex）** を定義しよう。

$K$ 個のカテゴリがある分類問題を考える。各カテゴリへの確率を $p_1, p_2, \ldots, p_K$ とすると、これらは以下の制約を満たす：

$$\sum_{i=1}^{K} p_i = 1, \quad p_i \geq 0$$

この制約を満たす点の集合が、 $(K-1)$ 次元の確率単体 $\Delta^{K-1}$ である。

| カテゴリ数 $K$ | 単体の次元 | 幾何学的形状 |
| --- | --- | --- |
| 2 | 1次元 | 線分（両端が $[1,0]$ と $[0,1]$ ） |
| 3 | 2次元 | 三角形 |
| 4 | 3次元 | 四面体 |
| $K$ | $(K-1)$ 次元 | $(K-1)$ -単体 |

確率単体は凸集合であり、その頂点はone-hotベクトル（ひとつだけ1、他は0）に対応する。単体の内部点は「確率分布」を、頂点は「確定的な選択」を表す。

この空間の幾何学を理解することが、分類問題を理解する第一歩である。単体の**頂点**はone-hotベクトル、すなわち「確定的な選択(離散)」を表す。一方、単体の**内部**は、複数のカテゴリに確率が分散した状態、すなわち「迷い(連続)」を表す。分類とは、この単体の内部から頂点へと移動するプロセスとして理解できる。

<img width="65%" src="04_fig_simplex.svg" alt="確率単体 Δ²（K=3）">

図： $K=3$ の確率単体 $\Delta^2$ 。赤い実線が境界（いずれかの $p_i = 0$ ）、青い破線の内側が開内部 $\text{int}(\Delta^2)$ （滑らかな多様体）。Softmaxの出力は常に内部に留まり、境界には到達しない。

### Softmaxの定義と性質

Softmax関数は、任意の実数ベクトル $\mathbf{z} \in \mathbb{R}^K$ （**ロジット**、logit）を確率単体上の点に変換する：

$$\text{softmax}(\mathbf{z})_i = \frac{\exp(z_i / \tau)}{\sum _{j=1}^{K} \exp(z_j / \tau)}$$

ここで $\tau > 0$ は温度パラメータ（後述）である。

> [!CAUTION]
> 数式から明らかだが、 **どの次元の値も、決して0にも1にもならない** ことに注意。
> 深層学習において極めて本質的に重要である。
> (コンピュータでは数値表現に限界があるため、0や1に丸められうるが、数理上は0にも1にもならない）

この関数を幾何学的に理解すると、Softmaxは「 $\mathbb{R}^K$ から確率単体の**内部（interior）** $\text{int}(\Delta^{K-1})$ への滑らかな写像」として捉えられる。入力のロジット空間は $K$ 次元のユークリッド空間であり、出力は $(K-1)$ 次元の確率単体の内部である。次元が1つ減るのは、確率の和が1という制約のためである。

<img width="70%" src="04_fig_softmax_map.svg" alt="Softmax写像">

図：Softmax写像の全体像。ロジット空間 $\mathbb{R}^K$ の各点が、確率空間である単体の内部 $\text{int}(\Delta^{K-1})$ に写される様子。オレンジの点は具体的なサンプル（ $z_1, z_2, z_3$ ）の対応関係を示す。特に、点 $z_2 = \mathbf{1} = (1, 1, 1)$ を通り $\mathbf{1}$ 方向に伸びる直線上にある点（紫色のドット $z_2 + c_1\mathbf{1}, z_2 + c_2\mathbf{1}$ ）は、Softmaxの平行移動不変性により、すべて単体上の同一の分布（一様分布）へと写される。Softmaxの出力は常に各要素が正（ $p_i > 0$ ）となるため、像は境界（赤）に到達せず、常にその内側に留まるのが特徴である

> [!NOTE]
> **指数関数の役割：** Softmaxが指数関数を使う理由は複数ある。(1) 負のロジットも正の確率に変換できる、(2) 微分可能で滑らか、(3) 後述する最大エントロピー原理との整合性。ただし、指数関数は数値的にオーバーフローしやすいため、実装では最大値を引く安定化が必要（実装ノート参照）。

> [!NOTE]
> **ロジット（logit）とは：** ロジット $z_i$ とは、確率 $p_i$ に変換される前の「生のスコア」であり、任意の実数値を取りうる。名前の由来は、2クラス分類における**ロジスティック関数**（シグモイド関数） $p = \sigma(z) = \frac{1}{1 + e^{-z}}$ の逆関数 $z = \sigma^{-1}(p) = \log\frac{p}{1-p}$ が「**log**-odds（対数オッズ）」と呼ばれることから、その短縮形として**logit**と命名された。
>
> ただし、多クラス分類における「ロジット」は、厳密には2クラスの対数オッズとは異なる。2クラスの場合、ロジット $z$ と確率 $p$ の間には $z = \log(p / (1-p))$ という一意な関係がある。一方、Softmaxでは、Softmaxの平行移動不変性により、ロジット $\mathbf{z}$ と確率 $\mathbf{p}$ の間には $z_i = \log p_i + C$ （ $C$ は任意定数）という関係しか成り立たない。つまり、**多クラスのロジットは対数確率に定数を加えたもの**であり、対数オッズそのものではない。にもかかわらず「ロジット」と呼ばれるのは、2クラスの場合の自然な拡張として、また確率の制約から解放された「生のスコア」という共通の役割を担うためである。

> [!TIP]
> **なぜ確率を直接扱わないのか：**<br>
> ニューラルネットワークで確率 $\mathbf{p} \in \Delta^{K-1}$ を直接出力しようとすると、いくつかの問題が生じる。<br>
> **(1) 制約の複雑さ**： $\sum_i p_i = 1, p_i \geq 0$ という制約を満たしながら勾配降下法を適用するのは面倒である。制約付き最適化や射影演算が必要になる。<br>
> **(2) 線形変換との不整合**：ネットワークの最終層は通常 $\mathbf{z} = W\mathbf{x} + \mathbf{b}$ という線形変換であるが、この出力は $\mathbb{R}^K$ 全体を取りうる。線形変換の出力が自動的に単体の制約 $\sum_i p_i = 1, p_i \geq 0$ を満たす保証はないため、線形変換の出力を直接確率として解釈できない。<br>
> **(3) 数値的な扱いづらさ**：確率が0に近いとき、勾配が不安定になりやすい。一方、ロジット空間 $\mathbb{R}^K$ は制約のない全空間であり、線形変換の出力をそのまま受け取れる。<br>
> Softmaxは、この「自由な実数空間」から「制約された確率空間」への滑らかな橋渡しを担う。つまり、**ネットワークは自由に予測し、Softmaxが事後的に整合性を保証する**という分業が、実装と学習の両面で効率的なのである。

> [!NOTE]
> **単体の「内部」と多様体構造：** 確率単体 $\Delta^{K-1}$ は、境界（いずれかの $p_i = 0$ ）を含む凸集合であり、境界の「角（かど）」や「辺」があるため、それ自体は滑らかな多様体ではない。しかし、Softmaxの出力は指数関数の性質上 $p_i > 0$ を常に満たし、境界に到達しない。したがって、Softmaxの像は単体の**開内部** $\text{int}(\Delta^{K-1}) = \{ \mathbf{p} \in \Delta^{K-1} \mid p_i > 0 \; \forall i \}$ に限られる。この開内部は境界を持たない $(K-1)$ 次元の滑らかな多様体であり、情報幾何学で扱う統計多様体として自然に機能する。本資料で確率単体を「多様体」と呼ぶとき、特に断りがない限り、この開内部を指す。

> [!TIP]
> **コラム：歴史的視点——「境界の呪い」とSoftmaxの功績**
>
> 上述の「Softmaxは決して0にならない（常に単体の内部に留まる）」という性質は、自然言語処理の歴史において、実は「計算の破綻」を防ぐための決定的な役割を果たしている。
>
> **1. 境界の呪い（無限大の損失）**
> かつてのn-gramモデル等では、学習データに無い単語の確率は $0$ になる。これは幾何学的に、分布が単体の「境界」に張り付く状態だ。もしモデルが正解データに対して確率 $0$ を出力してしまうと、負の対数尤度（クロスエントロピー）は $-\log 0 = \infty$ となり、学習が破綻する。
> （KLダイバージェンス $D_{KL}(P_{\text{真}} || P_{\text{予測}})$ の定義においても、 $P_{\text{予測}}(x)=0$ かつ $P_{\text{真}}(x)>0$ となる点が一箇所でもあると、対数項 $\log (P_{\text{真}}/P_{\text{予測}})$ が無限大に発散してしまうためだ。）
>
> **2. 構造による解決**
> 先人たちはこの「確率0」を回避するため、Laplace平滑化（カウントに $+1$ ）やKneser-Ney平滑化など、手動で分布を境界から引き剥がす処理に苦心した。
> 対してSoftmaxは、ロジットが有限である限り $\exp(z) > 0$ を保証し、**構造的に「確率0による計算破綻」を回避する**。
>
> 現代でもデータ疎性の問題自体は残るため（Label Smoothing等の技術が使われるのはそのためだ）、統計的な課題が消えたわけではない。しかし、Softmaxのおかげで、少なくとも「たった一箇所の確率0で損失が無限大になり、学習全体が破綻する」という悪夢からは解放され、安心してエンドツーエンド学習が行えるようになったのである。

### 球面からシンプレックスへ：第3回との接続

第3回で、私たちは表現空間を単位球面 $S^{d-1}$ に制約する設計を見た。ここでは、その球面上の表現が、どのようにして確率単体へと変換されるかを見ていこう。これは前回の連続表現と今回の離散化を繋ぐ橋である。

nGPTのような設計では、表現空間を単位球面 $S^{d-1}$ に制約する。分類を行う際、入力の埋め込み $\mathbf{x}$ と各クラスの代表ベクトル $\mathbf{w}_k$ の内積をロジットとして計算する：

$$z_k = \mathbf{w}_k^\top \mathbf{x}$$

**両者がともに単位ノルムに正規化されている場合**、この内積はコサイン類似度に等しく、 $z_k = \cos\theta_k$ となる。すなわち、ロジットは角度情報を直接反映する。

この変換の流れを整理すると:

$$
S^{d-1}\,(\text{方向})
\xrightarrow[\text{内積}]{}
\mathbb{R}^{K}\,(\text{ロジット；角度情報})
\xrightarrow[\text{Softmax}]{}
\Delta^{K-1}\,(\text{確率分布})
$$

nGPTのような設計では、この流れが明示的である。入力ベクトルと各クラスの代表ベクトルがともに球面上にあり、その類似度(角度)が直接ロジットとなる。Softmaxは、この角度情報を確率に変換する役割を担う。

> [!CAUTION]
> **正規化が前提：** この「角度→確率」の対応は、入力と重みの両方が正規化されている場合に限る。標準的なTransformerの出力層では、重み行列や埋め込みが正規化されていないことが多く、ノルムの影響もロジットに混在する。

### Softmaxの冗長性：平行移動不変性と商空間

Softmaxには見落とされがちな構造的性質がある。 $K$ 次元のロジットを入力として受け取るが、出力の確率は $\sum_i p_i = 1$ を満たすため、自由度は $K-1$ である。この「1次元分の冗長性」は、学習ダイナミクスや幾何学的な解釈において重要な意味を持つ。

**平行移動不変性：**
Softmaxは、すべてのロジットに同じ定数を加えても出力が変わらない。

$$\text{softmax}(\mathbf{z} + c \cdot \mathbf{1}) = \text{softmax}(\mathbf{z})$$

これは、ロジット空間における「全体的なシフト」がSoftmaxによって吸収されることを意味する。幾何学的には、Softmaxは $\mathbb{R}^K$ を $(K-1)$ 次元の確率単体に写すが、その際に $\mathbf{1} = (1, 1, \ldots, 1)$ 方向の情報が失われる。

このように、特定の方向（ここでは $\mathbf{1}$ 方向）の差を無視し、同じものとみなして扱う空間を、数学的には **商空間（Quotient Space）** $\mathbb{R}^K / \mathbb{R}\mathbf{1}$ と呼ぶ。Softmaxは $\mathbf{1}$ 方向の平行移動に対して不変であるため、この商空間を経由して（商空間上へ因子化して）、確率単体の内部へ値をとる写像として理解できる。なお、有限のロジット値では単体の境界（確率0）には到達せず、境界は極限としてのみ現れる。

**勾配への影響：**
この商空間の構造は、学習プロセスにも直接現れる。クロスエントロピー損失 $\mathcal{L} = -\log p_y$ の勾配を計算すると以下のようになる。

$$\frac{\partial \mathcal{L}}{\partial z_i} = p_i - \mathbf{1}[i = y]$$

この勾配は常に $\sum_i \frac{\partial \mathcal{L}}{\partial z_i} = 0$ を満たす。つまり、**勾配は $\mathbf{1}$ 方向に成分を持たない（零和制約）**。学習は自動的に冗長な方向を避け、実質的な $K-1$ 次元の商空間内だけで進むこととなる。

| 性質 | 影響 |
| :--- | :--- |
| **平行移動不変性** | ロジットの絶対値は意味を持たない（相対的な差のみが重要） |
| **勾配の零和** | パラメータ更新の勾配は $\mathbf{1}$ 方向に射影されない |
| **バイアスの冗長性** | 1つのクラスのバイアスを固定してもモデルの表現力は不変 |

> [!NOTE]
> **ゲージ自由度とバイアス項：**
> 物理学の言葉を借りれば、この $\mathbf{1}$ 方向の冗長性は**ゲージ自由度**に対応する。分類層 $\mathbf{z} = W\mathbf{x} + \mathbf{b}$ において、同定性（Identifiability）の確保やバイアス全体の一様シフト（ $\mathbf{1}$ 方向）のドリフトを抑えるために1成分を0に固定する実装があるが、これは「ゲージ固定」を行っていることに相当する。ただし、L2正則化のようなゲージ不変でない正則化を全成分にかける場合、どの成分を固定するかによって正則化項の値が変わり、最適化の経路がゲージ選択に依存し得る点には注意が必要だ。

> [!NOTE]
> **情報幾何学的な視点：**
> カテゴリカル分布のパラメータ空間が $(K-1)$ 次元であることは、Softmaxが商空間構造を持つことと直結している。より正確には、 $\mathbb{R}^K \to \mathbb{R}^K/\mathbb{R}\mathbf{1}$ の商射影を通ってSoftmaxが因子化されることで、確率単体（内部）への対応が与えられる。Fisher情報行列が $K$ 次元のままだと特異になるのは、この構造によって1次元分の情報が「潰れている」ことの反映である。

## なぜ指数形なのか：最大エントロピー原理

Softmaxが分類で普遍的に使われるのは、単なる慣習ではない。それは情報理論的に最も「偏見のない」選択であり、同時に最適化の観点からも優れた性質を持つ。

### 制約付きエントロピー最大化

Softmaxが分類で使われる理由を、より深く理解するために、**最大エントロピー原理** との接続を見よう。

問いを設定する：「あるカテゴリ分布について、平均的な特徴量（例えば、各カテゴリの期待スコア）だけが分かっているとき、最もランダムな（仮定の少ない）分布は何か？」

答えは、**指数型分布族** に属する分布である。カテゴリカル分布の場合、これがまさにSoftmaxの形式になる。

より具体的に述べると、以下の最適化問題を考える：

$$\max_{\mathbf{p}} H(\mathbf{p}) = -\sum_{i} p_i \log p_i$$
$$\text{subject to} \quad \sum_{i} p_i = 1, \quad \sum_{i} p_i f_i = \bar{f}$$

ここで $H(\mathbf{p})$ はシャノンエントロピー、 $f_i$ は各カテゴリの特徴量、 $\bar{f}$ はその期待値の制約である。

ラグランジュの未定乗数法を適用すると、解は以下の形になる：

$$p_i \propto \exp(\lambda f_i)$$

これはSoftmaxの形式そのものである。

> [!NOTE]
> **球面との対応：** 球面上で「平均方向が特定の値を取る」という制約の下でエントロピーを最大化すると、von Mises-Fisher (vMF) 分布が得られる（第3回参照）。Softmaxとvmfは、異なる空間（離散カテゴリ vs 連続球面）での最大エントロピー分布として、形式的に対応している。

シャノンエントロピーは、**ユークリッド的な比喩として**、確率単体上の「中心からの距離」のように解釈できる。ただし、これは厳密な幾何学的距離ではなく、直感的な理解のための比喩である。

| 位置 | エントロピー | 意味 |
| --- | --- | --- |
| 頂点（one-hot） | $H = 0$ | 完全に確定、情報なし |
| 中心（一様分布） | $H = \log K$ | 最大の不確実性 |
| 中間 | $0 < H < \log K$ | 部分的な確信 |

Softmaxは、ロジットの大きさに応じて、この「中心から頂点への位置」を決定する。ロジットの差が大きいほど頂点に近づき、差が小さいほど中心に近づく。

> [!NOTE]
> **計量の選び方による違い：** 「距離」をどう測るかは一意ではない。ユークリッド距離、KLダイバージェンス、Fisher計量に基づく測地距離など、採用する計量によって「中心からの距離」の意味は変わる。本節の説明はユークリッド的な直感に基づいているが、情報幾何学ではKLダイバージェンスや測地距離を使うことが多い。

### Log-Sum-Expの凸性：最適化しやすさの源泉

「なぜSoftmaxなのか」に対するもう一つの答えが、**Log-Sum-Exp (LSE) 関数の凸性** である。

Softmaxの分母に現れるLSE関数は以下で定義される：

$$\text{LSE}(\mathbf{z}) = \log \sum_{j=1}^{K} \exp(z_j)$$

この関数は **凸関数** である。凸性の直感的な理解として、LSEは「滑らかな最大値関数」と見なせる。実際、以下の関係が成り立つ：

$$\max_j z_j \leq \text{LSE}(\mathbf{z}) \leq \max_j z_j + \log K$$

max関数は凸だが微分不可能な点を持つ。LSEはこれを滑らかに近似しており、凸性を保ちながら微分可能性を獲得している。

**なぜ凸性が重要か？** クロスエントロピー損失は、正解クラス $y$ に対して以下のように書ける：

$$\mathcal{L}(\mathbf{z}) = -z_y + \text{LSE}(\mathbf{z})$$

第一項 $-z_y$ は線形（したがって凸かつ凹）、第二項 $\text{LSE}(\mathbf{z})$ は凸である。したがって、**ロジット $\mathbf{z}$ に関して損失関数全体が凸** になる。

| 性質 | 最適化への影響 |
| --- | --- |
| 凸関数 | 局所最適解＝大域最適解（ソフトマックス回帰では $W, \mathbf{b}$ に関しても凸） |
| 滑らかさ | 勾配法が安定して収束 |
| Lipschitz連続な勾配 | 学習率の設定が容易（下記参照） |

**勾配とヘッセ行列の構造：** LSEの勾配と二階微分は以下のように計算される：

$$\nabla \text{LSE}(\mathbf{z}) = \text{softmax}(\mathbf{z}) = \mathbf{p}$$

$$\nabla^2 \text{LSE}(\mathbf{z}) = \text{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top$$

このヘッセ行列は、Softmax出力 $\mathbf{p}$ の共分散行列と同じ形式である。
スペクトルノルムで上から抑えられ、最大固有値は高々 $1/2$ である（例えば $\mathbf{p} = (1/2, 1/2, 0, \ldots, 0)$ のように2クラスに確率が集中したときに達する）。
実際、2クラスの場合は非ゼロ固有値が $2p(1-p)$ となり、 $p=1/2$ で最大 $1/2$ を取る。
この有界性が、勾配のLipschitz連続性を保証し、適切な学習率の設定を可能にしている。

この凸性は、後述するCRFにおいても中心的な役割を果たす。CRFの分配関数 $Z(\mathbf{x})$ は、系列全体にわたるlog-sum-expとして表現され、同様の凸性を持つ。

> [!CAUTION]
> **深層学習では全体として非凸：** 上記の凸性は「ロジット $\mathbf{z}$ に関して」の話である。深層学習では、ロジットは入力からの非線形変換を経て計算されるため、**パラメータ全体に関しては損失関数は非凸** である。しかし、出力層付近ではこの凸構造が残っており、学習の最終段階での収束を助けている。

> [!NOTE]
> **数値安定性との関係：** LSEの計算では、オーバーフローを防ぐために $\text{LSE}(\mathbf{z}) = \max_j z_j + \log \sum_j \exp(z_j - \max_j z_j)$ という等価な形式が使われる。これは凸性を保ったまま数値安定性を確保する標準的なテクニックである（実装ノート参照）。

### 温度パラメータ：分布の鋭さの制御

Softmax関数には温度パラメータ $\tau$ が登場する：

$$\text{softmax}(\mathbf{z} / \tau)_i = \frac{\exp(z_i / \tau)}{\sum_j \exp(z_j / \tau)}$$

温度の効果を直感的に理解するには、「確率単体への望遠鏡の倍率」として捉えるのが有用である。

| 温度 $\tau$ | 効果 | 分布の形状 |
| --- | --- | --- |
| $\tau \to 0$ | 差が拡大される | 最大ロジットに集中（argmax的） |
| $\tau = 1$ | 標準的なSoftmax | 中程度の鋭さ |
| $\tau \to \infty$ | 差が縮小される | 一様分布に近づく |

低温ではロジットの差が強調され、分布は頂点（one-hot）に近づく。高温では差が平滑化され、分布は中心（一様）に近づく。

<img width="80%" src="04_fig_temperature.svg" alt="温度パラメータの効果">

図：温度パラメータ $\tau$ による分布の変化。ロジット $\mathbf{z} = (2.0, 1.0, 0.1)$ を固定し、 $\tau$ を変化させたときの単体上の位置。 $\tau \to 0$ で頂点（argmax）に集中し、 $\tau \to \infty$ で中心（一様分布）に近づく。

第3回で見たvMF分布との形式的対応を思い出そう。vMF分布の集中度パラメータ $\kappa$ は、温度の逆数 $1/\tau$ に対応する：

- 高 $\kappa$ （低 $\tau$ ）：鋭い分布、確信度が高い
- 低 $\kappa$ （高 $\tau$ ）：緩やかな分布、不確実性が高い

この対応は、Softmax分類を「球面上の方向分布からのサンプリング」として解釈する際の基礎となる。

> [!CAUTION]
> **対応の限界：** vMF分布は連続的な球面上の分布であり、Softmaxは離散カテゴリ上の分布である。両者は「指数関数×内積」という形式が対応しているだけで、確率モデルとして同一視はできない。

温度パラメータは、様々な場面で調整される：

| 場面 | 典型的な設定 | 目的 |
| --- | --- | --- |
| 学習時 | $\tau = 1$ | 標準的な勾配 |
| 知識蒸留 | $\tau = 2 \sim 20$ | ソフトラベルを生成 |
| 推論時（多様性重視） | $\tau > 1$ | 多様なサンプリング |
| 推論時（確定的） | $\tau \to 0$ | argmax近似 |

知識蒸留（Hinton et al., 2015）では、教師モデルの「暗黙の知識」をソフトラベルとして抽出するために高温を使う。これにより、「猫に似ている犬」といった微妙な関係性が生徒モデルに伝わる。

## 情報幾何学：確率空間の計量構造

確率単体は、ユークリッド空間とは異なる幾何学的性質を持つ。ここでは、その「歪み」を定量化し、最適化にどう活かすかを見ていく。

### Fisher情報行列：確率空間の計量

確率分布の空間を多様体として扱うのが **情報幾何学** である。この多様体上で「距離」や「角度」を測るための道具が **フィッシャー情報行列（Fisher Information Matrix）** である。

パラメータ $\boldsymbol{\theta}$ で決まる確率分布 $p(x; \boldsymbol{\theta})$ に対し、フィッシャー情報行列 $\mathbf{F}$ は以下で定義される：

$$F_{ij} = \mathbb{E}_{p(x;\boldsymbol{\theta})}\left[\frac{\partial \log p}{\partial \theta_i} \frac{\partial \log p}{\partial \theta_j}\right]$$

この行列は、正則性条件（分布の台がパラメータに依存しない、微分と積分の交換が可能など）の下で、確率分布の空間における **リーマン計量** として機能する。直感的には、「パラメータをどの方向に動かすと、分布が大きく変わるか」を定量化している。

カテゴリカル分布 $p(k) = \pi_k$ の場合、フィッシャー情報行列の構造は座標系の取り方に依存する。 $\pi_1, \ldots, \pi_K$ をそのまま座標として扱うと、 $\sum_k \pi_k = 1$ の制約により行列は特異（退化）になる。独立な $(K-1)$ 次元座標に落とすと、オフ対角成分も現れる。直感的には、Fisher-Rao計量の下で「確率が小さいカテゴリほど、わずかな変化で分布が大きく変わる（境界付近が強く重み付けされる）」という性質があり、これが自然勾配の挙動に影響する。より具体的には、独立座標で見たとき計量成分が概ね $1/\pi_k$ のオーダーで効くため、 $\pi_k \to 0$ で大きくなる。

> [!NOTE]
> **境界での特異性とSoftmaxによる回避：** Fisher情報行列は、単体の内部 $\text{int}(\Delta^{K-1})$ （すべての $\pi_k > 0$ ）では正定値なリーマン計量として機能する。しかし、**境界（いずれかの $\pi_k \to 0$ ）では計量成分が発散し、通常のリーマン多様体としての性質が失われる（計量特異点となる）**。Softmaxは指数関数の性質上この境界に到達しないため、出力は常に計量が正定値な領域に留まる。これが、Softmaxの像を「滑らかな統計多様体」として安心して扱える理由である。

> [!NOTE]
> **リーマン計量としてのフィッシャー情報：** 第0回で導入したリーマン計量の概念が、ここで確率分布空間に適用される。フィッシャー情報行列は、統計的な意味で「自然な」距離を定義する。この距離は、パラメータ空間での見かけの距離ではなく、分布間の「区別のしやすさ」を反映している。
> 空間の「物差し」については、[Appendix 4 （空間の「物差し」再考: 2点間から情報の密度まで）](appendix.4.md)でも述べる。
> 確率分布の空間での2種類の「まっすぐ」については、[Appendix 5: 情報幾何学における双対構造：2種類のまっすぐ](appendix.5.md)で述べる。

### 自然勾配：幾何学に従った降下

通常の勾配降下法は、パラメータ空間でのユークリッド勾配を使う：

$$\boldsymbol{\theta} _{t+1} = \boldsymbol{\theta} _t - \eta \nabla _{\boldsymbol{\theta}} \mathcal{L}$$

しかし、パラメータ空間での「1歩」が、分布空間での「どれだけの変化」に対応するかは、現在地によって異なる。

**自然勾配（Natural Gradient）** は、フィッシャー情報行列を使って勾配を補正することで、分布空間での「等しい変化量」を実現する（Amari, 1998）：

$$\tilde{\nabla} _{\boldsymbol{\theta}} \mathcal{L} = \mathbf{F}^{-1} \nabla _{\boldsymbol{\theta}} \mathcal{L}$$

幾何学的には、これは「測地線に沿った最急降下」に対応する。ユークリッド勾配が「地図上の北」を指すなら、自然勾配は「実際の地形での最も急な下り坂」を指す。

### 実用的な接続

厳密な自然勾配は計算コストが高い。そこで実務では、 **自然勾配の発想（前条件付け）に触れる近似** として、適応的最適化手法がしばしば議論される。

> [!IMPORTANT]
> **Adamとの関係（注意が必要）：** AdamやRMSpropを「経験的フィッシャー情報の近似として解釈できる」という研究提案がある。しかし、これは確定的な「理論的基礎」というより **解釈の一つ** である。実際には、(1) Adamは勾配の2次モーメントを座標ごとに正規化するだけで、クロス項を無視している、(2) 経験的フィッシャーと真のフィッシャーは一般に異なる、(3) 損失関数が負の対数尤度でない場合、フィッシャー情報の解釈自体が成り立たない、といった仮定・近似が多く入る。「Adam≈自然勾配」と言い切るのは過剰主張である。

> [!NOTE]
> **曲率情報を用いた最適化の実用化：** 自然勾配の計算はフィッシャー情報行列の逆行列を必要とし、大規模モデルでは直接計算が困難である。実用的なアプローチとして、(1) **Fisher近似系**：K-FAC（Martens & Grosse, 2015）はクロネッカー積によるブロック対角近似を用いて自然勾配を近似する、(2) **ヘシアン系**：Sophia（Liu et al., 2023）は対角ヘシアン推定を前条件として用いる二次最適化手法である。SophiaはFisher情報（情報幾何の計量）を直接近似しているわけではなく、損失関数のヘシアンを用いる点で自然勾配とは異なるアプローチだが、曲率情報を活用する点で広義の二次最適化として関連がある。

## 発展トピック：構造化と離散化

> [!TIP]
> **ここは読み飛ばしてもよい。**
>
> もし固有表現抽出などの「系列データの整合性」が重要なタスク（CRF）や、
> 強化学習のように「離散的な選択」を微分可能に扱いたい場合（Gumbel-Softmax）は、必要な時に立ち戻って参照すればよい。

### 構造化予測：点から線へ

**Softmaxの限界**

Softmaxは各時刻で独立した決定を行う。これは「点の幾何」である。しかし、自然言語処理の多くの問題——固有表現抽出、品詞タグ付け——では、**系列全体の整合性**が重要である。

例えば、固有表現抽出で「B-PER(人名の始まり)」タグの直後に「I-LOC(地名の内部)」タグが続くことは意味論的に矛盾している。しかし、各時刻で独立にSoftmaxを適用すると、このような矛盾した系列にも正の確率が割り当てられてしまう。

この問題を解決するには、バラバラな「点」の選択を、意味のある「線(パス)」の選択へと組織化する必要がある。

**CRF：Softmaxの構造化された拡張**

条件付き確率場(Conditional Random Field; CRF)は、この「点から線へ」の拡張を実現する古典的な手法である(Lafferty et al., 2001)。

幾何学的には、CRFは以下のように理解できる:

- **独立Softmaxの場合**: 各時刻 $t$ で独立に確率単体 $\Delta^{K-1}$ 上の点を出力。出力空間全体は直積空間 $(\Delta^{K-1})^T$
  
- **CRFの場合**: 系列 $\mathbf{y} \in \{1, \ldots, K\}^T$ 上の確率分布を直接定義。これは $K^T$ 個の可能な系列上の確率分布である

$K^T$ 個すべての確率を独立に扱うのは非現実的なので、CRFは**ポテンシャル関数による因子分解**を用いる。これにより、局所的なスコアの和として系列全体のスコアを表現できる:

$$S(\mathbf{y} | \mathbf{x}) = \sum_{t=1}^{T} \psi_t(y_t, \mathbf{x}) + \sum_{t=1}^{T-1} \phi(y_t, y_{t+1})$$

ここで:

- $\psi_t(y_t, \mathbf{x})$ : 出力ポテンシャル(各時刻のロジットに相当)
- $\phi(y_t, y_{t+1})$ : 遷移ポテンシャル(隣接ラベル間の親和性)

系列全体の確率は、Softmaxと同じ構造で定義される:

$$p(\mathbf{y} | \mathbf{x}) = \frac{\exp(S(\mathbf{y} | \mathbf{x}))}{Z(\mathbf{x})}$$

ここで $Z(\mathbf{x}) = \sum_{\mathbf{y}'} \exp(S(\mathbf{y}' | \mathbf{x}))$ は分配関数である。これは、2.2節で見たlog-sum-exp関数の系列版に他ならない。

**Softmaxとの対比**

| 側面 | Softmax | CRF |
| --- | --- | --- |
| 出力空間 | 各時刻の分布列 $(\Delta^{K-1})^T$ | 系列上の分布 |
| 決定単位 | 各時刻の「点」 | 系列全体の「パス」 |
| 系列整合性 | なし | 遷移ポテンシャルによる制御 |
| log-partition | $\log \sum_k \exp(z_k)$ | $\log \sum_{\mathbf{y}} \exp(S(\mathbf{y}))$ |
| 計算量 | $O(TK)$ | $O(TK^2)$ (動的計画法) |

遷移ポテンシャル $\phi$ は学習可能であり、訓練を通じて「好ましい遷移」には高いスコア、「矛盾した遷移」には低いスコアが割り当てられる。完全に禁止したい遷移には $-\infty$ を設定することで、ハード制約として扱うこともできる。

**現代的な接続**

CRFは、深層学習以前の手法だが、その設計原理は現代でも有効である。BERT+CRFのようなハイブリッドモデルは、局所的な文脈理解(Transformer)と大域的な系列整合性(CRF)を組み合わせることで、固有表現抽出などで高い性能を発揮する。

この「局所的な決定を大域的な構造で組織化する」という発想は、注意機構やグラフニューラルネットワークにも通底している。

> [!NOTE]
> **計算効率**: CRFで最も確からしい系列を見つけるには、Viterbiアルゴリズム(動的計画法)を使う。これにより $K^T$ 通りの系列を $O(TK^2)$ で評価できる。

> [!NOTE]
> **ニューラルCRF**: 現代の実装では、出力ポテンシャル $\psi_t$ をニューラルネットワーク(BiLSTMやTransformer)で計算し、遷移ポテンシャル $\phi$ は学習可能な $K \times K$ 行列として扱う。

### 離散への着地：Gumbel-Softmaxとサンプリング

確率分布(単体の内部)から、最終的に一つの離散的な選択(頂点)を行うには、いくつかの方法がある:

- **Argmax**: 最も確率の高いクラスを選ぶ(推論時の標準)
- **サンプリング**: 確率に従ってランダムに選ぶ(LLMの生成時)
- **Gumbel-Softmax**: 微分可能な離散サンプリング(訓練時)

**Gumbel-Softmax**

argmaxの問題は**微分不可能**であることだ。勾配が流れないため、離散的な決定を含むモデルを端から端まで学習することが難しい。

**Gumbel-Softmax** はこの問題を解決する連続緩和である：

$$y_i = \frac{\exp((z_i + g_i)/\tau)}{\sum_j \exp((z_j + g_j)/\tau)}$$

ここで $g_i \sim \text{Gumbel}(0, 1)$ はGumbelノイズである。

幾何学的には、Gumbel-Softmaxは「シンプレックス内部でランダムに揺らしながら、 $\tau \to 0$ で頂点に収束する」操作として理解できる。ノイズはカテゴリカル分布からのサンプリングを近似し、低温極限ではargmaxに一致する。

> [!NOTE]
> **Gumbel分布の役割：** なぜGumbel分布なのか？ Gumbel分布の最大値の分布が、カテゴリカル分布からのサンプリングと一致するという性質（Gumbel-Max trick）に基づいている。詳細は Jang et al. (2017) を参照。

**Straight-Through Estimator (STE)**

もう一つのアプローチが **Straight-Through Estimator (STE)** である：

- **順伝播（Forward）**：離散（argmax）
- **逆伝播（Backward）**：連続（Softmaxの勾配をそのまま使う）

幾何学的に言えば、「見かけは頂点にいるが、勾配は内部から来る」というトリックである。厳密には勾配の推定量としてバイアスがあるが、実用上うまく機能することが多い。

**LLMにおける意味**

LLMの生成過程を、この「連続→離散」の界面から眺めてみよう：

1. 連続的な「思考の軌跡」（hidden states）が球面上を移動する
2. Softmaxで確率分布（シンプレックス内部）に変換される
3. サンプリング or argmax で離散トークン（頂点）に「着地」する

この過程で、連続表現が持つ微妙なニュアンスは、離散化で失われうる。Top-k や Top-p サンプリングは、「頂点の近傍」からランダムに選ぶことで多様性を確保し、情報の損失を緩和する試みである。

## まとめ：3つの視点の統合

本回では、Softmaxという一つの関数を、3つの異なる視点から見てきた:

1. **幾何学**: 球面上の方向 → ロジット → 確率単体(内部) → 頂点(離散)という変換プロセス。各段階が異なる空間の間の写像として理解できる。

2. **統計学**: 最大エントロピー原理により、Softmaxの指数形は「最も偏見のない」選択である。Log-sum-exp関数の凸性は、最適化の良さを保証する。

3. **情報幾何学**: 確率単体は歪んだ空間であり、Fisher計量がその歪みを定量化する。自然勾配は、この計量に従った「真の最急降下」を与える。

これら3つの視点は独立ではなく、互いに補強し合う。幾何学が「場所」を、統計学が「なぜその形か」を、情報幾何学が「どう歩くか」を教えてくれる。

CRFは、この枠組みを「点から線へ」拡張する例である。Softmaxの本質(指数+正規化)は保ちつつ、候補集合を単一のカテゴリから系列全体へと広げることで、構造化予測を可能にする。

次回(第5回)では、視点を変える。今回は「確率分布」として分類を捉えたが、次回は「決定境界」そのものに焦点を移す。マージン、サポートベクトルマシン、そしてArcFaceなどの現代的な手法が、どのように頑健な境界を構築するかを見ていく。確率と幾何、2つのアプローチを統合することで、分類の全体像が見えてくる。

## 実装ノート

> [!NOTE]
> 以下のコードは PyTorch >= 1.9 を前提とする。再現性のために `torch.manual_seed(42)` 等でシードを固定することを推奨。

### 数値安定なSoftmax

<details>
<summary>コード例: 04_stable_softmax.py</summary>

```04_stable_softmax.py
import torch
import torch.nn.functional as F


def stable_softmax(logits, temperature=1.0, dim=-1):
    """数値安定なSoftmax

    Args:
        logits: 入力ロジット
        temperature: 温度パラメータ（default: 1.0）
        dim: softmaxを適用する次元

    Returns:
        確率分布
    """
    # 温度でスケーリング
    scaled = logits / temperature
    # 最大値を引いてオーバーフローを防ぐ
    # （PyTorchのsoftmaxは内部でこれを行うが、明示的に示す）
    shifted = scaled - scaled.max(dim=dim, keepdim=True).values
    exp_shifted = torch.exp(shifted)
    return exp_shifted / exp_shifted.sum(dim=dim, keepdim=True)


# PyTorchの組み込み関数を使う場合
def softmax_with_temperature(logits, temperature=1.0, dim=-1):
    """温度付きSoftmax（推奨）"""
    return F.softmax(logits / temperature, dim=dim)


# 使用例
logits = torch.tensor([2.0, 1.0, 0.1])

print("τ=0.5 (低温):", softmax_with_temperature(logits, 0.5))
print("τ=1.0 (標準):", softmax_with_temperature(logits, 1.0))
print("τ=2.0 (高温):", softmax_with_temperature(logits, 2.0))

# 期待される結果：
# τ=0.5: [0.88, 0.12, 0.01] に近い（最大に集中）
# τ=1.0: [0.66, 0.24, 0.10] に近い
# τ=2.0: [0.49, 0.32, 0.19] に近い（一様に近づく）
```

</details>

### Gumbel-Softmax

<details>
<summary>コード例: 04_gumbel_softmax.py</summary>

```04_gumbel_softmax.py
import torch
import torch.nn.functional as F


def gumbel_softmax(logits, temperature=1.0, hard=False):
    """Gumbel-Softmax（微分可能な離散サンプリング）

    Args:
        logits: 入力ロジット [batch, num_classes]
        temperature: 温度パラメータ
        hard: Trueなら順伝播でone-hot、逆伝播でソフト勾配（STE）

    Returns:
        サンプリングされた確率ベクトル
    """
    # PyTorchの組み込み関数を使用
    return F.gumbel_softmax(logits, tau=temperature, hard=hard)


# 使用例
logits = torch.tensor([[2.0, 1.0, 0.1]])

# ソフトサンプリング（微分可能）
soft_sample = gumbel_softmax(logits, temperature=0.5, hard=False)
print("Soft sample:", soft_sample)

# ハードサンプリング（STE）
hard_sample = gumbel_softmax(logits, temperature=0.5, hard=True)
print("Hard sample:", hard_sample)  # one-hotに近い

# 温度による変化を可視化
temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]
for tau in temperatures:
    samples = torch.stack([gumbel_softmax(logits, tau, hard=False) for _ in range(1000)])
    print(f"τ={tau}: mean={samples.mean(dim=0)}, std={samples.std(dim=0)}")
```

</details>

### 正規化された分類器

<details>
<summary>コード例: 04_normalized_classifier.py</summary>

```04_normalized_classifier.py
import torch
import torch.nn as nn
import torch.nn.functional as F


class NormalizedClassifier(nn.Module):
    """正規化された分類器（角度ベースの分類）

    入力と重みの両方を正規化し、ロジットを純粋な角度情報にする。
    """

    def __init__(self, in_features, num_classes, scale=30.0):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(num_classes, in_features))
        self.scale = scale  # ロジットのスケール（温度の逆数的な役割）

        # 重みを正規化して初期化
        with torch.no_grad():
            self.weight.data = F.normalize(self.weight.data, dim=1)

    def forward(self, x):
        # 入力を正規化
        x_norm = F.normalize(x, dim=-1)
        # 重みを正規化
        w_norm = F.normalize(self.weight, dim=1)
        # 内積 = cosine similarity
        logits = F.linear(x_norm, w_norm)
        # スケーリング（角度マージンの効果を出すため）
        return logits * self.scale


# 使用例
classifier = NormalizedClassifier(768, 10)
embeddings = torch.randn(32, 768)
logits = classifier(embeddings)

# ロジットの範囲を確認
print(f"Logits range: [{logits.min():.2f}, {logits.max():.2f}]")
# 期待値: [-scale, +scale] の範囲（cosineは[-1, 1]なので）
```

</details>

### 自然勾配の近似（教育目的）

<details>
<summary>コード例: 04_natural_gradient_step.py</summary>

```04_natural_gradient_step.py
import torch


def natural_gradient_step(params, loss_fn, data, lr=0.01, damping=1e-4):
    """自然勾配ステップの簡易実装（教育目的）

    警告: これは小規模モデルでの概念実証用。
    大規模モデルではK-FAC等の近似が必要。

    Args:
        params: モデルパラメータ
        loss_fn: 損失関数
        data: 入力データ
        lr: 学習率
        damping: 数値安定性のための正則化項
    """
    # 通常の勾配を計算
    loss = loss_fn(data)
    grads = torch.autograd.grad(loss, params, create_graph=True)

    # フィッシャー情報行列の対角近似（経験的フィッシャー）
    # 注: これは非常に粗い近似
    fisher_diag = []
    for g in grads:
        fisher_diag.append(g.detach() ** 2 + damping)

    # 自然勾配 = F^{-1} * grad
    natural_grads = []
    for g, f in zip(grads, fisher_diag, strict=False):
        natural_grads.append(g.detach() / f)

    # パラメータ更新
    with torch.no_grad():
        for p, ng in zip(params, natural_grads, strict=False):
            p.data -= lr * ng

    return loss.item()


# 注: 実用的な自然勾配の実装には、K-FAC (Martens & Grosse, 2015)
# や EKFAC (George et al., 2018) などの手法を参照されたい。
```

</details>

## 参考文献

### Softmaxと最大エントロピー

- Jaynes, E. T. (1957). Information Theory and Statistical Mechanics. *Physical Review*, 106(4), 620–630.
    - 最大エントロピー原理の古典的論文。統計力学との接続を示した。
- Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
    - 凸最適化の標準的教科書。Log-Sum-Exp関数の凸性や、凸関数の合成規則などを扱う。オンライン版: [https://web.stanford.edu/~boyd/cvxbook/](https://web.stanford.edu/~boyd/cvxbook/)
- Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network. *arXiv:1503.02531*.
    - 知識蒸留の原論文。温度パラメータの実用的な使い方を示した。

### 情報幾何学と自然勾配

- Amari, S. (1998). Natural Gradient Works Efficiently in Learning. *Neural Computation*, 10(2), 251–276.
    - 自然勾配の一次文献。パラメータ空間の計量構造が学習効率に与える影響を示した古典。
- Martens, J., & Grosse, R. (2015). Optimizing Neural Networks with Kronecker-factored Approximate Curvature. *ICML 2015*. arXiv: [arXiv:1503.05671](https://arxiv.org/abs/1503.05671).
    - K-FAC: クロネッカー積によるブロック対角近似を用いた自然勾配の実用的な近似手法。
- Kunstner, F., Balles, L., & Hennig, P. (2019). Limitations of the Empirical Fisher Approximation for Natural Gradient Descent. *NeurIPS 2019*.
    - 経験的フィッシャー近似の限界を理論的に分析。Adamと自然勾配の関係を議論する際の重要な参照。
- Liu, H., Li, Z., Hall, D., Liang, P., & Ma, T. (2023). Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. *ICLR 2024*. arXiv: [arXiv:2305.14342](https://arxiv.org/abs/2305.14342).
    - 対角ヘシアン推定を前条件として用いる二次最適化手法。自然勾配（Fisher近似）とは異なるアプローチ。

### Gumbel-Softmaxと離散サンプリング

- Jang, E., Gu, S., & Poole, B. (2017). Categorical Reparameterization with Gumbel-Softmax. *ICLR 2017*. arXiv: [arXiv:1611.01144](https://arxiv.org/abs/1611.01144).
    - Gumbel-Softmaxの原論文。
- Maddison, C. J., Mnih, A., & Teh, Y. W. (2017). The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. *ICLR 2017*. arXiv: [arXiv:1611.00712](https://arxiv.org/abs/1611.00712).
    - Gumbel-Softmaxと独立に提案された「Concrete分布」。

### 条件付き確率場（CRF）と構造化予測

- Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. *ICML 2001*.
    - CRFの原論文。系列ラベリングにおける構造化予測の古典。
- Sutton, C., & McCallum, A. (2012). An Introduction to Conditional Random Fields. *Foundations and Trends in Machine Learning*, 4(4), 267–373.
    - CRFの包括的なサーベイ論文。理論から実装まで網羅。
- Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL 2019*. arXiv: [arXiv:1810.04805](https://arxiv.org/abs/1810.04805).
    - BERTの原論文。Transformerによる文脈表現学習の基盤。
- Souza, F., Nogueira, R., & Lotufo, R. (2019). Portuguese Named Entity Recognition using BERT-CRF. *arXiv:1909.10649* (v2, 2020).
    - BERT+CRFのハイブリッドモデルの実例（固有表現抽出）。この構成は多くのNERタスクで採用されている。

### 情報幾何学の教科書

- Amari, S. (2016). *Information Geometry and Its Applications*. Applied Mathematical Sciences, Vol. 194. Springer Japan.
    - 情報幾何学の標準的教科書。フィッシャー情報行列、自然勾配、双対構造などを体系的に扱う。
