# 第4回：分類の再統一 I ～Softmaxと情報幾何学～

## 注意事項

- 「ロジット＝角度」の解釈は、重みベクトルと入力ベクトルの両方が $L_2$ 正規化されている場合に成り立つ。標準的なSoftmax分類器では、ノルムの影響も混在する。
- 情報幾何学との接続（Fisher情報＝リーマン計量）は、正則性条件（パラメータに関する微分可能性、台の一致など）の下で成り立つ標準的な対応である。本回では正則性を仮定して議論を進める。
- 自然勾配とAdamの関係は「解釈の一つ」であり、確定的な理論的基礎ではない。

## 導入：連続から離散への架け橋

第3回で、私たちは単位球面という「プラネタリウム」を建設した。すべてのベクトルがノルム1に正規化され、意味は「向き」によって表現される。しかし、この連続的な球面上の点から、最終的にはカテゴリカルな決定——「これは猫である」「次のトークンは"the"である」——を下さなければならない。

この「連続から離散への翻訳」を担うのが **Softmax関数** である。

Softmaxは、深層学習で最も普遍的に使われる関数の一つでありながら、その幾何学的・情報幾何学的な意味は十分に理解されているとは言いがたい。本回では、Softmaxを「確率単体（シンプレックス）への写像」として幾何学的に読み直し、さらに情報幾何学の観点から、温度パラメータや自然勾配の意味を明らかにする。

## Softmax：確率単体への写像

### シンプレックスとは何か

Softmax関数の出力空間を理解するために、まず **確率単体（probability simplex）** を定義しよう。

$K$ 個のカテゴリがある分類問題を考える。各カテゴリへの確率を $p_1, p_2, \ldots, p_K$ とすると、これらは以下の制約を満たす：

$$\sum_{i=1}^{K} p_i = 1, \quad p_i \geq 0$$

この制約を満たす点の集合が、 $(K-1)$ 次元の確率単体 $\Delta^{K-1}$ である。

| カテゴリ数 $K$ | 単体の次元 | 幾何学的形状 |
| --- | --- | --- |
| 2 | 1次元 | 線分（両端が $[1,0]$ と $[0,1]$ ） |
| 3 | 2次元 | 三角形 |
| 4 | 3次元 | 四面体 |
| $K$ | $(K-1)$ 次元 | $(K-1)$ -単体 |

確率単体は凸集合であり、その頂点はone-hotベクトル（ひとつだけ1、他は0）に対応する。単体の内部点は「確率分布」を、頂点は「確定的な選択」を表す。

![確率単体 Δ²（K=3）](04_fig_simplex.svg)
図： $K=3$ の確率単体 $\Delta^2$ 。赤い実線が境界（いずれかの $p_i = 0$ ）、青い破線の内側が開内部 $\text{int}(\Delta^2)$ （滑らかな多様体）。Softmaxの出力は常に内部に留まり、境界には到達しない。

### Softmaxの定義と幾何学的解釈

Softmax関数は、任意の実数ベクトル $\mathbf{z} \in \mathbb{R}^K$ （ロジット）を確率単体上の点に変換する：

$$\text{softmax}(\mathbf{z})_i = \frac{\exp(z_i / \tau)}{\sum _{j=1}^{K} \exp(z_j / \tau)}$$

ここで $\tau > 0$ は温度パラメータ（後述）である。

この関数を幾何学的に理解すると、Softmaxは「 $\mathbb{R}^K$ から確率単体の**内部（interior）** $\text{int}(\Delta^{K-1})$ への滑らかな写像」として捉えられる。入力のロジット空間は $K$ 次元のユークリッド空間であり、出力は $(K-1)$ 次元の確率単体の内部である。次元が1つ減るのは、確率の和が1という制約のためである。

> [!NOTE]
> **単体の「内部」と多様体構造：** 確率単体 $\Delta^{K-1}$ は、境界（いずれかの $p_i = 0$ ）を含む凸集合であり、境界の「角（かど）」や「辺」があるため、それ自体は滑らかな多様体ではない。しかし、Softmaxの出力は指数関数の性質上 $p_i > 0$ を常に満たし、境界に到達しない。したがって、Softmaxの像は単体の**開内部** $\text{int}(\Delta^{K-1}) = \{ \mathbf{p} \in \Delta^{K-1} \mid p_i > 0 \; \forall i \}$ に限られる。この開内部は境界を持たない $(K-1)$ 次元の滑らかな多様体であり、情報幾何学で扱う統計多様体として自然に機能する。本資料で確率単体を「多様体」と呼ぶとき、特に断りがない限り、この開内部を指す。

![Softmax写像](04_fig_softmax_map.svg)
図：Softmax写像の全体像。ロジット空間 $\mathbb{R}^K$ の各点が単体の内部 $\text{int}(\Delta^{K-1})$ に写される。 $\mathbf{1}$ 方向に平行移動した点（紫）は同一の分布に写る。像は常に境界（赤）の内側に留まる。

> [!NOTE]
> **指数関数の役割：** Softmaxが指数関数を使う理由は複数ある。(1) 負のロジットも正の確率に変換できる、(2) 微分可能で滑らか、(3) 後述する最大エントロピー原理との整合性。ただし、指数関数は数値的にオーバーフローしやすいため、実装では最大値を引く安定化が必要（実装ノート参照）。

### 球面からシンプレックスへ

第3回で見たように、nGPTのような設計では、表現空間を単位球面 $S^{d-1}$ に制約する。分類を行う際、入力の埋め込み $\mathbf{x}$ と各クラスの代表ベクトル $\mathbf{w}_k$ の内積をロジットとして計算する：

$$z_k = \mathbf{w}_k^\top \mathbf{x}$$

**両者がともに単位ノルムに正規化されている場合**、この内積はコサイン類似度に等しく、 $z_k = \cos\theta_k$ となる。すなわち、ロジットは角度情報を直接反映する。

この流れを図式化すると：

$$
S^{d-1}\,(\text{方向})
\xrightarrow[\text{内積}]{}
\mathbb{R}^{K}\,(\text{ロジット；角度情報})
\xrightarrow[\text{Softmax}]{}
\Delta^{K-1}\,(\text{確率分布})
$$

Softmaxは、球面上の「方向の類似性」を確率分布に変換する装置として読める。

> [!CAUTION]
> **正規化が前提：** この「角度→確率」の対応は、入力と重みの両方が正規化されている場合に限る。標準的なTransformerの出力層では、重み行列や埋め込みが正規化されていないことが多く、ノルムの影響もロジットに混在する。

### Softmaxの冗長性：1次元分の自由度の行方

Softmaxには見落とされがちな構造的性質がある。 $K$ 次元のロジットを入力として受け取るが、出力の確率は $\sum_i p_i = 1$ を満たすため、**自由度は $K-1$ である**。この「1次元分の冗長性」は、学習ダイナミクスに微妙な影響を与える。

**平行移動不変性：** Softmaxは、すべてのロジットに同じ定数を加えても出力が変わらない：

$$\text{softmax}(\mathbf{z} + c \cdot \mathbf{1}) = \text{softmax}(\mathbf{z})$$

これは、ロジット空間における「全体的なシフト」がSoftmaxによって吸収されることを意味する。幾何学的には、Softmaxは $\mathbb{R}^K$ を $(K-1)$ 次元の確率単体に写すが、その際に $\mathbf{1} = (1, 1, \ldots, 1)$ 方向の情報が失われる。

**勾配への影響：** クロスエントロピー損失 $\mathcal{L} = -\log p_y$ の勾配を計算すると：

$$\frac{\partial \mathcal{L}}{\partial z_i} = p_i - \mathbf{1}[i = y]$$

この勾配は常に $\sum_i \frac{\partial \mathcal{L}}{\partial z_i} = 0$ を満たす。つまり、**勾配は $\mathbf{1}$ 方向に成分を持たない**。これは冗長性の帰結であり、学習が自然に $(K-1)$ 次元の部分空間内で進むことを意味する。

| 性質 | 影響 |
| --- | --- |
| 平行移動不変性 | ロジットの絶対値は意味を持たない（差のみが重要） |
| 勾配の零和 | 学習は自動的に冗長な方向を避ける |
| バイアス項の役割 | 1つのクラスのバイアスを固定しても表現力は変わらない |

> [!NOTE]
> **バイアス項の扱い：** 分類層 $\mathbf{z} = W\mathbf{x} + \mathbf{b}$ において、バイアス $\mathbf{b}$ のうち1成分は冗長である。実装では全 $K$ 成分を持つことが多いが、理論的には $K-1$ 成分で十分である。一部の実装では、最後のクラスのバイアスを0に固定する。ただし、L2正則化をバイアス全成分にかける場合、どの成分を固定するか（ゲージ選択）によって正則化項の値が変わるため、正則化や最適化の経路はゲージ選択に依存し得る。

> [!NOTE]
> **情報幾何学的な視点：** この冗長性は、カテゴリカル分布のパラメータ空間が $(K-1)$ 次元であることと対応している。Softmaxは $\mathbb{R}^K$ から確率単体への**商写像**（quotient map）として理解でき、 $\mathbf{1}$ 方向を「潰す」操作を行っている。Fisher情報行列が $\pi$ をそのまま座標に取ると特異になるのも、この冗長性の反映である。

> [!NOTE]
> **ゲージ自由度と商空間の多様体構造：** $\mathbb{R}^K$ の点のうち、 $\mathbf{1} = (1, 1, \ldots, 1)$ 方向に平行移動した点はすべて単体上の同一の点に写る。すなわち、ロジット $\mathbf{z}$ と $\mathbf{z} + c\mathbf{1}$ は同じ確率分布を定める。この同値関係により、ロジット空間は「 $\mathbf{1}$ 方向を同一視した商空間 $\mathbb{R}^K / \mathbb{R}\mathbf{1}$ 」として $(K-1)$ 次元の多様体構造を持つ。物理学の言葉を借りれば、 $\mathbf{1}$ 方向は**ゲージ自由度**であり、Softmaxはこのゲージを固定して確率単体の内部への全射を与えている。

### 独立から系列へ：CRFの導入

Softmaxは各時刻での独立した決定を行う。しかし、多くの実世界の問題——自然言語処理における固有表現抽出、品詞タグ付け、音声認識——では、**系列全体の整合性** が重要である。

例えば、固有表現抽出で「B-PER（人名の始まり）」タグの直後に「I-LOC（地名の内部）」タグが続くことは意味論的に矛盾している。しかし、各時刻で独立にSoftmaxを適用すると、このような矛盾した系列にも正の確率が割り当てられてしまう。

**条件付き確率場（Conditional Random Field; CRF）** は、この問題に対する古典的な解決策である（Lafferty et al., 2001）。CRFは、バラバラな「点」の選択を、意味のある「線（パス）」の選択へと組織化する。

**幾何学的視点：離散系列上の確率分布**

長さ $T$ の系列を考える。各時刻 $t$ でのラベルは $K$ 個のカテゴリのいずれかを取る。

**独立Softmaxの場合：** 各時刻 $t$ で独立に確率単体 $\Delta^{K-1}$ 上の点（カテゴリ分布）を出力する。モデル出力全体は積空間 $(\Delta^{K-1})^T$ 上にあり、各時刻の分布は独立に決まる。

**CRFの場合：** 系列 $\mathbf{y} \in \{1, \ldots, K\}^T$ （可能な系列は $K^T$ 通り）上の確率分布を直接定義する。この分布は $K^T - 1$ 次元の確率単体 $\Delta^{K^T-1}$ 上の点である。しかし、 $K^T$ 個すべての確率を独立に扱うのは非現実的なので、CRFは **ポテンシャル関数による因子分解** を用いて効率的に表現する。

この因子分解により、CRFは局所的なスコア（各時刻の出力ポテンシャル $\psi_t$ と隣接時刻間の遷移ポテンシャル $\phi$ ）の和として、系列全体のスコアを定義する。特に、**線形鎖構造（各時刻が前後の時刻とのみ依存する）** を仮定することで、動的計画法が適用でき、 $K^T$ 通りの系列上の確率分布（ $\Delta^{K^T-1}$ ）を扱いながら計算量を $O(TK^2)$ に抑えられる。

CRFは系列全体に対して以下の**スコア関数**を定義する：

$$S(\mathbf{y} | \mathbf{x}) = \sum_{t=1}^{T} \psi_t(y_t, \mathbf{x}) + \sum_{t=1}^{T-1} \phi(y_t, y_{t+1})$$

ここで、

- $\psi_t(y_t, \mathbf{x})$ は **出力ポテンシャル**（emission potential）：時刻 $t$ で入力 $\mathbf{x}$ からラベル $y_t$ への親和性を表すスコア（独立Softmaxのロジットに相当する。ニューラルCRFでは、これはネットワークが出力するロジットそのものである）
- $\phi(y_t, y_{t+1})$ は **遷移ポテンシャル**（transition potential）：隣接するラベル $y_t$ から $y_{t+1}$ への遷移の好ましさを表す学習可能な重み（スコアが高いほど、その遷移が好まれる）

系列全体の確率は、スコアを用いてSoftmaxを系列空間全体へ拡張した形で定義される：

$$p(\mathbf{y} | \mathbf{x}) = \frac{1}{Z(\mathbf{x})} \exp(S(\mathbf{y} | \mathbf{x}))$$

ここで $Z(\mathbf{x}) = \sum_{\mathbf{y}' \in \{1,\ldots,K\}^T} \exp(S(\mathbf{y}' | \mathbf{x}))$ は分配関数（すべての可能な系列 $K^T$ 通りにわたる和）である。

遷移ポテンシャル $\phi(y_t, y_{t+1})$ は、学習によって「好ましい遷移には高いスコア、好ましくない遷移には低いスコア」を割り当てる。例えば、「B-PER → I-LOC」のような意味論的に矛盾した遷移には、学習を通じて低いスコアが付く。完全に禁止したい場合は、特定の遷移に $-\infty$ （実装では非常に大きな負値）を設定することで、ハード制約として扱うこともできる。

**Softmaxとの対比：点 vs パス**

| 側面 | Softmax（独立） | CRF（構造化） |
| --- | --- | --- |
| 出力空間 | 各時刻の分布列 $(\Delta^{K-1})^T$ | 系列上の分布 $\Delta^{K^T-1}$ |
| 決定単位 | 各時刻の「点」 | 系列全体の「パス」 |
| 系列整合性 | なし（各時刻独立） | 遷移ポテンシャル $\phi$ による学習可能なバイアス |
| 表現方法 | 独立な分布の積 | ポテンシャル関数による因子分解 |
| 計算量 | $O(TK)$ | $O(TK^2)$ （動的計画法） |

**構造化予測の祖としてのCRF**

CRFは、単なる「系列ラベリングの道具」ではない。それは **構造化予測** （structured prediction）という広大な分野の出発点である。画像セグメンテーション、構文解析、グラフ上のラベル付けなど、多くの問題は「離散構造上の確率分布をポテンシャル関数で因子分解する」という枠組みで定式化できる。

深層学習の時代においても、この視点は健在である。BERTのようなTransformerで得られる文脈表現に、CRF層を重ねるハイブリッドモデル（いわゆるBERT+CRF）は、固有表現抽出などで広く使われている。これは、局所的な文脈理解（Transformer）と大域的な系列整合性（CRF）を組み合わせることで、より精度の高い予測を実現する。

さらに、この「局所的な決定を大域的な構造で組織化する」という発想は、注意機構やグラフニューラルネットワークにも通底している。CRFは、深層学習以前の知恵が現代の構造化予測にどう引き継がれているかを示す好例である。

> [!NOTE]
> **計算効率とViterbiアルゴリズム：** CRFで最も確からしい系列を見つけるには、 $K^T$ 通りすべてを評価するのではなく、**Viterbiアルゴリズム**（動的計画法）を使う。これは有向非巡回グラフ（DAG）上での最大スコア経路探索として定式化でき、 $O(TK^2)$ で最適パスを求められる。この効率化こそが、CRFを実用的にする鍵である。

> [!NOTE]
> **ニューラルCRF：** 現代の実装では、出力ポテンシャル $\psi_t$ をニューラルネットワーク（BiLSTMやTransformer）で計算することが多い。遷移ポテンシャル $\phi$ は学習可能な $K \times K$ 行列として扱う。この組み合わせにより、深層学習の表現力とCRFの構造化予測が統合される。

## 最大エントロピー原理：なぜSoftmaxなのか

### 制約付きエントロピー最大化

Softmaxが分類で使われる理由を、より深く理解するために、**最大エントロピー原理** との接続を見よう。

問いを設定する：「あるカテゴリ分布について、平均的な特徴量（例えば、各カテゴリの期待スコア）だけが分かっているとき、最もランダムな（仮定の少ない）分布は何か？」

答えは、**指数型分布族** に属する分布である。カテゴリカル分布の場合、これがまさにSoftmaxの形式になる。

より具体的に述べると、以下の最適化問題を考える：

$$\max_{\mathbf{p}} H(\mathbf{p}) = -\sum_{i} p_i \log p_i$$
$$\text{subject to} \quad \sum_{i} p_i = 1, \quad \sum_{i} p_i f_i = \bar{f}$$

ここで $H(\mathbf{p})$ はシャノンエントロピー、 $f_i$ は各カテゴリの特徴量、 $\bar{f}$ はその期待値の制約である。

ラグランジュの未定乗数法を適用すると、解は以下の形になる：

$$p_i \propto \exp(\lambda f_i)$$

これはSoftmaxの形式そのものである。

> [!NOTE]
> **球面との対応：** 球面上で「平均方向が特定の値を取る」という制約の下でエントロピーを最大化すると、von Mises-Fisher (vMF) 分布が得られる（第3回参照）。Softmaxとvmfは、異なる空間（離散カテゴリ vs 連続球面）での最大エントロピー分布として、形式的に対応している。

### エントロピーの幾何学的意味

シャノンエントロピーは、**ユークリッド的な比喩として**、確率単体上の「中心からの距離」のように解釈できる。ただし、これは厳密な幾何学的距離ではなく、直感的な理解のための比喩である。

| 位置 | エントロピー | 意味 |
| --- | --- | --- |
| 頂点（one-hot） | $H = 0$ | 完全に確定、情報なし |
| 中心（一様分布） | $H = \log K$ | 最大の不確実性 |
| 中間 | $0 < H < \log K$ | 部分的な確信 |

Softmaxは、ロジットの大きさに応じて、この「中心から頂点への位置」を決定する。ロジットの差が大きいほど頂点に近づき、差が小さいほど中心に近づく。

> [!NOTE]
> **計量の選び方による違い：** 「距離」をどう測るかは一意ではない。ユークリッド距離、KLダイバージェンス、Fisher計量に基づく測地距離など、採用する計量によって「中心からの距離」の意味は変わる。本節の説明はユークリッド的な直感に基づいているが、情報幾何学ではKLダイバージェンスや測地距離を使うことが多い。

### Log-Sum-Expの凸性：最適化しやすさの源泉

「なぜSoftmaxなのか」に対するもう一つの答えが、**Log-Sum-Exp (LSE) 関数の凸性** である。

Softmaxの分母に現れるLSE関数は以下で定義される：

$$\text{LSE}(\mathbf{z}) = \log \sum_{j=1}^{K} \exp(z_j)$$

この関数は **凸関数** である。凸性の直感的な理解として、LSEは「滑らかな最大値関数」と見なせる。実際、以下の関係が成り立つ：

$$\max_j z_j \leq \text{LSE}(\mathbf{z}) \leq \max_j z_j + \log K$$

max関数は凸だが微分不可能な点を持つ。LSEはこれを滑らかに近似しており、凸性を保ちながら微分可能性を獲得している。

**なぜ凸性が重要か？** クロスエントロピー損失は、正解クラス $y$ に対して以下のように書ける：

$$\mathcal{L}(\mathbf{z}) = -z_y + \text{LSE}(\mathbf{z})$$

第一項 $-z_y$ は線形（したがって凸かつ凹）、第二項 $\text{LSE}(\mathbf{z})$ は凸である。したがって、**ロジット $\mathbf{z}$ に関して損失関数全体が凸** になる。

| 性質 | 最適化への影響 |
| --- | --- |
| 凸関数 | 局所最適解＝大域最適解（ソフトマックス回帰では $W, \mathbf{b}$ に関しても凸） |
| 滑らかさ | 勾配法が安定して収束 |
| Lipschitz連続な勾配 | 学習率の設定が容易（下記参照） |

**勾配とヘッセ行列の構造：** LSEの勾配と二階微分は以下のように計算される：

$$\nabla \text{LSE}(\mathbf{z}) = \text{softmax}(\mathbf{z}) = \mathbf{p}$$

$$\nabla^2 \text{LSE}(\mathbf{z}) = \text{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top$$

このヘッセ行列は、Softmax出力 $\mathbf{p}$ の共分散行列と同じ形式である。スペクトルノルムで上から抑えられ、最大固有値は高々 $1/2$ である（例えば $\mathbf{p} = (1/2, 1/2, 0, \ldots, 0)$ のように2クラスに確率が集中したときに達する）。この有界性が、勾配のLipschitz連続性を保証し、適切な学習率の設定を可能にしている。

> [!CAUTION]
> **深層学習では全体として非凸：** 上記の凸性は「ロジット $\mathbf{z}$ に関して」の話である。深層学習では、ロジットは入力からの非線形変換を経て計算されるため、**パラメータ全体に関しては損失関数は非凸** である。しかし、出力層付近ではこの凸構造が残っており、学習の最終段階での収束を助けている。

> [!NOTE]
> **数値安定性との関係：** LSEの計算では、オーバーフローを防ぐために $\text{LSE}(\mathbf{z}) = \max_j z_j + \log \sum_j \exp(z_j - \max_j z_j)$ という等価な形式が使われる。これは凸性を保ったまま数値安定性を確保する標準的なテクニックである（実装ノート参照）。

## 情報幾何学：確率分布空間の計量

### フィッシャー情報行列

確率分布の空間を多様体として扱うのが **情報幾何学** である。この多様体上で「距離」や「角度」を測るための道具が **フィッシャー情報行列（Fisher Information Matrix）** である。

パラメータ $\boldsymbol{\theta}$ で決まる確率分布 $p(x; \boldsymbol{\theta})$ に対し、フィッシャー情報行列 $\mathbf{F}$ は以下で定義される：

$$F_{ij} = \mathbb{E}_{p(x;\boldsymbol{\theta})}\left[\frac{\partial \log p}{\partial \theta_i} \frac{\partial \log p}{\partial \theta_j}\right]$$

この行列は、正則性条件（分布の台がパラメータに依存しない、微分と積分の交換が可能など）の下で、確率分布の空間における **リーマン計量** として機能する。直感的には、「パラメータをどの方向に動かすと、分布が大きく変わるか」を定量化している。

カテゴリカル分布 $p(k) = \pi_k$ の場合、フィッシャー情報行列の構造は座標系の取り方に依存する。 $\pi_1, \ldots, \pi_K$ をそのまま座標として扱うと、 $\sum_k \pi_k = 1$ の制約により行列は特異（退化）になる。独立な $(K-1)$ 次元座標に落とすと、オフ対角成分も現れる。直感的には、Fisher-Rao計量の下で「確率が小さいカテゴリほど、わずかな変化で分布が大きく変わる（境界付近が強く重み付けされる）」という性質があり、これが自然勾配の挙動に影響する。より具体的には、独立座標で見たとき計量成分が概ね $1/\pi_k$ のオーダーで効くため、 $\pi_k \to 0$ で大きくなる。

> [!NOTE]
> **境界での特異性とSoftmaxによる回避：** Fisher情報行列は、単体の内部 $\text{int}(\Delta^{K-1})$ （すべての $\pi_k > 0$ ）では正定値なリーマン計量として機能する。しかし、**境界（いずれかの $\pi_k \to 0$ ）では計量成分が発散し、通常のリーマン多様体としての性質が失われる（計量特異点となる）**。Softmaxは指数関数の性質上この境界に到達しないため、出力は常に計量が正定値な領域に留まる。これが、Softmaxの像を「滑らかな統計多様体」として安心して扱える理由である。

> [!NOTE]
> **リーマン計量としてのフィッシャー情報：** 第0回で導入したリーマン計量の概念が、ここで確率分布空間に適用される。フィッシャー情報行列は、統計的な意味で「自然な」距離を定義する。この距離は、パラメータ空間での見かけの距離ではなく、分布間の「区別のしやすさ」を反映している。

### 自然勾配：分布空間での最急降下

通常の勾配降下法は、パラメータ空間でのユークリッド勾配を使う：

$$\boldsymbol{\theta} _{t+1} = \boldsymbol{\theta} _t - \eta \nabla _{\boldsymbol{\theta}} \mathcal{L}$$

しかし、パラメータ空間での「1歩」が、分布空間での「どれだけの変化」に対応するかは、現在地によって異なる。

**自然勾配（Natural Gradient）** は、フィッシャー情報行列を使って勾配を補正することで、分布空間での「等しい変化量」を実現する（Amari, 1998）：

$$\tilde{\nabla} _{\boldsymbol{\theta}} \mathcal{L} = \mathbf{F}^{-1} \nabla _{\boldsymbol{\theta}} \mathcal{L}$$

幾何学的には、これは「測地線に沿った最急降下」に対応する。ユークリッド勾配が「地図上の北」を指すなら、自然勾配は「実際の地形での最も急な下り坂」を指す。

> [!IMPORTANT]
> **Adamとの関係（注意が必要）：** AdamやRMSpropを「経験的フィッシャー情報の近似として解釈できる」という研究提案がある。しかし、これは確定的な「理論的基礎」というより **解釈の一つ** である。実際には、(1) Adamは勾配の2次モーメントを座標ごとに正規化するだけで、クロス項を無視している、(2) 経験的フィッシャーと真のフィッシャーは一般に異なる、(3) 損失関数が負の対数尤度でない場合、フィッシャー情報の解釈自体が成り立たない、といった仮定・近似が多く入る。「Adam≈自然勾配」と言い切るのは過剰主張である。

> [!NOTE]
> **曲率情報を用いた最適化の実用化：** 自然勾配の計算はフィッシャー情報行列の逆行列を必要とし、大規模モデルでは直接計算が困難である。実用的なアプローチとして、(1) **Fisher近似系**：K-FAC（Martens & Grosse, 2015）はクロネッカー積によるブロック対角近似を用いて自然勾配を近似する、(2) **ヘシアン系**：Sophia（Liu et al., 2023）は対角ヘシアン推定を前条件として用いる二次最適化手法である。SophiaはFisher情報（情報幾何の計量）を直接近似しているわけではなく、損失関数のヘシアンを用いる点で自然勾配とは異なるアプローチだが、曲率情報を活用する点で広義の二次最適化として関連がある。

## 温度パラメータ：確信度の制御

### 温度の定義と効果

Softmax関数には温度パラメータ $\tau$ が登場する：

$$\text{softmax}(\mathbf{z} / \tau)_i = \frac{\exp(z_i / \tau)}{\sum_j \exp(z_j / \tau)}$$

温度の効果を直感的に理解するには、「確率単体への望遠鏡の倍率」として捉えるのが有用である。

| 温度 $\tau$ | 効果 | 分布の形状 |
| --- | --- | --- |
| $\tau \to 0$ | 差が拡大される | 最大ロジットに集中（argmax的） |
| $\tau = 1$ | 標準的なSoftmax | 中程度の鋭さ |
| $\tau \to \infty$ | 差が縮小される | 一様分布に近づく |

低温ではロジットの差が強調され、分布は頂点（one-hot）に近づく。高温では差が平滑化され、分布は中心（一様）に近づく。

![温度パラメータの効果](04_fig_temperature.svg)
図：温度パラメータ $\tau$ による分布の変化。ロジット $\mathbf{z} = (2.0, 1.0, 0.1)$ を固定し、 $\tau$ を変化させたときの単体上の位置。 $\tau \to 0$ で頂点（argmax）に集中し、 $\tau \to \infty$ で中心（一様分布）に近づく。

### 温度と集中度の対応

第3回で見たvMF分布との形式的対応を思い出そう。vMF分布の集中度パラメータ $\kappa$ は、温度の逆数 $1/\tau$ に対応する：

- 高 $\kappa$ （低 $\tau$ ）：鋭い分布、確信度が高い
- 低 $\kappa$ （高 $\tau$ ）：緩やかな分布、不確実性が高い

この対応は、Softmax分類を「球面上の方向分布からのサンプリング」として解釈する際の基礎となる。

> [!CAUTION]
> **対応の限界：** vMF分布は連続的な球面上の分布であり、Softmaxは離散カテゴリ上の分布である。両者は「指数関数×内積」という形式が対応しているだけで、確率モデルとして同一視はできない。

### 実務での温度の使い方

温度パラメータは、様々な場面で調整される：

| 場面 | 典型的な設定 | 目的 |
| --- | --- | --- |
| 学習時 | $\tau = 1$ | 標準的な勾配 |
| 知識蒸留 | $\tau = 2 \sim 20$ | ソフトラベルを生成 |
| 推論時（多様性重視） | $\tau > 1$ | 多様なサンプリング |
| 推論時（確定的） | $\tau \to 0$ | argmax近似 |

知識蒸留（Hinton et al., 2015）では、教師モデルの「暗黙の知識」をソフトラベルとして抽出するために高温を使う。これにより、「猫に似ている犬」といった微妙な関係性が生徒モデルに伝わる。

## ロジットと角度：条件付きの対応

### 内積の分解

分類器のロジットは、一般に重みベクトル $\mathbf{w}_k$ と入力 $\mathbf{x}$ の内積として計算される：

$$z_k = \mathbf{w}_k^\top \mathbf{x}$$

この内積は、以下のように分解できる：

$$\mathbf{w}_k^\top \mathbf{x} = \|\mathbf{w}_k\| \|\mathbf{x}\| \cos\theta_k$$

ここで $\theta_k$ は $\mathbf{w}_k$ と $\mathbf{x}$ のなす角である。

### 正規化による角度分離

**両方のベクトルが単位ノルムに正規化されている場合**：

$$\mathbf{w}_k^\top \mathbf{x} = 1 \cdot 1 \cdot \cos\theta_k = \cos\theta_k$$

このとき、ロジットは純粋に角度情報のみを反映する。Softmaxは「角度を確率に変換する装置」として機能する。

この設計は、ArcFace（第5回で詳述）やnGPT（第3回）で採用されている。角度情報のみで分類を行うことで、表現の「方向」に意味を集中させることができる。

> [!IMPORTANT]
> **標準的な設計との違い：** 標準的なTransformerの分類ヘッドでは、重み行列も入力埋め込みも正規化されていないことが多い。この場合、ロジットには角度だけでなくノルムの影響も混在する。「ロジット＝角度」の解釈は、明示的に正規化設計を採用した場合にのみ成り立つ。

## 補論：離散と連続の界面

本講義は連続多様体を主役にしているが、LLMの最終出力は **離散トークン** である。この「界面」を幾何学的に理解することは、実装上極めて重要である。

### シンプレックスから頂点への「着地」

Softmaxの出力は確率単体の **内部** の点（連続）であり、実際の出力はシンプレックスの **頂点**（one-hot、離散）である。argmaxは、**ユークリッド距離の意味で**「最も近い頂点への射影」として理解できる。ただし、KLダイバージェンスなど他の計量を採用すると、「最近傍」の定義が変わりうる点に注意されたい。

この連続から離散への移行で情報が失われる。例えば、確率 $[0.51, 0.49]$ と $[0.99, 0.01]$ は、argmaxを取ると同じ結果になるが、元の分布は全く異なる。この「量子化誤差」は、シンプレックス内部から頂点への距離として定量化できる。

### Gumbel-Softmax：微分可能な離散化

argmaxの問題は **微分不可能** であることだ。勾配が流れないため、離散的な決定を含むモデルを端から端まで学習することが難しい。

**Gumbel-Softmax** はこの問題を解決する連続緩和である：

$$y_i = \frac{\exp((z_i + g_i)/\tau)}{\sum_j \exp((z_j + g_j)/\tau)}$$

ここで $g_i \sim \text{Gumbel}(0, 1)$ はGumbelノイズである。

幾何学的には、Gumbel-Softmaxは「シンプレックス内部でランダムに揺らしながら、 $\tau \to 0$ で頂点に収束する」操作として理解できる。ノイズはカテゴリカル分布からのサンプリングを近似し、低温極限ではargmaxに一致する。

> [!NOTE]
> **Gumbel分布の役割：** なぜGumbel分布なのか？ Gumbel分布の最大値の分布が、カテゴリカル分布からのサンプリングと一致するという性質（Gumbel-Max trick）に基づいている。詳細は Jang et al. (2017) を参照。

### Straight-Through Estimator (STE)

もう一つのアプローチが **Straight-Through Estimator (STE)** である：

- **順伝播（Forward）**：離散（argmax）
- **逆伝播（Backward）**：連続（Softmaxの勾配をそのまま使う）

幾何学的に言えば、「見かけは頂点にいるが、勾配は内部から来る」というトリックである。厳密には勾配の推定量としてバイアスがあるが、実用上うまく機能することが多い。

### LLMにおける意味

LLMの生成過程を、この「連続→離散」の界面から眺めてみよう：

1. 連続的な「思考の軌跡」（hidden states）が球面上を移動する
2. Softmaxで確率分布（シンプレックス内部）に変換される
3. サンプリング or argmax で離散トークン（頂点）に「着地」する

この過程で、連続表現が持つ微妙なニュアンスは、離散化で失われうる。Top-k や Top-p サンプリングは、「頂点の近傍」からランダムに選ぶことで多様性を確保し、情報の損失を緩和する試みである。

## まとめ

| 概念 | 定義 | Softmaxでの役割 |
| --- | --- | --- |
| **確率単体** | 確率分布の空間（凸集合） | Softmaxの出力空間（内部） |
| **最大エントロピー原理** | 制約下で最もランダムな分布 | Softmaxが指数型になる理由 |
| **Log-Sum-Expの凸性** | 滑らかな最大値関数 | 損失関数の最適化しやすさ |
| **フィッシャー情報行列** | 確率空間のリーマン計量 | 自然勾配の定義 |
| **自然勾配** | 分布空間での最急降下 | 学習効率の改善（理論的） |
| **温度パラメータ** | 分布の鋭さを制御 | 確信度の調整 |
| **Gumbel-Softmax** | 微分可能な離散サンプリング | 離散選択の学習を可能に |
| **CRF** | 系列全体のエネルギー関数 | 確率空間における経路の妥当性の制約 |

### 本回のポイント

Softmaxは単なる「確率への変換」ではない。情報幾何学の観点からは、それは確率単体の内部——境界を含まない滑らかな統計多様体——への写像であり、温度パラメータはその多様体上での「位置」を制御する。そして、入力と重みが正規化されている場合、Softmaxは「角度を確率に変換する装置」として読める。

次回は、この幾何学的視点をさらに発展させ、角度マージンの設計とSVMとの統一的理解を目指す。

## 次回予告

第5回「分類の再統一 II ～マージンの幾何学～」では、角度マージンの直感をSVM的視点と接続する。

球面上での分類において、「マージンを最大化する」とはどういうことか。ArcFace、CosFace、SphereFaceといった手法が採用する角度マージンの設計意図を、幾何学的に理解する。さらに、SVMの最大マージン原理との統一的な見方を提示する。

## 実装ノート

> [!NOTE]
> 以下のコードは PyTorch >= 1.9 を前提とする。再現性のために `torch.manual_seed(42)` 等でシードを固定することを推奨。

### 数値安定なSoftmax

<details>
<summary>コード例: 04_stable_softmax.py</summary>

```04_stable_softmax.py
import torch
import torch.nn.functional as F


def stable_softmax(logits, temperature=1.0, dim=-1):
    """数値安定なSoftmax

    Args:
        logits: 入力ロジット
        temperature: 温度パラメータ（default: 1.0）
        dim: softmaxを適用する次元

    Returns:
        確率分布
    """
    # 温度でスケーリング
    scaled = logits / temperature
    # 最大値を引いてオーバーフローを防ぐ
    # （PyTorchのsoftmaxは内部でこれを行うが、明示的に示す）
    shifted = scaled - scaled.max(dim=dim, keepdim=True).values
    exp_shifted = torch.exp(shifted)
    return exp_shifted / exp_shifted.sum(dim=dim, keepdim=True)


# PyTorchの組み込み関数を使う場合
def softmax_with_temperature(logits, temperature=1.0, dim=-1):
    """温度付きSoftmax（推奨）"""
    return F.softmax(logits / temperature, dim=dim)


# 使用例
logits = torch.tensor([2.0, 1.0, 0.1])

print("τ=0.5 (低温):", softmax_with_temperature(logits, 0.5))
print("τ=1.0 (標準):", softmax_with_temperature(logits, 1.0))
print("τ=2.0 (高温):", softmax_with_temperature(logits, 2.0))

# 期待される結果：
# τ=0.5: [0.88, 0.12, 0.01] に近い（最大に集中）
# τ=1.0: [0.66, 0.24, 0.10] に近い
# τ=2.0: [0.49, 0.32, 0.19] に近い（一様に近づく）
```

</details>

### Gumbel-Softmax

<details>
<summary>コード例: 04_gumbel_softmax.py</summary>

```04_gumbel_softmax.py
import torch
import torch.nn.functional as F


def gumbel_softmax(logits, temperature=1.0, hard=False):
    """Gumbel-Softmax（微分可能な離散サンプリング）

    Args:
        logits: 入力ロジット [batch, num_classes]
        temperature: 温度パラメータ
        hard: Trueなら順伝播でone-hot、逆伝播でソフト勾配（STE）

    Returns:
        サンプリングされた確率ベクトル
    """
    # PyTorchの組み込み関数を使用
    return F.gumbel_softmax(logits, tau=temperature, hard=hard)


# 使用例
logits = torch.tensor([[2.0, 1.0, 0.1]])

# ソフトサンプリング（微分可能）
soft_sample = gumbel_softmax(logits, temperature=0.5, hard=False)
print("Soft sample:", soft_sample)

# ハードサンプリング（STE）
hard_sample = gumbel_softmax(logits, temperature=0.5, hard=True)
print("Hard sample:", hard_sample)  # one-hotに近い

# 温度による変化を可視化
temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]
for tau in temperatures:
    samples = torch.stack([gumbel_softmax(logits, tau, hard=False) for _ in range(1000)])
    print(f"τ={tau}: mean={samples.mean(dim=0)}, std={samples.std(dim=0)}")
```

</details>

### 正規化された分類器

<details>
<summary>コード例: 04_normalized_classifier.py</summary>

```04_normalized_classifier.py
import torch
import torch.nn as nn
import torch.nn.functional as F


class NormalizedClassifier(nn.Module):
    """正規化された分類器（角度ベースの分類）

    入力と重みの両方を正規化し、ロジットを純粋な角度情報にする。
    """

    def __init__(self, in_features, num_classes, scale=30.0):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(num_classes, in_features))
        self.scale = scale  # ロジットのスケール（温度の逆数的な役割）

        # 重みを正規化して初期化
        with torch.no_grad():
            self.weight.data = F.normalize(self.weight.data, dim=1)

    def forward(self, x):
        # 入力を正規化
        x_norm = F.normalize(x, dim=-1)
        # 重みを正規化
        w_norm = F.normalize(self.weight, dim=1)
        # 内積 = cosine similarity
        logits = F.linear(x_norm, w_norm)
        # スケーリング（角度マージンの効果を出すため）
        return logits * self.scale


# 使用例
classifier = NormalizedClassifier(768, 10)
embeddings = torch.randn(32, 768)
logits = classifier(embeddings)

# ロジットの範囲を確認
print(f"Logits range: [{logits.min():.2f}, {logits.max():.2f}]")
# 期待値: [-scale, +scale] の範囲（cosineは[-1, 1]なので）
```

</details>

### 自然勾配の近似（教育目的）

<details>
<summary>コード例: 04_natural_gradient_step.py</summary>

```04_natural_gradient_step.py
import torch


def natural_gradient_step(params, loss_fn, data, lr=0.01, damping=1e-4):
    """自然勾配ステップの簡易実装（教育目的）

    警告: これは小規模モデルでの概念実証用。
    大規模モデルではK-FAC等の近似が必要。

    Args:
        params: モデルパラメータ
        loss_fn: 損失関数
        data: 入力データ
        lr: 学習率
        damping: 数値安定性のための正則化項
    """
    # 通常の勾配を計算
    loss = loss_fn(data)
    grads = torch.autograd.grad(loss, params, create_graph=True)

    # フィッシャー情報行列の対角近似（経験的フィッシャー）
    # 注: これは非常に粗い近似
    fisher_diag = []
    for g in grads:
        fisher_diag.append(g.detach() ** 2 + damping)

    # 自然勾配 = F^{-1} * grad
    natural_grads = []
    for g, f in zip(grads, fisher_diag, strict=False):
        natural_grads.append(g.detach() / f)

    # パラメータ更新
    with torch.no_grad():
        for p, ng in zip(params, natural_grads, strict=False):
            p.data -= lr * ng

    return loss.item()


# 注: 実用的な自然勾配の実装には、K-FAC (Martens & Grosse, 2015)
# や EKFAC (George et al., 2018) などの手法を参照されたい。
```

</details>

## 参考文献

### Softmaxと最大エントロピー

- Jaynes, E. T. (1957). Information Theory and Statistical Mechanics. *Physical Review*, 106(4), 620–630.
    - 最大エントロピー原理の古典的論文。統計力学との接続を示した。
- Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
    - 凸最適化の標準的教科書。Log-Sum-Exp関数の凸性や、凸関数の合成規則などを扱う。オンライン版: [https://web.stanford.edu/~boyd/cvxbook/](https://web.stanford.edu/~boyd/cvxbook/)
- Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network. *arXiv:1503.02531*.
    - 知識蒸留の原論文。温度パラメータの実用的な使い方を示した。

### 情報幾何学と自然勾配

- Amari, S. (1998). Natural Gradient Works Efficiently in Learning. *Neural Computation*, 10(2), 251–276.
    - 自然勾配の一次文献。パラメータ空間の計量構造が学習効率に与える影響を示した古典。
- Martens, J., & Grosse, R. (2015). Optimizing Neural Networks with Kronecker-factored Approximate Curvature. *ICML 2015*. arXiv: [arXiv:1503.05671](https://arxiv.org/abs/1503.05671).
    - K-FAC: クロネッカー積によるブロック対角近似を用いた自然勾配の実用的な近似手法。
- Kunstner, F., Balles, L., & Hennig, P. (2019). Limitations of the Empirical Fisher Approximation for Natural Gradient Descent. *NeurIPS 2019*.
    - 経験的フィッシャー近似の限界を理論的に分析。Adamと自然勾配の関係を議論する際の重要な参照。
- Liu, H., Li, Z., Hall, D., Liang, P., & Ma, T. (2023). Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. *ICLR 2024*. arXiv: [arXiv:2305.14342](https://arxiv.org/abs/2305.14342).
    - 対角ヘシアン推定を前条件として用いる二次最適化手法。自然勾配（Fisher近似）とは異なるアプローチ。

### Gumbel-Softmaxと離散サンプリング

- Jang, E., Gu, S., & Poole, B. (2017). Categorical Reparameterization with Gumbel-Softmax. *ICLR 2017*. arXiv: [arXiv:1611.01144](https://arxiv.org/abs/1611.01144).
    - Gumbel-Softmaxの原論文。
- Maddison, C. J., Mnih, A., & Teh, Y. W. (2017). The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. *ICLR 2017*. arXiv: [arXiv:1611.00712](https://arxiv.org/abs/1611.00712).
    - Gumbel-Softmaxと独立に提案された「Concrete分布」。

### 条件付き確率場（CRF）と構造化予測

- Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. *ICML 2001*.
    - CRFの原論文。系列ラベリングにおける構造化予測の古典。
- Sutton, C., & McCallum, A. (2012). An Introduction to Conditional Random Fields. *Foundations and Trends in Machine Learning*, 4(4), 267–373.
    - CRFの包括的なサーベイ論文。理論から実装まで網羅。
- Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL 2019*. arXiv: [arXiv:1810.04805](https://arxiv.org/abs/1810.04805).
    - BERTの原論文。Transformerによる文脈表現学習の基盤。
- Souza, F., Nogueira, R., & Lotufo, R. (2019). Portuguese Named Entity Recognition using BERT-CRF. *arXiv:1909.10649* (v2, 2020).
    - BERT+CRFのハイブリッドモデルの実例（固有表現抽出）。この構成は多くのNERタスクで採用されている。

### 情報幾何学の教科書

- Amari, S. (2016). *Information Geometry and Its Applications*. Applied Mathematical Sciences, Vol. 194. Springer Japan.
    - 情報幾何学の標準的教科書。フィッシャー情報行列、自然勾配、双対構造などを体系的に扱う。
