# 第2回：ノルムの呪い ～意味と不確実性の未分化～

## 注意事項

本回で扱う高次元空間の性質には、成立条件がある：

- 「高次元でほぼ直交」は、ベクトルが**独立にランダムサンプリング**された場合に成り立つ。学習済み埋め込みは必ずしもランダムではなく、意図的な構造を持つため、この性質が常に成り立つとは限らない。
- 「距離の集中」は、各成分が**独立同分布**のランダムベクトルを仮定した場合の現象である。実データが低次元多様体上に集中している場合、集中は緩和されうる。
- 「距離より角度が安定」は多くの場合に経験的に観察されるが、普遍的法則ではない。データの性質・タスク・正規化の有無に依存する。

## 導入：一つの数値に詰め込まれた複数の意味

ベクトルには「向き」と「長さ」がある。向きが意味を、長さが大きさを表す——この素朴な直感は、高次元の表現学習では破綻する。

深層学習以前、そして深層学習の初期においても、ベクトルの**ノルム（長さ）** は曖昧な存在だった。それは「意味の強度」なのか、「モデルの確信度」なのか、それとも「単なるスケーリングノイズ」なのか。これらが区別されないまま、一つの数値に詰め込まれていた。

本回では、この「ノルムの呪い」を解剖する。そして、高次元空間が持つ奇妙な性質——距離の集中と直交性——を理解することで、なぜ「角度中心の設計」が必要になるのかを準備する。

## 未分化な指標としてのノルム

### 三重の意味

ベクトル $\mathbf{v}$ のノルム $\|\mathbf{v}\|$ が担っていた意味を整理しよう：

| 解釈 | 意味 | 例 |
| --- | --- | --- |
| **意味の強度** | 概念がどれだけ「強く」表現されているか | Word2Vecで高頻度語のノルムが大きい |
| **モデルの確信度** | 予測に対するモデルの自信 | ロジットの絶対値が大きいほど確信 |
| **スケーリングノイズ** | 学習過程で生じた偶発的なスケール | 初期化や学習率に依存する変動 |

問題は、これらが**同じ数値に混在している**ことだ。

### Word2Vecにおけるノルムの曖昧さ

Word2Vec（Mikolov et al., 2013）を例に考えよう。学習後の単語ベクトルを観察すると、**高頻度語ほどノルムが大きい傾向がある**ことが報告されている（Schakel & Wilson, 2015）。

> [!NOTE]
> **条件依存性：** この傾向は常に成り立つわけではなく、手法（SGNS/CBOW）、サブサンプリングの有無、負例分布、正則化、学習率スケジュール等によって変わりうる。また、入力側の埋め込み、出力側の埋め込み、あるいはそれらの平均など、**どの表現を観察するか**でも傾向は異なる。以下の議論は「典型的に観察される傾向」として理解されたい。

これは何を意味するのか？

- **解釈A（意味の強度）：** 高頻度語は「意味が濃い」から大きい
- **解釈B（学習の偏り）：** 高頻度語は更新回数が多いから大きくなっただけ
- **解釈C（情報量）：** 高頻度語は文脈を多く見ているから情報が詰まっている

正解は不明瞭である。おそらく、これらが混ざり合っている。

### 確信度とノルムの混同

分類タスクでは、最終層の出力（ロジット）がSoftmaxに入力される。ロジットの絶対値が大きいほど、Softmax後の確率分布は「尖る」。

$$p_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}$$

ここで、ロジット $z$ を一様にスケーリングすると（ $z \to \alpha z$ , $\alpha > 1$ ）、確率分布はより尖る。つまり、**ノルムのスケールが確信度に影響する**。

しかし、このスケールは学習過程で制御されていないことが多い。結果として、「モデルが本当に確信しているのか」「たまたまスケールが大きいだけなのか」が区別できない。

> [!NOTE]
> **温度パラメータとの関係：** Softmaxの「温度」 $T$ は、ロジットを $z/T$ に置き換える操作である。これはノルムのスケーリングと等価であり、第4回で情報幾何学の観点から再検討する。

### 正規化しない時代の問題

深層学習の初期（2010年代前半）には、多くの場合、表現ベクトルを正規化せずに使うことが一般的だった。コサイン類似度を使う場合でも、計算時に正規化するだけで、表現自体は非正規化のままであることが多かった。

> [!NOTE]
> **例外の存在：** タスクによっては、昔からコサイン類似度を前提として、事実上の正規化相当の運用をしていた例もある（情報検索、距離学習（metric learning）など）。ここでの記述は「代表的な深層学習の実践」についてである。

この設計には、以下の問題がある：

1. **類似度とノルムの干渉（ハブ化の問題）：** 内積 $\mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos \theta$ を使う場合、ノルムが大きいベクトルは、あらゆるベクトルとの内積が大きくなりやすい。これにより、高頻度語などが無関係な検索クエリでも上位に出現する「ハブ化（hubness）」と呼ばれる現象が生じる。
2. **学習の不安定性：** ノルムが発散または崩壊すると、勾配も不安定になる
3. **解釈の困難：** ベクトルの各成分が何を意味するか分かりにくい

> [!TIP]
> **現代的な緩和策（Layer Normalization）：**
> 現代のTransformerモデルで標準的に使われる **Layer Normalization** も、この問題への回答の一つだ。LayerNormは各トークンベクトルの平均と分散を正規化するため、結果としてベクトルのノルムを $\sqrt{d}$ 付近に強制的に揃える効果を持つ。これにより、明示的な球面上での最適化を行わなくとも、実質的に「ノルムの呪い」を緩和した状態で学習が進むようになっている。

## 次元の呪いの幾何学的理解

### 高次元空間の逆説

「次元の呪い（curse of dimensionality）」という言葉は、Richard Bellmanが動的計画法の文脈で1957年に導入した。機械学習では、高次元空間でデータを扱う際に生じる様々な困難を指す。

その中でも最も直感に反するのが、**距離の集中（concentration of distances）** である。

### 距離の集中：すべてが「同じくらい遠い」

$d$ 次元空間で、原点からランダムな点への距離を考えよう。各成分が独立に標準正規分布 $N(0, 1)$ に従うとき、原点からの距離 $r = \|\mathbf{x}\|$ の分布はどうなるか。

**期待値：**
$$\mathbb{E}[r^2] = \mathbb{E}\left[\sum_{i=1}^d x_i^2\right] = d$$

したがって $\mathbb{E}[r] \approx \sqrt{d}$ （厳密には $\sqrt{d}$ より少し小さい）。

**分散：**
$$\text{Var}[r^2] = \sum_{i=1}^d \text{Var}[x_i^2] = 2d$$

標準偏差は $\sqrt{2d}$ なので、 $r^2$ の**相対的なばらつき**（変動係数）は：

$$\frac{\sqrt{\text{Var}[r^2]}}{\mathbb{E}[r^2]} = \frac{\sqrt{2d}}{d} = \sqrt{\frac{2}{d}} \to 0 \quad (d \to \infty)$$

つまり、**次元が上がるほど、原点からの距離は期待値 $\sqrt{d}$ 付近に集中する**。

### 実験による確認

この現象を実験で確認しよう：

| 次元 $d$ | 距離の期待値 | 相対標準偏差 | 挙動 |
| --- | --- | --- | --- |
| 3 | $\approx 1.7$ | $\approx 0.58$ | ばらつきが大きい |
| 100 | $\approx 10$ | $\approx 0.10$ | かなり集中 |
| 1000 | $\approx 31.6$ | $\approx 0.032$ | ほぼ一定値 |
| 10000 | $\approx 100$ | $\approx 0.010$ | 事実上、定数 |

> [!IMPORTANT]
> **集中の条件：** この距離の集中は、各成分が**独立同分布**であることを仮定している。実データが低次元多様体上に集中している場合、あるいは成分間に強い相関がある場合、集中の程度は緩和される。「高次元だから必ず距離が区別できなくなる」わけではない。

### ユークリッド距離が情報を失う瞬間

距離の集中は、**最近傍探索（kNN）** に深刻な影響を与えうる。

理想的には、「近い点」と「遠い点」を明確に区別したい。しかし、典型的な i.i.d. 設定では、次元が上がるにつれて最近傍と最遠傍の相対差が小さくなることが知られている。これは以下のように表現される：

$$\frac{d_{\max} - d_{\min}}{d_{\min}} \to 0$$

> [!CAUTION]
> **極限の条件：** この式の収束は、**どの極限を取るか**（ $d \to \infty$ で $n$ 固定か、 $n, d$ 同時に増やすか等）や、**データの分布**（i.i.d.か、多様体上か）に依存する。「高次元なら常に最近傍が無意味化する」わけではなく、「典型的な i.i.d. 設定で、 $d$ が $n$ に対して十分大きい場合に顕著になる」というのがより正確な記述である。詳細は Beyer et al. (1999) を参照。

すべての点が「だいたい同じ距離」になると、「近さ」の概念が曖昧になる。これが、第1回で触れた「kNNの高次元での困難」の数学的背景の一つである。

> [!CAUTION]
> **内在次元との区別：** アンビエント次元（座標の次元）が高くても、データの**内在次元**（多様体の次元）が低ければ、kNNは機能しうる。例えば、1000次元空間に埋め込まれた10次元多様体上のデータでは、多様体上の距離は依然として意味を持つ。

## 直交性の発見：呪いから祝福へ

### ランダムベクトルはほぼ直交する

高次元空間には、もう一つの驚くべき性質がある。**ランダムに選んだ二つのベクトルは、ほぼ直交する**。

単位球面上で一様ランダムに選んだ二つのベクトル $\mathbf{u}, \mathbf{v}$ の内積 $\langle \mathbf{u}, \mathbf{v} \rangle$ を考えよう。

**期待値：**
$$\mathbb{E}[\langle \mathbf{u}, \mathbf{v} \rangle] = 0$$

これは対称性から明らか。

**分散：**
$$\text{Var}[\langle \mathbf{u}, \mathbf{v} \rangle] = \frac{1}{d}$$

したがって、内積の標準偏差は $1/\sqrt{d}$ 。 $d = 1000$ なら約0.032、 $d = 10000$ なら約0.01。

つまり、**高次元では、ランダムなベクトル同士の角度は90°に集中する**。

### 赤道への集中

この現象は、第0回で予告した「赤道集中」と表裏一体である。

$d$ 次元単位球面上で、北極 $(1, 0, \ldots, 0)$ との角度 $\theta$ を考える。一様分布のもとで、 $\theta$ の期待値は $\pi/2$ （赤道）に近づき、分散は0に近づく。

**幾何学的直感：** 高次元球面では、「表面積のほとんどが赤道付近」に存在する。北極から見れば、ほぼすべての点が「真横」（90°）にある。

### 呪いを祝福に変える

この直交性は、呪いではなく**祝福**と見なすこともできる。

**祝福としての解釈：**

| 性質 | 呪いとしての側面 | 祝福としての側面 |
| --- | --- | --- |
| **距離の集中** | 近傍探索が困難 | ノイズに対してロバスト |
| **直交性** | ランダムな構造が見えない | 独立な表現を大量に詰め込める |

直交するベクトルは「互いに干渉しない」。高次元空間には、内積が小さい（ほぼ直交する）ベクトルを**多数**配置できる。

> [!NOTE]
> **球面符号との関係：** 「内積が $\epsilon$ 以下のベクトルをいくつ配置できるか」は、球面符号（spherical code）の問題として研究されている。確率的構成（ランダムにベクトルを選ぶ）により、 $d$ 次元で内積 $\epsilon$ 以下のベクトルが $\exp(c \cdot d)$ 個程度**存在しうる**ことが示せる（ $c$ は $\epsilon$ に依存する定数）。これは存在の下界であり、最適化された配置では同程度以上が期待できる場合もある。ただし、係数 $c$ や成立条件は $\epsilon$ の取り方、誤差の定義、構成法によって変わる。詳細は Conway & Sloane (1999) を参照。

### 大規模言語モデルへの示唆

この「直交性の祝福」は、大規模言語モデルの埋め込み空間が機能する理由の一端を説明する。

数万から数十万の語彙を持つモデルが、数千次元の空間で単語を区別できるのはなぜか。一つの答えは、「高次元空間には、互いにほぼ直交するベクトルを大量に詰め込める」からである。

ただし、これは必要条件であって十分条件ではない。学習によって、意味的に関連する単語は近く、関連しない単語は遠く（直交に近く）配置される必要がある。ランダムな直交性だけでは、意味構造は生まれない。

## ヒントンの苦闘：向きと長さを分離する試み

### 問題意識

Geoffrey Hintonは、ニューラルネットワークの表現における「向き」と「長さ」の分離に長年取り組んできた。その背景には、「ノルムが複数の意味を担っている」という問題意識がある。

### RBM：エネルギー関数による表現学習

**制限ボルツマンマシン（RBM: Restricted Boltzmann Machine）** は、Hintonらが2000年代に精力的に研究した生成モデルである（Hinton, 2002; Hinton & Salakhutdinov, 2006）。

RBMは、可視層と隠れ層の間のエネルギー関数を定義し、低エネルギーの状態が「良い表現」に対応するように学習する。

$$E(\mathbf{v}, \mathbf{h}) = -\mathbf{b}^\top \mathbf{v} - \mathbf{c}^\top \mathbf{h} - \mathbf{v}^\top W \mathbf{h}$$

ここで $\mathbf{v}$ は可視層、 $\mathbf{h}$ は隠れ層、 $W$ は重み行列。

RBMの文脈では、隠れ層の活性化パターンが「表現」となる。しかし、この表現においても、ノルム（活性化の総和）が何を意味するかは曖昧だった。

### カプセルネットワーク：明示的な分離の試み

**カプセルネットワーク（Capsule Networks）** は、Hintonらが2017年に提案したアーキテクチャである（Sabour, Frosst, & Hinton, 2017）。

カプセルネットワークの核心的アイデアは、**「向き」と「長さ」を明示的に分離する**ことである：

| 成分 | 意味 | 例（顔認識） |
| --- | --- | --- |
| **向き（方向ベクトル）** | 特徴の「姿勢」や「属性」 | 顔の向き、表情、照明 |
| **長さ（ノルム）** | 特徴の「存在確率」 | その顔がどれだけ確実に存在するか |

カプセルの出力ベクトル $\mathbf{u}$ に対し、「squash」関数を適用して長さを0〜1に正規化する：

$$\mathbf{v} = \frac{\|\mathbf{u}\|^2}{1 + \|\mathbf{u}\|^2} \cdot \frac{\mathbf{u}}{\|\mathbf{u}\|}$$

これにより、 $\|\mathbf{v}\|$ は「存在確率」として解釈でき、 $\mathbf{v}/\|\mathbf{v}\|$ は「姿勢」として解釈できる。

### カプセルネットワークの評価

カプセルネットワークは、理論的には美しいアイデアだが、実用上の課題が多いと指摘されている（Peer et al., 2021; Gu & Tresp, 2020）：

- **計算コスト：** 動的ルーティングが計算量を増やす
- **スケーラビリティ：** ImageNetのような大規模データセットへの適用が困難
- **性能：** MNISTやsmallNORBなどの小規模データセットでは競争力があるが、CNNやTransformerに比べて、大規模ベンチマークでの優位性は限定的と報告されている

> [!NOTE]
> **カプセルネットワークの意義：** 実用的な成功は限定的だったが、「向きと長さの分離」という問題提起は重要である。後のCosFace/ArcFace（第5回）やnGPTの正規化設計にも、「向き（方向）を重視し、長さを制御する」という共通の発想が見られる。ただし、これらの手法がカプセルネットワークから**直接的な影響**を受けたかどうかは、公開されている資料からは確認できない。共通する問題意識が独立に発展した可能性もある。

### 「なぜ長さを捨てられないのか？」

Hintonの苦闘は、根本的な問いを投げかける：

> ノルムに意味があるなら、それを明示的にモデル化すべきではないか？
> ノルムに意味がないなら、なぜ捨てないのか？

この問いに対する一つの回答が、「最初から単位球面上で計算する」という設計方針である。次回、この方針を「プラネタリウムの建設」として導入する。

## 補足：情報幾何学からの視点

第0回で導入した情報幾何学の観点から、ノルムの問題を再考しよう。

### フィッシャー情報と「重要な方向」

フィッシャー情報行列は、パラメータ空間の各方向が「どれだけ情報を持っているか」を測る。すべての方向が等しく重要なわけではない。

ユークリッド空間では、すべての方向に同じ「重み」が与えられる（等方性）。しかし、確率分布の空間では、フィッシャー情報行列が「方向ごとの重要性」を定める。

### ノルムと情報量

表現ベクトルのノルムが「情報量」や「確信度」を表すべきなら、それは学習によって**明示的に**獲得されるべきである。暗黙のうちにノルムに意味が込められ、しかもその意味が曖昧なままでは、モデルの解釈も制御も困難になる。

第4回では、Softmaxを情報幾何学の観点から再解釈し、「温度」や「スケール」が確率分布空間でどのような意味を持つかを整理する。

## 実装ノート：高次元の性質を体感する

> [!NOTE]
> 以下のコードは NumPy >= 1.20, Matplotlib >= 3.5 を前提とする。再現性のために `np.random.seed(42)` 等でシードを固定することを推奨。

### 距離の集中の可視化

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

def distance_concentration(dims, n_samples=10000):
    """各次元での原点からの距離分布を計算"""
    results = []
    for d in dims:
        # 標準正規分布からサンプリング
        X = np.random.randn(n_samples, d)
        distances = np.linalg.norm(X, axis=1)
        results.append({
            'dim': d,
            'mean': distances.mean(),
            'std': distances.std(),
            'relative_std': distances.std() / distances.mean(),
            'distances': distances
        })
    return results

dims = [3, 10, 100, 1000]
results = distance_concentration(dims)

# 結果の表示
print("次元 | 平均距離 | 標準偏差 | 相対標準偏差")
print("-" * 45)
for r in results:
    print(f"{r['dim']:4d} | {r['mean']:8.2f} | {r['std']:8.2f} | {r['relative_std']:8.4f}")

# ヒストグラムの可視化
fig, axes = plt.subplots(1, 4, figsize=(16, 3))
for ax, r in zip(axes, results):
    ax.hist(r['distances'], bins=50, density=True, alpha=0.7)
    ax.axvline(r['mean'], color='red', linestyle='--', label=f'mean={r["mean"]:.1f}')
    ax.set_title(f'd = {r["dim"]}')
    ax.set_xlabel('Distance from origin')
    ax.legend()
plt.tight_layout()
plt.show()
```

### 高次元での直交性の確認

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

def check_orthogonality(dims, n_pairs=5000):
    """ランダムな単位ベクトル対の内積分布を計算"""
    results = []
    for d in dims:
        # 単位球面上の一様ランダムベクトル
        v1 = np.random.randn(n_pairs, d)
        v1 = v1 / np.linalg.norm(v1, axis=1, keepdims=True)
        v2 = np.random.randn(n_pairs, d)
        v2 = v2 / np.linalg.norm(v2, axis=1, keepdims=True)
        
        # 内積の計算
        dots = np.sum(v1 * v2, axis=1)
        
        # 角度に変換（度数法）
        angles = np.arccos(np.clip(dots, -1, 1)) * 180 / np.pi
        
        results.append({
            'dim': d,
            'dot_mean': dots.mean(),
            'dot_std': dots.std(),
            'angle_mean': angles.mean(),
            'angle_std': angles.std(),
            'angles': angles
        })
    return results

dims = [3, 10, 100, 1000]
results = check_orthogonality(dims)

# 結果の表示
print("次元 | 内積平均 | 内積標準偏差 | 角度平均 | 角度標準偏差")
print("-" * 60)
for r in results:
    print(f"{r['dim']:4d} | {r['dot_mean']:+8.4f} | {r['dot_std']:12.4f} | "
          f"{r['angle_mean']:8.1f}° | {r['angle_std']:8.1f}°")

# 角度のヒストグラム
fig, axes = plt.subplots(1, 4, figsize=(16, 3))
for ax, r in zip(axes, results):
    ax.hist(r['angles'], bins=50, density=True, alpha=0.7)
    ax.axvline(90, color='red', linestyle='--', label='90°')
    ax.set_title(f'd = {r["dim"]}')
    ax.set_xlabel('Angle (degrees)')
    ax.set_xlim(0, 180)
    ax.legend()
plt.tight_layout()
plt.show()
```

### Word2Vecにおけるノルムと頻度の関係（概念実験）

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

# 擬似的なWord2Vecベクトル（実際のWord2Vecモデルがあれば置き換え可能）
# 高頻度語ほど更新回数が多く、ノルムが大きくなる傾向をシミュレート

n_words = 1000
dim = 100

# Zipf則に従う頻度
ranks = np.arange(1, n_words + 1)
frequencies = 1.0 / ranks  # Zipf則: f ∝ 1/rank
frequencies = frequencies / frequencies.sum()

# 頻度に比例した「更新回数」でノルムが成長すると仮定
# （これは単純化したモデルであり、実際のWord2Vecとは異なる）
norms = np.sqrt(frequencies * 10000)  # 更新回数に比例してノルム成長

# 散布図
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.scatter(np.log10(frequencies), norms, alpha=0.5, s=10)
plt.xlabel('log10(Frequency)')
plt.ylabel('Norm (simulated)')
plt.title('頻度とノルムの関係（シミュレーション）')

plt.subplot(1, 2, 2)
plt.scatter(np.log10(ranks), norms, alpha=0.5, s=10)
plt.xlabel('log10(Rank)')
plt.ylabel('Norm (simulated)')
plt.title('順位とノルムの関係（シミュレーション）')

plt.tight_layout()
plt.show()

# 注意：これは概念的なシミュレーションです。
# 実際のWord2Vecベクトルでの検証には、gensimなどのライブラリを使用してください。
```

## 参考文献

### 次元の呪い・距離の集中

- Bellman, R. (1957). *Dynamic Programming*. Princeton University Press.
    - 「次元の呪い」という用語の起源。
- Beyer, K., Goldstein, J., Ramakrishnan, R., & Shaft, U. (1999). When Is "Nearest Neighbor" Meaningful? *ICDT 1999*, 217–235.
    - 高次元での距離の集中と、kNNの困難を分析。
- Aggarwal, C. C., Hinneburg, A., & Keim, D. A. (2001). On the Surprising Behavior of Distance Metrics in High Dimensional Space. *ICDT 2001*, 420–434.
    - 様々な距離尺度の高次元での挙動を比較。

### 単語埋め込みとノルム

- Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. *NeurIPS 2013*.
    - Word2Vecの論文。
- Schakel, A. M. J., & Wilson, B. J. (2015). Measuring Word Significance using Distributed Representations of Words. *arXiv:1508.02297*.
    - Word2Vecにおけるノルムと頻度の関係を分析。

### RBMとカプセルネットワーク

- Hinton, G. E. (2002). Training Products of Experts by Minimizing Contrastive Divergence. *Neural Computation*, 14(8), 1771–1800.
    - RBMの学習アルゴリズム（Contrastive Divergence）。
- Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the Dimensionality of Data with Neural Networks. *Science*, 313(5786), 504–507.
    - 深層オートエンコーダとRBMによる事前学習。
- Sabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic Routing Between Capsules. *NeurIPS 2017*. arXiv: [arXiv:1710.09829](https://arxiv.org/abs/1710.09829).
    - カプセルネットワークの原論文。
- Peer, D., Stabinger, S., & Rodríguez-Sánchez, A. (2021). Limitation of Capsule Networks. *Pattern Recognition Letters*, 144, 68–75. arXiv: [arXiv:1905.08744](https://arxiv.org/abs/1905.08744) (2019).
    - カプセルネットワークの限界（計算コスト、動的ルーティングの効果の疑問）を実験的に分析。
- Gu, J., & Tresp, V. (2020). Improving the Robustness of Capsule Networks to Image Affine Transformations. *CVPR 2020*.
    - カプセルネットワークの改良と、大規模データセットでの課題を議論。

### 球面符号・高次元幾何

- Conway, J. H., & Sloane, N. J. A. (1999). *Sphere Packings, Lattices and Groups* (3rd ed.). Springer.
    - 球面符号とパッキング問題の包括的な参考書。

## 次回予告

第3回「プラネタリウムの建設」では、ノルムの問題を根本から解決する設計方針——**球面上での計算**——を導入する。

角度中心の設計、正規化の実装上の注意点、そして球面上の確率分布（von Mises-Fisher分布）を扱う。「平らな地図」を捨て、「丸い地球儀」の上で表現学習を再構築する旅が始まる。
