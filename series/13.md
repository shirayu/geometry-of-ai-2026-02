
# 第13回：高次元の深淵 ～幾何学的な恐怖と祝福～

## 注意事項

本講義で扱う内容には、確立された数学的事実と、研究途上の解釈が混在している。特に以下の点に留意されたい：

- 「赤道集中現象」は高次元球面の数学的性質であり、球面上一様分布という設定の下で無条件に成り立つ。
- 「距離の集中」や「kNNの破綻」は、データの分布・相関構造・内在次元に依存し、常に成り立つわけではない。
- MoEの「各Expertが直交部分空間を担当」は理想化された仮説であり、実際の学習済みMoEで厳密に成り立つとは限らない。
- 敵対的サンプルの幾何学的解釈は研究途上であり、確立された理論ではない。

## 導入：高次元という「異世界」

私たちの直感は3次元空間で鍛えられてきた。しかし、現代の深層学習モデルが扱う表現空間は数百から数千次元に及ぶ。この「高次元」という異世界では、私たちの幾何学的直感はしばしば裏切られる。

第2回で「ノルムの呪い」として高次元の問題に触れた。本講義では、その議論をさらに深め、高次元空間の性質が**恐怖**（予測不可能性、敵対的サンプル）と**祝福**（表現力、スパース性の活用）の両面を持つことを見ていく。そして、この性質を積極的に活用するMixture of Experts（MoE）アーキテクチャへと議論を接続する。

## 高次元の二つの顔：第2回からの復習と深化

### 「ほぼ直交」という奇妙な世界

第2回で述べたように、高次元空間では以下の現象が起きる：

> [!NOTE]
> 高次元空間でランダムに2つのベクトルを選ぶと、それらはほぼ確実に直交に近い。

これは数学的に証明できる事実である。 $d$ 次元単位球面上で**一様ランダム**に選んだ2つのベクトル $\mathbf{u}, \mathbf{v}$ の内積 $\langle \mathbf{u}, \mathbf{v} \rangle$ は、 $$d \to \infty$$ で平均0、分散 $1/d$ に集中する。

この性質は**表現力の源泉**となる。なぜなら、互いにほぼ直交する多数のベクトルを「詰め込む」ことができるからだ。高次元空間には、互いの内積が小さい（例えば絶対値0.1以下）ベクトルを**指数的に多く**配置できることが知られている。これが、大規模言語モデルが膨大な語彙やコンセプトを区別できる理由の一つである。

> [!NOTE]
> 「内積 $\epsilon$ 以下のベクトルをいくつ配置できるか」は球面符号（spherical code）やパッキング問題として研究されており、次元と $\epsilon$ に依存した上界・下界が知られている。詳細は Spherical Codes に関する文献を参照されたい。

### 「ほぼ等距離」という落とし穴

しかし、同じ性質が**予測不可能性の源**にもなりうる。高次元では、ランダムな点同士の距離がほぼ等しくなる傾向がある。これを**距離の集中現象**（concentration of distances）と呼ぶ。

典型的な設定（例：各成分が独立同分布のランダムベクトル）では、 $d$ 次元空間で原点からランダムな点への距離は、 $d$ が大きくなると相対的なばらつきが減少する：

$$\frac{\text{標準偏差}}{\text{平均}} \propto \frac{1}{\sqrt{d}} \to 0 \quad (d \to \infty)$$

直感的に言えば、高次元では「近い」と「遠い」の区別が曖昧になりうる。

> [!CAUTION]
> この現象の程度は、データの分布・相関構造・スパース性・内在次元（データが実際に存在する多様体の次元）に強く依存する。実データが低次元多様体上に集中している場合や、強い相関構造を持つ場合、距離の集中は緩和されることがある。「高次元なら必ず距離が区別できなくなる」わけではない。
> 参考：Houle et al., "On the Behavior of Intrinsically High-Dimensional Spaces" (JMLR, 2018)

## 赤道集中現象：高次元球面の奇妙な地理

### 「ほぼすべての点が赤道付近」

高次元球面には、直感に反する性質がある。北極を一点固定したとき、球面上のほぼすべての点は「赤道付近」に存在する。

これを数学的に述べよう。 $d$ 次元単位球面 $S^{d-1}$ 上で一様分布に従う点を取り、北極（例えば $(1, 0, \ldots, 0)$ ）との角度を $\theta$ とする。このとき、 $\theta$ の分布は $d$ が大きくなると $\pi/2$ （赤道）に集中する。

$$\mathbb{E}[\theta] \to \frac{\pi}{2}, \quad \text{Var}[\theta] \to 0 \quad (d \to \infty)$$

これは**純粋に数学的な事実**であり、球面上一様分布という設定のもとで無条件に成り立つ（球面等周不等式から導かれる）。

### 直感的理解

なぜこのようなことが起きるのか。球面上の「帯」の面積を考えてみよう。赤道付近の帯は、極付近の帯よりも圧倒的に広い。高次元ではこの差が指数的に拡大し、結果として「ほぼすべての面積が赤道付近」に集中する。

この現象は、第3回で導入した「プラネタリウム」（球面上の表現空間）の設計において重要な含意を持つ。球面上で「均等に」点を配置しようとしても、高次元では自然と赤道付近に集中してしまう。これは意図的な設計なしには避けられない。

> [!NOTE]
> 参考：ETH Zurich, "Measure Concentration" lecture notes

## kNNの幾何学：高次元での破綻と復活

### k近傍法の基本

k近傍法（k-Nearest Neighbors, kNN）は、最も直感的な機械学習アルゴリズムの一つである。クエリ点に最も近いk個の訓練データを見つけ、その多数決（分類）または平均（回帰）で予測を行う。

この単純さゆえに、kNNは高次元の問題を如実に浮かび上がらせる。

### 高次元でのkNNの困難

距離の集中現象により、高次元ではkNNが困難になる**場合がある**。最近傍と最遠傍の距離比を考えよう：

$$\frac{d_{\max} - d_{\min}}{d_{\min}} \to 0 \quad (d \to \infty, \text{特定の分布仮定下})$$

これは、「最も近い点」と「最も遠い点」の区別がつきにくくなることを意味する。kNNの根幹である「近さ」の概念が曖昧になるのだ。

> [!IMPORTANT]
> この現象は「アンビエント次元（座標の次元）が高い」ことと「内在次元（データが実際に存在する多様体の次元）が高い」ことを区別する必要がある。実データが低次元多様体上に集中している場合、見かけ上高次元でもkNNは機能しうる。
> 参考：Beyer et al., "On the Surprising Behavior of Distance Metrics in High Dimensional Spaces" (ICDT 1999)

### 角度による改善：第3回との接続

ユークリッド距離の代わりに**コサイン類似度**（角度）を使うことで、高次元でもkNNの性能が改善する**場合がある**。

なぜか。正規化されたベクトル（単位球面上の点）間のコサイン類似度は、内積そのものである：

$$\text{cos\\_sim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} = \mathbf{u} \cdot \mathbf{v} \quad (\|\mathbf{u}\| = \|\mathbf{v}\| = 1)$$

さらに重要な点として、単位ベクトル間ではユークリッド距離とコサイン類似度は**単調同値**の関係にある：

$$\|\mathbf{u} - \mathbf{v}\|^2 = 2(1 - \mathbf{u}^\top \mathbf{v})$$

つまり、コサイン類似度が高いほどユークリッド距離は小さくなり、両者は同じ順位付けを与える。これが「正規化してからユークリッド距離を使う」ことの数学的根拠である。

高次元でランダムベクトルがほぼ直交する（内積≈0）一方で、**意味的に関連するベクトルは非ランダムな構造を持つ**。この構造が、角度ベースの類似度で捉えられる。

> [!CAUTION]
> コサイン類似度が有効なのは、データが「意味構造を持つよう学習されている」場合（埋め込み空間など）である。ランダムに近いデータでは、角度も一様に近くなり、区別は困難なままである。「角度を使えば常に救済される」わけではない。

> [!WARNING]
> コサイン類似度はノルムで除算するため、ノルムが極端に小さいベクトル（原点付近のノイズ）に対しては計算が不安定になる。実装上は、正規化前にノルムの下限を設けるか、そのようなベクトルを除外する処理が必要になることがある。
> また、高次元空間での近傍探索には**Hubness問題**が知られている。これは、特定の点（hub）が多くのクエリの最近傍として過剰に選ばれる現象であり、コサイン類似度でも完全には解消されない。対策として、相互近傍（mutual nearest neighbors）や局所スケーリング（local scaling）などの手法が提案されている。

### 近似最近傍探索（ANN）とスパース性の活用

厳密なkNNは計算量が $O(nd)$ （ $n$ : データ数、 $d$ : 次元数）であり、大規模データには適用困難である。ここで**近似最近傍探索**（Approximate Nearest Neighbor, ANN）が登場する。

代表的な手法とその幾何学的アイデアを整理しよう：

| 手法 | 幾何学的アイデア | 参考文献 |
| ------ | ------------------ | ---------- |
| **LSH** (Locality Sensitive Hashing) | ランダム射影で空間を分割し、近い点が同じバケットに入る確率を高める | Indyk & Motwani, "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality" (STOC 1998) |
| **Product Quantization** | 高次元空間を低次元部分空間の直積に分解し、各部分空間で量子化 | Jégou et al. (IEEE TPAMI 2011) |
| **HNSW** (Hierarchical Navigable Small World) | 階層的なグラフ構造で「近道」を作り、効率的に探索 | Malkov & Yashunin (arXiv 2016 / IEEE TPAMI 2020) |

特に**Product Quantization**は、MoEの部分空間分割と構造的に類似している。高次元ベクトルを複数の部分空間に分割し、各部分空間で独立に処理するというアイデアは、後述するMoEの「Expert = 部分空間」という解釈と共鳴する。

### RAGとの接続：埋め込み空間での近傍検索

Retrieval-Augmented Generation（RAG）は、大規模言語モデルの知識を外部データベースで補強する手法である（Lewis et al., 2020）。その中核は、クエリの埋め込みベクトルに対する**近傍検索**（意味的に近い文書の検索）にある。

実装上は厳密なkNNではなく、FAISS、ScaNN、HNSWなどのANN手法が用いられることが多い。

RAGが機能するのは、埋め込み空間が「意味的な近さ ≈ 幾何学的な近さ（角度）」という構造を持つよう学習されているからだ。これは偶然ではなく、対照学習（Contrastive Learning）によって意図的に設計された構造である。

## 敵対的サンプルの幾何学：高次元の「抜け道」

### 微小ノイズで誤分類が起きる

敵対的サンプル（Adversarial Examples）は、深層学習の脆弱性を示す現象である。人間には知覚できないほど微小なノイズを加えるだけで、モデルの予測が大きく変わる。

$$\mathbf{x}_{\text{adv}} = \mathbf{x} + \epsilon \cdot \text{sign}(\nabla _{\mathbf{x}} \mathcal{L})$$

ここで $\epsilon$ は微小な摂動の大きさ、 $\nabla_{\mathbf{x}} \mathcal{L}$ は損失関数の入力に関する勾配である（Fast Gradient Sign Method; Goodfellow et al., 2015）。

### 幾何学的解釈（比喩）

敵対的サンプルの存在原因については複数の仮説がある。以下は確立された理論ではなく、教育的な比喩として理解されたい。

**線形性仮説（Goodfellow et al.）：** 高次元空間での線形モデルは、各次元への微小な摂動が累積して大きな出力変化を引き起こしうる。深層ネットワークも局所的には線形に近い振る舞いをするため、この効果が現れる。

**決定境界近傍の「抜け道」仮説（比喩）：** 高次元空間では、決定境界（異なるクラスを分ける超曲面）が非常に複雑な形状を持ちうる。人間の直感では追えない「近道」や「抜け穴」が存在し、微小な移動でも境界を越えてしまう可能性がある。

**次元の多さと「逃げ道」：** 1000次元空間には1000の独立な方向がある。たとえ各方向への摂動が微小でも、「悪意ある」方向を選べば決定境界を越えられる可能性がある。

> [!NOTE]
> 参考：Goodfellow et al., "Explaining and Harnessing Adversarial Examples" (ICLR 2015)

## スパース性の幾何学：Mixture of Expertsの世界

高次元空間の「ほとんどが空」という性質は、呪いではなく**設計原理**として活用できる。Mixture of Experts（MoE）は、この原理の実践例である。

### MoEの基本構造

MoEは、複数の「専門家」（Expert）ネットワークを用意し、入力に応じて一部の専門家だけを活性化する（スパース活性化）アーキテクチャである。

```text
入力 → ルーター → [Expert 1, Expert 2, ..., Expert N] → 重み付き和 → 出力
              ↑
        「どのExpertを使うか」を決定
```

例えば、8つのExpertのうち2つだけを活性化する場合、計算量は約1/4に抑えられる一方、パラメータ総数は8倍近くになる。これにより「容量は大きいが計算は軽い」モデルが実現する。

### ルーティングの幾何学：kNNとの類似性

ルーター（どのExpertを使うかを決める機構）は、しばしば以下のように実装される（簡略化した形）：

$$\text{router}(\mathbf{x}) = \text{TopK}(\text{softmax}(\mathbf{W}_r \mathbf{x}))$$

ここで $\mathbf{W}_r$ の各行は、各Expertの「ゲートベクトル」と解釈できる。すると、ルーティングは本質的に「入力ベクトルに最も近い（内積が大きい）Expertを選ぶ」というkNN的な操作になる。

この視点から見ると、MoEのルーターは「学習されたANN構造」とも言える。各Expertが高次元空間の部分領域を担当し、入力に応じて適切な部分領域へ「ルーティング」される。

> [!NOTE]
> 実際の実装では、ノイズ付きTop-k、Soft MoEなど、より複雑なルーティング機構が用いられることも多い。

### 各Expertは「部分空間」を担当する（仮説）

MoEの幾何学的解釈として、各Expertが高次元空間の異なる部分空間を担当しているという**仮説**がある：

- **Expert A:** 数学的推論に関連する部分空間
- **Expert B:** 言語的ニュアンスに関連する部分空間
- **Expert C:** コード生成に関連する部分空間

第2回で見たように、高次元ではランダムベクトルはほぼ直交する。もし各Expertの担当領域が互いにほぼ直交していれば、Expert間の干渉が少なく、専門性が保たれる。

> [!CAUTION]
> 「学習が進むとExpert間の類似度が下がる」という観察は一部の研究で報告されているが、モデル・学習条件・測定方法に依存し、普遍的な法則とは言えない。また、これが「直交部分空間への分離」と厳密に対応するかは未解明である。

### MoEの課題と幾何学的理解

#### Expert Collapse（専門家崩壊）

MoEの主要な課題は、一部のExpertに負荷が集中し、他のExpertが使われなくなる現象である。これを幾何学的に解釈すると、表現空間が一部の部分空間に「縮退」し、次元崩壊の変種が起きていると考えられる。

対策として**Load Balancing Loss**が用いられる。Switch Transformer（Fedus et al., 2022）では、以下のような補助損失が導入されている（簡略化した概念形）：

$$\mathcal{L}_{\text{balance}} = \alpha \cdot n \cdot \sum_i f_i \cdot p_i$$

- $f_i$ : Expert $i$ に割り当てられたトークンの割合
- $p_i$ : Expert $i$ へのルーティング確率の平均
- $\alpha$ : バランス係数

> [!WARNING]
> 上記は概念を示すための簡略形であり、**この式をそのまま実装に使用してはならない**。実際の論文では、バッチ内での補助損失として定義され、importance loss と load loss を分けて定義するなど、より精緻な形式が用いられている。また、Expertごとの確率分布の扱いやバッチサイズとの関係など、実装上の詳細は原論文を必ず参照されたい。

この損失項は、各Expertへのルーティングを均等化するよう促す。幾何学的には、**入力空間の分割を均等化する正則化**と解釈できる。

#### Shared Expert：共通部分空間の導入

完全に独立なExpertでは、共通知識が重複して非効率になる。そこで、全入力に対して常に活性化する「Shared Expert」を導入する手法がある。

幾何学的には、これは「共通部分空間 + 専門部分空間」の階層構造を明示的に設計することに相当する。DeepSeek-MoE（Dai et al., 2024）などがこのアプローチを採用している。

### MoEとMulti-head Attentionの対比

MoEと第6回で扱うMulti-head Attentionは、ともに「部分空間への分割」というアイデアを共有している。しかし、その性質は異なる：

| 機構 | 分割の単位 | 選択方式 | 幾何学的解釈 |
| --- | --- | --- | --- |
| Multi-head Attention | 表現次元 | 常に全ヘッド活性化 | 部分空間への同時射影 |
| MoE | ネットワーク全体 | 動的に選択 | 部分空間への条件付き射影 |

Multi-headは「常にすべての視点から見る」のに対し、MoEは「状況に応じた視点を選ぶ」。後者はスパースであり、計算効率が高い。

### 最新動向（2024-2025年）

※以下は急速に発展中の分野であり、2026年2月時点の代表例である。

**MoEアーキテクチャの発展：**

- **Mixtral** (Mistral AI, 2023): 8 Experts, Top-2 routing
- **Switch Transformer** (Fedus et al., 2022): Top-1 routing でさらにスパース化
- **DeepSeek-MoE / DeepSeek-V3** (Dai et al., 2024): 細粒度Expert + 共有Expert、コスト効率の高いスケーリング
- **Soft MoE** (Puigcerver et al., ICLR 2024): 離散的ルーティングを連続化（第4回補論との接続）
- **Llama 4** (Meta, 2025): MoEを採用（ただしLlama 3はdense）

**非ユークリッド幾何学との融合：**

- **双曲空間でのLLM表現学習**: Chen et al., "Hyperbolic Large Language Models" (arXiv:2509.05757) など、階層構造の表現に双曲幾何を活用する研究が進展
- **HypLoRA** (Yang et al., 2024): 双曲多様体でのLoRA適用
- **Spectro-Riemannian GNNs** (ICLR 2025): 異なる曲率を組み合わせたグラフニューラルネット

> [!IMPORTANT]
> MoE vs Denseの選択は、モデル規模・用途・推論環境により異なる。MoEが「唯一の主流」というわけではない。

## アライメント問題の幾何学的定式化（比喩）

高次元空間の性質は、AIアライメント問題にも示唆を与える。以下は厳密な定式化ではなく、思考のための比喩である。

人間の価値観が定める「望ましい行動」の集合を、高次元空間内の部分多様体と考えよう。同様に、AIが目的関数を最適化した結果たどり着く行動の集合も、別の部分多様体である。

問題は、高次元空間ではこの2つの多様体が予期せぬ形で乖離しうることだ。低次元の射影（人間が観測できる範囲）では重なって見えても、高次元では大きく離れている可能性がある。

**MoEとの接続：** 「倫理Expert」を明示的に設け、常に活性化させることは可能か？これは技術的には実装可能だが、何を「倫理」として学習させるかという根本的な問題は残る。

## 未解決問題

本講義で扱った内容は、多くの未解決問題を含んでいる：

1. **理解可能性の限界:** 高次元多様体の「全体像」を人間は理解できるのか？私たちの3次元的直感は、どこまで信頼できるのか？

2. **説明可能性の幾何学:** 「このモデルがなぜこの予測をしたか」を説明することは、幾何学的に何を意味するのか？高次元空間での「経路」や「近さ」を、人間に伝達可能な形で表現できるか？

3. **Expertの解釈可能性:** MoEの各Expertは「何を学んだか」を解釈できるのか？部分空間の意味論的ラベル付けは可能か？

4. **敵対的ロバスト性と幾何学:** 敵対的サンプルに強いモデルの決定境界は、どのような幾何学的性質を持つべきか？

## 警告

> [!WARNING]
> 幾何学は中立的な道具だが、その形状は価値観を埋め込む

表現空間の設計は、技術的選択であると同時に、価値判断でもある。何を「近い」と定義し、何を「直交」と見なすかは、最終的に人間の判断である。

## 実装ノート

### 敵対的サンプルの生成（FGSM）

```python
import torch

def fgsm_attack(model, x, y, epsilon=0.01, clip_min=0.0, clip_max=1.0):
    """Fast Gradient Sign Method による敵対的サンプル生成
    
    Args:
        model: 攻撃対象のモデル
        x: 入力テンソル
        y: 正解ラベル
        epsilon: 摂動の大きさ
        clip_min, clip_max: データのレンジ（画像なら0-1、標準化済みなら別の値）
    """
    model.eval()  # 評価モードに設定
    x = x.clone().detach().requires_grad_(True)
    
    # 勾配の混入を防ぐ
    model.zero_grad(set_to_none=True)
    
    output = model(x)
    loss = torch.nn.functional.cross_entropy(output, y)
    loss.backward()
    
    # 勾配の符号方向に摂動
    x_adv = x + epsilon * x.grad.sign()
    
    # データのレンジにクリップ（データ依存なので引数で指定）
    x_adv = torch.clamp(x_adv, clip_min, clip_max)
    
    return x_adv.detach()
```

### MoEモデルの利用例（Hugging Face transformers）

```python
from transformers import MixtralForCausalLM, AutoTokenizer
import torch

# Mixtral-8x7B のロード
model = MixtralForCausalLM.from_pretrained(
    "mistralai/Mixtral-8x7B-v0.1",
    torch_dtype=torch.float16,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-v0.1")
```

### Expert活性化パターンの可視化

```python
import matplotlib.pyplot as plt
import torch

def visualize_expert_activation(router_logits, top_k=2):
    """ルーター出力をヒートマップとして可視化"""
    # router_logits: [batch_size, num_experts]
    probs = torch.softmax(router_logits, dim=-1).detach().cpu().numpy()
    
    plt.figure(figsize=(12, 4))
    plt.imshow(probs, aspect='auto', cmap='viridis')
    plt.xlabel('Expert Index')
    plt.ylabel('Token Index')
    plt.colorbar(label='Routing Probability')
    plt.title(f'Expert Activation Pattern (Top-{top_k} routing)')
    plt.show()
```

### コサイン類似度ベースのkNN

```python
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import normalize

def cosine_knn(X_train, X_query, k=5):
    """コサイン類似度ベースのk近傍探索
    
    L2正規化により、ユークリッド距離とコサイン類似度は単調同値になる：
    ||u - v||^2 = 2(1 - u・v)  （|u| = |v| = 1 のとき）
    
    注意: データが意味構造を持つ埋め込み空間である場合に有効
    """
    # L2正規化により、ユークリッド距離での近傍 = コサイン類似度での近傍
    X_train_norm = normalize(X_train)
    X_query_norm = normalize(X_query)
    
    nn = NearestNeighbors(n_neighbors=k, metric='cosine')
    nn.fit(X_train_norm)
    distances, indices = nn.kneighbors(X_query_norm)
    return distances, indices
```

## 参考文献

- Beyer, K. et al. (1999). "When Is 'Nearest Neighbor' Meaningful?" ICDT.
- Dai, D. et al. (2024). "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models." arXiv:2401.06066.
- Fedus, W. et al. (2021/2022). "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity." arXiv:2101.03961 (2021) / JMLR 23(120):1-39 (2022).
- Goodfellow, I. et al. (2015). "Explaining and Harnessing Adversarial Examples." ICLR.
- Houle, M. et al. (2018). "On the Behavior of Intrinsically High-Dimensional Spaces." JMLR.
- Indyk, P. & Motwani, R. (1998). "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality." STOC.
- Jégou, H. et al. (2011). "Product Quantization for Nearest Neighbor Search." IEEE TPAMI 33(1):117-128.
- Lewis, P. et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." NeurIPS.
- Malkov, Y. & Yashunin, D. (2016/2020). "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs." arXiv:1603.09320 (2016) / IEEE TPAMI 42(4):824-836 (2020).
- Radovanović, M. et al. (2010). "Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data." JMLR 11:2487-2531.

## 次回予告

第14回では、「形」ではなく「構造」を測る視点として**トポロジー**を導入する。高次元空間の「穴」や「ループ」といった位相的特徴は、次元削減しても保存されうる不変量である。Topological Data Analysis（TDA）が表現学習に与える示唆を整理する。
