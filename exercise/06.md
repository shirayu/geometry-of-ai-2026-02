# 第6回：Transformerという測量士

## 問題 6-1 ★ Attentionと角度

標準的なTransformerのAttentionで、Q・Kの内積が純粋な $\cos\theta$ にならない理由を説明せよ。

**ヒント:** LayerNormは何を正規化するか？　L2正規化との違いは？

**解答:**
**LayerNormの動作:**

- 各ベクトルの平均を0、分散を1に正規化
- つまり、ベクトルの「平均と分散」を揃える
- ノルム（長さ）は1にならない

**L2正規化の動作:**

- 各ベクトルのノルムを1に正規化
- つまり、ベクトルを単位球面上に射影

**結果:**

- LayerNorm後のQとKは、ノルムが1ではない
- したがって $Q \cdot K = \|Q\| \|K\| \cos\theta$ であり、ノルムの影響が混在
- 「角度だけ」を見ているわけではない

**補足:** √d_k でのスケーリングも、ノルムの影響を緩和するためのもの（分散を調整）だが、純粋な角度比較にはしない。

## 問題 6-2 ★★ Cosine Attentionの実装

Cosine Attentionを実装し、標準Attentionと出力を比較せよ。どのような違いが生じるか？

**ヒント:** Q, Kをdim=-1でL2正規化してから内積を取る。

**解答:**

```06_cosine_attention.py
import torch
import torch.nn.functional as F
import math

def standard_attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    weights = F.softmax(scores, dim=-1)
    return torch.matmul(weights, V), weights

def cosine_attention(Q, K, V, scale=10.0):
    Q_norm = F.normalize(Q, dim=-1)
    K_norm = F.normalize(K, dim=-1)
    scores = torch.matmul(Q_norm, K_norm.transpose(-2, -1)) * scale
    weights = F.softmax(scores, dim=-1)
    return torch.matmul(weights, V), weights

# テスト
Q = torch.randn(1, 4, 8)  # batch=1, seq=4, dim=8
K = torch.randn(1, 4, 8)
V = torch.randn(1, 4, 8)

out_std, w_std = standard_attention(Q, K, V)
out_cos, w_cos = cosine_attention(Q, K, V)

print("Standard weights:\n", w_std[0].round(decimals=3))
print("Cosine weights:\n", w_cos[0].round(decimals=3))
```

**観察:**

- 標準Attentionは、ノルムが大きいベクトル同士の内積が大きくなりやすい
- Cosine Attentionは、純粋に方向の類似性で重みが決まる
- Cosine Attentionにはスケールパラメータが必要（ $\cos\theta$ は-1〜1なので、Softmax前に拡大しないと重みがぼやける）

## 問題 6-3 ★★★ RoPEの回転表現

RoPE（Rotary Positional Embeddings）が「相対位置を回転で表現する」とはどういうことか、2次元の例で説明せよ。

**ヒント:** 2次元回転行列 $R(\theta) = [[\cos\theta, -\sin\theta], [\sin\theta, \cos\theta]]$ を使う。

**解答:**
**2次元での例:**
位置 $m$ のクエリ $q$ と位置 $n$ のキー $k$ に、それぞれ回転を適用する：

- $q' = R(m\theta) q$ （角度 $m\theta$ だけ回転）
- $k' = R(n\theta) k$ （角度 $n\theta$ だけ回転）

内積を計算すると：
$q' \cdot k' = (R(m\theta)q) \cdot (R(n\theta)k) = q \cdot (R(-m\theta)R(n\theta)k) = q \cdot (R((n-m)\theta)k)$

**重要な性質:** 内積は $(n-m)$ 、つまり相対位置のみに依存する。絶対位置 $m, n$ がどこであっても、距離が同じなら同じスコアになる。

**幾何学的解釈:**

- 各位置は「どれだけ回転したか」で表現される
- 相対位置は「回転角の差」
- これは球面（円）上での角度として自然に解釈できる
