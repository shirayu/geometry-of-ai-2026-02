# 第2回：ノルムの呪い

## 問題 2-1 ★ 高次元での距離の集中

100次元の標準正規分布から2点をサンプリングし、そのユークリッド距離を計算するコードを書け。これを1000回繰り返し、距離のヒストグラムを描け。何が観察されるか？

**ヒント:** `np.random.randn(100)` で1点をサンプリングできる。

**解答:**

```02_distance_concentration.py
import matplotlib.pyplot as plt
import numpy as np

distances = []
for _ in range(1000):
    x = np.random.randn(100)
    y = np.random.randn(100)
    distances.append(np.linalg.norm(x - y))

plt.hist(distances, bins=30)
plt.xlabel("Euclidean Distance")
plt.title("Distance distribution in 100D")
plt.show()
print(f"Mean: {np.mean(distances):.2f}, Std: {np.std(distances):.2f}")
```

**観察:** 距離は約14.1付近（√200 ≈ 14.14）に集中し、分散が非常に小さい。つまり「すべての点がほぼ同じ距離」であり、ユークリッド距離による区別が困難になる。これが次元の呪いの一側面。

## 問題 2-2 ★★ 高次元での角度の分布

同じ設定で、2点間の角度（cosine類似度から計算）のヒストグラムを描け。距離と比較して何が言えるか？

**ヒント:** $\cos\theta = (x \cdot y) / (\|x\| \|y\|)$ 、 $\theta = \arccos(\cos\theta)$

**解答:**

```02_angle_concentration.py
import matplotlib.pyplot as plt
import numpy as np

angles = []
for _ in range(1000):
    x = np.random.randn(100)
    y = np.random.randn(100)
    cos_sim = np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))
    cos_sim = np.clip(cos_sim, -1, 1)  # 数値誤差対策
    angles.append(np.degrees(np.arccos(cos_sim)))

plt.hist(angles, bins=30)
plt.xlabel("Angle (degrees)")
plt.title("Angle distribution in 100D")
plt.show()
print(f"Mean: {np.mean(angles):.1f}°, Std: {np.std(angles):.1f}°")
```

**観察:** 角度は約90°付近に集中する（高次元ではランダムベクトルはほぼ直交）。cosine類似度の分散は概ね $1/n$ スケールであり、**無学習のランダムベクトル同士では角度もかなり鋭く90°付近に集中する**。

**比較:** 距離も角度も、ランダムな状態では特定の値に集中する。しかし重要な違いは、**学習によって角度の構造を作りやすい**点にある。角度ベースの設計では、学習を通じて「近い（小角）」「遠い（大角）」のペアを意図的に作り出せる。これが「角度中心の設計」の動機の一つ。

## 問題 2-3 ★★★ 直交性とMixture of Experts

「高次元ではランダムベクトルがほぼ直交する」という性質は、Mixture of Experts（第13回）でどのように活用されうるか、仮説を立てよ。

**ヒント:** 各Expertが担当する「部分空間」を考えよ。干渉とは何か？

**解答:**
**仮説:** 高次元空間では、ランダムに初期化された各Expertの重みベクトル（または各Expertが担当する部分空間の基底）は、互いにほぼ直交する。このため、あるExpertの学習が他のExpertに与える干渉が小さい。

**活用:**

- Expert間の直交性が高いほど、各Expertは独立に専門化できる
- 入力ベクトルとExpert重心の内積（角度）でルーティングする際、直交性が高いと「どのExpertに近いか」が明確に決まる
- これはMoEのLoad Balancingにも寄与しうる（特定Expertへの偏りが減る）

**注意:** これは「良いMoEの仮説」であり、実際の学習済みMoEで厳密に成り立つとは限らない。
