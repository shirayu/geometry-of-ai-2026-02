# 第3回：プラネタリウムの建設

## 問題 3-1 ★ vMF分布の可視化

2次元単位円上のvMF分布（実質的にはvon Mises分布）を、 $\kappa = 1, 10, 100$ の3通りで可視化せよ。 $\kappa$ の増加に伴い、分布がどう変化するか観察せよ。

**ヒント:** scipy.stats.vonmises を使うと簡単。

**解答:**

```03_vmf_visualization.py
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import vonmises

theta = np.linspace(-np.pi, np.pi, 1000)
fig, axes = plt.subplots(1, 3, figsize=(12, 4))

for ax, kappa in zip(axes, [1, 10, 100], strict=False):
    pdf = vonmises.pdf(theta, kappa)
    ax.plot(theta, pdf)
    ax.set_title(f"κ = {kappa}")
    ax.set_xlabel("θ")
    ax.set_ylabel("Density")

plt.tight_layout()
plt.show()
```

**観察:** $\kappa$ が大きくなるほど、分布は平均方向（ $\theta=0$ ）に鋭く集中する。 $\kappa=1$ では比較的平坦、 $\kappa=100$ では針のように鋭いピーク。これが「確信度」の幾何学的解釈： $\kappa$ が大きい＝「この方向しかない」と確信。

## 問題 3-2 ★★ L2正規化と内積の変化

PyTorchで、バッチ内の全ベクトルをL2正規化する関数を書け。正規化前後で内積がどう変わるか、具体例で確認せよ。

**ヒント:** `F.normalize(x, p=2, dim=-1)` を使う。

**解答:**

```03_l2_normalization_dot.py
import torch
import torch.nn.functional as F

# 正規化前
x = torch.tensor([[3.0, 4.0], [1.0, 2.0]])
y = torch.tensor([[1.0, 0.0], [0.0, 1.0]])

dot_before = torch.sum(x * y, dim=-1)
print(f"正規化前の内積: {dot_before}")  # [3.0, 2.0]

# 正規化後
x_norm = F.normalize(x, p=2, dim=-1)
y_norm = F.normalize(y, p=2, dim=-1)

dot_after = torch.sum(x_norm * y_norm, dim=-1)
print(f"正規化後の内積: {dot_after}")  # [0.6, 0.894...]

# 確認：正規化後の内積はcosθに一致
print(f"||x_norm||: {torch.norm(x_norm, dim=-1)}")  # [1.0, 1.0]
```

**解説:**

- 正規化前：内積は「方向の類似性 × ノルムの積」
- 正規化後：内積は純粋に $\cos\theta$ （-1から1の範囲）
- [3,4]と[1,0]の角度のコサインは 3/5 = 0.6

## 問題 3-3 ★★★ nGPTの設計思想

nGPTは「すべてのベクトルを常にノルム1に保つ」設計だが、これを標準Transformerに後付けで適用しようとすると、どのような困難が生じるか考察せよ。

**ヒント:** 残差接続、LayerNorm、学習済み重みの扱いを考えよ。

**解答:**
**困難1：残差接続との相性**
残差接続 $h' = h + f(h)$ では、 $h$ と $f(h)$ が両方ノルム1でも、和 $h'$ のノルムは1にならない。毎回再正規化すると、残差の「加算」という意味が変質する。

**困難2：既存の学習済み重みとの不整合**
標準Transformerの重みは、ノルム1制約なしで学習されている。事後的に正規化すると、学習時の仮定が崩れ、性能が劣化しうる。

**困難3：勾配の流れの変化**
正規化は非線形操作であり、勾配の流れが変わる。学習済みモデルをfine-tuneする際、想定外の挙動が生じうる。

**結論:** nGPTの設計は「最初から球面上で学習する」ことを前提としており、後付け適用は困難。これが「設計の徹底」の意味。
